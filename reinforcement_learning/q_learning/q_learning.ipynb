{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0. Load the Environment\n",
    "def load_frozen_lake(desc=None, map_name=None, is_slippery=False):\n",
    "    \"\"\"\n",
    "    Function that loads the pre-made FrozenLakeEnv from OpenAI's gym\n",
    "    Args:\n",
    "        desc: Either None or list of lists containing a custom description of\n",
    "            the map to load for the environment\n",
    "        man_name: either None or string containing the pre-made map to load\n",
    "        *** If both desc and map_name are None, the environment will load a\n",
    "            randomly generated 8x8 map ***\n",
    "        is_slippery: boolean to determine if the ice is slippery\n",
    "\n",
    "    Returns: the environment\n",
    "    \"\"\"\n",
    "    environment = gym.make('FrozenLake-v1',\n",
    "                           desc=desc,\n",
    "                           map_name=map_name,\n",
    "                           is_slippery=is_slippery,\n",
    "                           render_mode=\"ansi\")\n",
    "    return environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'S' b'F' b'F' b'F' b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'F' b'F' b'H' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H' b'H' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'H' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'H' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'H' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'G']]\n",
      "[(1.0, 0, 0.0, False)]\n",
      "[[b'S' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
      " [b'H' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
      " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'G']]\n",
      "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 8, 0.0, True)]\n",
      "[[b'S' b'F' b'F']\n",
      " [b'F' b'H' b'H']\n",
      " [b'F' b'F' b'G']]\n",
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n"
     ]
    }
   ],
   "source": [
    "# 0-main\n",
    "np.random.seed(0)\n",
    "env = load_frozen_lake()\n",
    "print(env.desc)\n",
    "print(env.P[0][0])\n",
    "env = load_frozen_lake(is_slippery=True)\n",
    "print(env.desc)\n",
    "print(env.P[0][0])\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "print(env.desc)\n",
    "env = load_frozen_lake(map_name='4x4')\n",
    "print(env.desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "def q_init(env):\n",
    "    \"\"\"\n",
    "    Initialize the Q-table\n",
    "    Args:\n",
    "        env: The FrozenLakeEnv instance\n",
    "    Returns:\n",
    "        The Q-table as a numpy.ndarray of zeros\n",
    "    \"\"\"\n",
    "    nb_states = env.observation_space.n\n",
    "    nb_actions = env.action_space.n\n",
    "    return np.zeros((nb_states, nb_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4)\n",
      "(64, 4)\n",
      "(9, 4)\n",
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "# 1-main\n",
    "env = load_frozen_lake()\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "env = load_frozen_lake(is_slippery=True)\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "env = load_frozen_lake(map_name='4x4')\n",
    "Q = q_init(env)\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2. Epsilon Greedy\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    \"\"\"\n",
    "    Function that uses epsilon-greedy to determine the next action\n",
    "    Args:\n",
    "        Q: numpy.ndarray shape(state, action) containing the q-table\n",
    "        state: current state\n",
    "        epsilon: epsilon to use for the calculation\n",
    "    \n",
    "    Should sample p with numpy.random.uniform to determine if the algorithm\n",
    "        should explore or exploit\n",
    "    \n",
    "    If exploring, should pick the next action with numpy.random.randint from\n",
    "        all possible actions\n",
    "\n",
    "    Returns:\n",
    "        the next action index\n",
    "    \"\"\"\n",
    "    # print(Q)\n",
    "    p = np.random.uniform()\n",
    "    # print(\"p = \", p)\n",
    "    # print(\"epsilon = \", epsilon)\n",
    "    # Explore\n",
    "    if p < epsilon:\n",
    "        action = np.random.randint(0, 4)\n",
    "    # Exploit\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 2-main\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "Q[7] = np.array([0.5, 0.7, 1, -1])\n",
    "np.random.seed(0)\n",
    "print(epsilon_greedy(Q, 7, 0.5))\n",
    "np.random.seed(1)\n",
    "print(epsilon_greedy(Q, 7, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3. Q-learning\n",
    "def train(env, Q, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99,\n",
    "          epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
    "    \"\"\"\n",
    "    Function that performs Q-learning\n",
    "    Args:\n",
    "        env: the FrozenLakeEnv instance\n",
    "        Q: np.ndarray shape(state, action) containing the Q-table\n",
    "        episodes: the total number of episodes to train over\n",
    "        max_steps: maximum number of steps per episode\n",
    "        alpha: learning rate\n",
    "        gamma: discount rate\n",
    "        epsilon: the initial threshold for epsilon greedy\n",
    "        min_epsilon: the minimum value that epsilon should decay to\n",
    "        epsilon_decay: decay rate for updating epsilon between episodes\n",
    "    When the agent falls in a hole, the reward should be updated to -1\n",
    "    Returns:\n",
    "        Q: the updated Q-table\n",
    "        total_rewards: list containing the rewards per episode\n",
    "    \"\"\"\n",
    "    total_rewards = []\n",
    "\n",
    "    # Loop through each episode\n",
    "    for episode in range(episodes):\n",
    "        # Reset all of our episode values\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        # Loop through steps (stop at max_steps if we didn't fall or succeed)\n",
    "        for step in range(max_steps):\n",
    "            action = epsilon_greedy(Q, state, epsilon)\n",
    "            # print(\"Action: \", action)\n",
    "\n",
    "            new_state, reward, terminated, _, _ = env.step(action)\n",
    "            # print(\"Episode: \", episode, \"Step: \", step)\n",
    "            # print(\"New State: \", new_state, \"Reward: \", reward, \"Terminated: \", terminated)\n",
    "\n",
    "            if reward == 1.0 and terminated == True:\n",
    "                episode_reward += reward\n",
    "            if reward == 0.0 and terminated == True:\n",
    "                episode_reward -= 1\n",
    "                reward = -1\n",
    "            if reward == 0.0 and step + 1 == max_steps:\n",
    "                episode_reward += 0\n",
    "\n",
    "            # Bellman Equation\n",
    "            Q[state, action] = (1 - alpha) * Q[state, action] + alpha * (reward + gamma * max(Q[new_state]))\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "            if terminated:\n",
    "                break\n",
    "\n",
    "        epsilon = min_epsilon + (1 - min_epsilon) * np.exp(-epsilon_decay * episode)\n",
    "        total_rewards.append(episode_reward)\n",
    "\n",
    "    # print(total_rewards)\n",
    "\n",
    "    return Q, total_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.96059593  0.970299    0.95098488  0.96059396]\n",
      " [ 0.96059557 -0.77123208  0.0094072   0.37627228]\n",
      " [ 0.18061285 -0.1         0.          0.        ]\n",
      " [ 0.97029877  0.9801     -0.99999988  0.96059583]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.98009763  0.98009933  0.99        0.9702983 ]\n",
      " [ 0.98009922  0.98999782  1.         -0.99999952]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "500 : 0.812\n",
      "1000 : 0.88\n",
      "1500 : 0.9\n",
      "2000 : 0.9\n",
      "2500 : 0.88\n",
      "3000 : 0.844\n",
      "3500 : 0.892\n",
      "4000 : 0.896\n",
      "4500 : 0.852\n",
      "5000 : 0.928\n"
     ]
    }
   ],
   "source": [
    "# 3-main\n",
    "np.random.seed(0)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "\n",
    "Q, total_rewards  = train(env, Q)\n",
    "print(Q)\n",
    "split_rewards = np.split(np.array(total_rewards), 10)\n",
    "for i, rewards in enumerate(split_rewards):\n",
    "    print((i+1) * 500, ':', np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4. Play\n",
    "def play(env, Q, max_steps=100):\n",
    "    \"\"\"\n",
    "    Function that has th trained agent play an episode\n",
    "    Args:\n",
    "        env: FrozenLakeEnv instance\n",
    "        Q: numpy.ndarray shape (state, action) containing the trained Q-table\n",
    "        max_steps: Maximum number of steps in the episode\n",
    "    Each state of the board should be displayed via the console\n",
    "    We should always exploit the Q-table\n",
    "    Returns the total rewards for the episode\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    state, _ = env.reset()\n",
    "    print(env.render(), end='')\n",
    "    for _ in range(max_steps):\n",
    "        action = np.argmax(Q[state])\n",
    "        state, reward, terminated, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        print(env.render(), end='')\n",
    "        if terminated:\n",
    "            break\n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFF\n",
      "FHH\n",
      "FFG\n",
      "  (Down)\n",
      "SFF\n",
      "\u001b[41mF\u001b[0mHH\n",
      "FFG\n",
      "  (Down)\n",
      "SFF\n",
      "FHH\n",
      "\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFF\n",
      "FHH\n",
      "F\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFF\n",
      "FHH\n",
      "FF\u001b[41mG\u001b[0m\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 4-main\n",
    "np.random.seed(0)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "\n",
    "Q, total_rewards  = train(env, Q)\n",
    "print(play(env, Q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
