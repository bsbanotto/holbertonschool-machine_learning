{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import gym\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 0. Simple Policy Function\n",
    "def policy(matrix, weight):\n",
    "    \"\"\"\n",
    "    Computes a policy using the given weight for the provided matrix\n",
    "\n",
    "    Args:\n",
    "        matrix: np.ndarray shape (state, action)\n",
    "        weight: np.ndarray shape (action, weight)\n",
    "\n",
    "    Returns:\n",
    "        The policy computed using the given weight\n",
    "        np.ndarray shape (state, weight)\n",
    "    \"\"\"\n",
    "    dot_prod = matrix.dot(weight)\n",
    "    exp = np.exp(dot_prod)\n",
    "    policy = exp / np.sum(exp)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50351642 0.49648358]]\n"
     ]
    }
   ],
   "source": [
    "# 0-main\n",
    "weight = np.ndarray((4, 2), buffer=np.array([\n",
    "    [4.17022005e-01, 7.20324493e-01], \n",
    "    [1.14374817e-04, 3.02332573e-01], \n",
    "    [1.46755891e-01, 9.23385948e-02], \n",
    "    [1.86260211e-01, 3.45560727e-01]\n",
    "    ]))\n",
    "state = np.ndarray((1, 4), buffer=np.array([\n",
    "    [-0.04428214,  0.01636746,  0.01196594, -0.03095031]\n",
    "    ]))\n",
    "\n",
    "res = policy(state, weight)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1. Compute the Monte-Carlo policy gradient\n",
    "def policy_gradient(state, weight):\n",
    "    \"\"\"\n",
    "    Function that computes the Monte-Carlo policy gradient based on a state\n",
    "        and a weight matrix\n",
    "\n",
    "    Args:\n",
    "        state: matrix representing the current observation of the environment\n",
    "        weight: matrix of random weight\n",
    "\n",
    "    Returns:\n",
    "        The action and the gradieng(in this order)\n",
    "    \"\"\"\n",
    "    MCPolicy = policy(state, weight)\n",
    "    action = np.random.choice(len(MCPolicy[0]), p=MCPolicy[0])\n",
    "\n",
    "    # Need to reshape the policy to build softmax, so we do that here\n",
    "    s = MCPolicy.reshape(-1, 1)\n",
    "\n",
    "    softmax = (np.diagflat(s) - np.dot(s, s.T))[action, :]\n",
    "\n",
    "    log_derivative = softmax / MCPolicy[0, action]\n",
    "\n",
    "    grad = state.T.dot(log_derivative[None, :])\n",
    "\n",
    "    return action, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-main\n",
    "env = gym.make('CartPole-v1')\n",
    "np.random.seed(1)\n",
    "\n",
    "weight = np.random.rand(4, 2)\n",
    "state = env.reset()[None,:]\n",
    "print(weight)\n",
    "print(state)\n",
    "\n",
    "# Create new state values to match example file to test function\n",
    "state = np.ndarray((1, 4), buffer=np.array([\n",
    "    [ 0.04228739, -0.04522399,  0.01190918, -0.03496226]\n",
    "    ]))\n",
    "\n",
    "action, grad = policy_gradient(state, weight)\n",
    "print(action)\n",
    "print(grad)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2. Implement the training\n",
    "def train(env, nb_episodes, alpha=0.000045, gamma=0.98):\n",
    "    \"\"\"\n",
    "    Function that implements a full training of the policy gradient\n",
    "    Args:\n",
    "        env: initial environment\n",
    "        nb_episodes: number of episodes used for training\n",
    "        alpha: learning rate\n",
    "        gamma: discount factor\n",
    "\n",
    "    Returns:\n",
    "        all values of the score (sum of all rewards during one episode loop)\n",
    "    \"\"\"\n",
    "    # Assign weights randomly based on the given environment\n",
    "    number_observations = env.observation_space.shape[0]\n",
    "    number_actions = env.action_space.n\n",
    "    weight = np.random.rand(number_observations, number_actions)\n",
    "\n",
    "    # Initialize a list to track scores of episodes\n",
    "    scores = []\n",
    "\n",
    "    # Iterate over all episodes\n",
    "    for episode in range(nb_episodes + 1):\n",
    "        # Reset the environment and get the initial state\n",
    "        state = env.reset()[None, :]\n",
    "        # Empty out gradients / rewards list and reset episode score to 0\n",
    "        gradients = []\n",
    "        rewards = []\n",
    "        episode_score = 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:  # episode has not ended\n",
    "            # Get action and gradient from the policy_gradient function\n",
    "            action, gradient = policy_gradient(state, weight)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Append gradient and reward to their respective lists\n",
    "            gradients.append(gradient)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # Add the reward to the episode score\n",
    "            episode_score += reward\n",
    "\n",
    "            state = next_state[None, :]\n",
    "\n",
    "        # Convert rewards to a numpy array\n",
    "        rewards = np.array(rewards)\n",
    "\n",
    "        # Update weights using the policy gradient algorithm\n",
    "        for i in range(len(gradients)):\n",
    "            learning = (alpha * gradients[i])\n",
    "            discount = sum(gamma ** rewards[i:] * rewards[i:])\n",
    "            weight += learning * discount\n",
    "\n",
    "        # Append the episode's score to scores list\n",
    "        scores.append(episode_score)\n",
    "\n",
    "        # Print episode scores\n",
    "        print(\"Episode: \" + str(episode) + \" Score: \" + str(episode_score),\n",
    "              end=\"\\r\", flush=False) \n",
    "\n",
    "    # Return the list of scores and the list of weights\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-main\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "scores = train(env, 10000)\n",
    "\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.show()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3. Animate Iteration\n",
    "# Made a copy of task 2 to run here with rendering certain episodes\n",
    "# renamed function from train to render\n",
    "def render(env, nb_episodes, alpha=0.000045, gamma=0.98, show_result=False,\n",
    "           episodes_to_render=[], save_gifs=False):\n",
    "    \"\"\"\n",
    "    Function that implements a full training of the policy gradient\n",
    "    Args:\n",
    "        env: initial environment\n",
    "        nb_episodes: number of episodes used for training\n",
    "        alpha: learning rate\n",
    "        gamma: discount factor\n",
    "\n",
    "    Returns:\n",
    "        all values of the score (sum of all rewards during one episode loop)\n",
    "    \"\"\"    \n",
    "    # Assign weights randomly based on the given environment\n",
    "    number_observations = env.observation_space.shape[0]\n",
    "    number_actions = env.action_space.n\n",
    "    weight = np.random.rand(number_observations, number_actions)\n",
    "\n",
    "    # Initialize a list to track scores of episodes\n",
    "    scores = []\n",
    "\n",
    "    # Iterate over all episodes\n",
    "    for episode in range(nb_episodes + 1):\n",
    "        # Reset the environment and get the initial state\n",
    "        state = env.reset()[None, :]\n",
    "        # Empty out gradients / rewards list and reset episode score to 0\n",
    "        gradients = []\n",
    "        rewards = []\n",
    "        frames = []\n",
    "        episode_score = 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:  # episode has not ended\n",
    "            # Render the game, human format\n",
    "            if show_result and not save_gifs and episode in episodes_to_render:\n",
    "                env.render(mode='human')\n",
    "            # If we want to save gifs, build the frame array\n",
    "            if show_result and save_gifs and episode in episodes_to_render:\n",
    "                frame_arr = env.render(mode='rgb_array')\n",
    "                frames.append(frame_arr)\n",
    "            # Convert that frame array to a .gif and save locally\n",
    "            path = \"episode_gifs\"\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "            if show_result and save_gifs and episode in episodes_to_render:\n",
    "                imageio.mimsave(path + '/episode_' + str(episode) + '.gif', \n",
    "                                frames, \n",
    "                                fps = 16, \n",
    "                                )\n",
    "            # Get action and gradient from the policy_gradient function\n",
    "            action, gradient = policy_gradient(state, weight)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Append gradient and reward to their respective lists\n",
    "            gradients.append(gradient)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            # Add the reward to the episode score\n",
    "            episode_score += reward\n",
    "\n",
    "            state = next_state[None, :]\n",
    "\n",
    "        # Convert rewards to a numpy array\n",
    "        rewards = np.array(rewards)\n",
    "\n",
    "        # Update weights using the policy gradient algorithm\n",
    "        for i in range(len(gradients)):\n",
    "            learning = (alpha * gradients[i])\n",
    "            discount = sum(gamma ** rewards[i:] * rewards[i:])\n",
    "            weight += learning * discount\n",
    "\n",
    "        # Append the episode's score to scores list\n",
    "        scores.append(episode_score)\n",
    "\n",
    "        # Print episode scores\n",
    "        if episode in episodes_to_render:\n",
    "            print(\"Episode: \" + str(episode) + \" Score: \" + str(episode_score))\n",
    "\n",
    "    # Return the list of scores and the list of weights\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Score: 12.0\n",
      "Episode: 250 Score: 30.0\n",
      "Episode: 500 Score: 23.0\n",
      "Episode: 1000 Score: 37.0\n",
      "Episode: 2500 Score: 494.0\n"
     ]
    }
   ],
   "source": [
    "# 3-main\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "scores = render(env=env,\n",
    "                nb_episodes=2500,\n",
    "                alpha=0.000045,\n",
    "                gamma=0.98,\n",
    "                show_result=True,\n",
    "                episodes_to_render=[0, 250, 500, 1000, 2500],\n",
    "                save_gifs=True)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
