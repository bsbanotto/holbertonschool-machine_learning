{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0 Monte Carlo\n",
    "def run_episode(env, max_steps, policy):\n",
    "    \"\"\"\n",
    "    Runs an episode of the environment\n",
    "    Args:\n",
    "        env: the openAI environment instance\n",
    "        max_steps: maximum number of steps per episode\n",
    "\n",
    "    Return:\n",
    "        episode_results: np.ndarray of integers shape (state, reward)\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    episode_results = []\n",
    "\n",
    "    # Run each episode until we reach max_steps\n",
    "    for step in range(max_steps):\n",
    "        action = policy(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_results.append([state, reward])\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return np.array(episode_results, dtype=int)\n",
    "\n",
    "\n",
    "def monte_carlo(env, V, policy, episodes=5000, max_steps=100, alpha=0.1,\n",
    "                gamma=0.99):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: the openAI environment instance\n",
    "        V: np.ndarray shape (s,) containing the value estimate\n",
    "        policy: a function that takes in a state and returns the next action\n",
    "        episodes: total number of episodes to train over\n",
    "        max_steps: maximum number of steps per episode\n",
    "        alpha: learning rate\n",
    "        gamma: discount rate\n",
    "\n",
    "    Returns:\n",
    "        V: the updated value estimate\n",
    "    \"\"\"\n",
    "    # Loop through our episodes\n",
    "    for episode in range(episodes):\n",
    "        cumulative_reward = 0\n",
    "        episode_results = run_episode(env, max_steps, policy)\n",
    "        # Perform Monte Carlo Algorithm from finish to start\n",
    "        for time in reversed(range(0, len(episode_results))):\n",
    "            state, reward = episode_results[time]\n",
    "            cumulative_reward = gamma * cumulative_reward + reward\n",
    "            if state not in episode_results[:episode, 0]:\n",
    "                V[state] = V[state] + alpha * (cumulative_reward - V[state])\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81    0.9     0.4783  0.4305  0.3874  0.4305  0.6561  0.9   ]\n",
      " [ 0.9     0.729   0.5905  0.4783  0.5905  0.2824  0.2824  0.3874]\n",
      " [ 1.      0.5314  0.729  -1.      1.      0.3874  0.2824  0.4305]\n",
      " [ 1.      0.5905  0.81    0.9     1.     -1.      0.3874  0.6561]\n",
      " [ 1.      0.6561  0.81   -1.      1.      1.      0.729   0.5314]\n",
      " [ 1.     -1.     -1.      1.      1.      1.     -1.      0.9   ]\n",
      " [ 1.     -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
      " [ 1.      1.      1.     -1.      1.      1.      1.      1.    ]]\n"
     ]
    }
   ],
   "source": [
    "# 0-main\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
    "np.set_printoptions(precision=4)\n",
    "env.seed(0)\n",
    "print(monte_carlo(env, V, policy).reshape((8, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - TD(λ)\n",
    "def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100,\n",
    "               alpha=0.1, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: the openAI environment instance\n",
    "        V: numpy.ndarray of shape (s,) containing the value estimate\n",
    "        policy: function that takes in state and returns  next action to take\n",
    "        lambtha: eligibility trace factor\n",
    "        episodes: total number of episodes to train over\n",
    "        max_steps: maximum number of steps per episode\n",
    "        alpha: learning rate\n",
    "        gamma: discount rate\n",
    "    Returns:\n",
    "        V: the updated value estimate\n",
    "    \"\"\"\n",
    "    eligibility_trace = np.zeros_like(V)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # TD error\n",
    "            delta = reward + (gamma * V[next_state] - V[state])\n",
    "\n",
    "            # Update eligibility trace\n",
    "            eligibility_trace *= (gamma * lambtha)\n",
    "            eligibility_trace[state] += 1\n",
    "\n",
    "            # Update value estimate\n",
    "            V += delta * alpha * eligibility_trace\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done or step > max_steps:\n",
    "                break\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8455 -0.8293 -0.8007 -0.7884 -0.7922 -0.7395 -0.7184 -0.7291]\n",
      " [-0.8697 -0.8663 -0.8324 -0.8601 -0.8517 -0.7871 -0.7165 -0.707 ]\n",
      " [-0.9069 -0.9047 -0.921  -1.     -0.8818 -0.87   -0.7787 -0.7885]\n",
      " [-0.9262 -0.9284 -0.9416 -0.9952 -0.984  -1.     -0.9025 -0.8248]\n",
      " [-0.9205 -0.9471 -0.9595 -1.     -0.9654 -0.9457 -0.9259 -0.6976]\n",
      " [-0.9515 -1.     -1.      0.6371 -0.9658 -0.9013 -1.     -0.1736]\n",
      " [-0.962  -1.     -0.3394  0.2564 -1.     -0.6066 -1.      0.292 ]\n",
      " [-0.9403 -0.9457 -0.8711 -1.      1.      0.6161  0.9151  1.    ]]\n"
     ]
    }
   ],
   "source": [
    "# 1-main\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
    "np.set_printoptions(precision=4)\n",
    "print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 Sarsa(λ)\n",
    "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1,\n",
    "                  gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: the openAI environment instance\n",
    "        Q: numpy.ndarray of shape (s,a) containing the Q table\n",
    "        lambtha: eligibility trace factor\n",
    "        episodes: total number of episodes to train over\n",
    "        max_steps: maximum number of steps per episode\n",
    "        alpha: learning rate\n",
    "        gamma: discount rate\n",
    "        epsilon: initial threshold for epsilon greedy\n",
    "        min_epsilon: minimum value that epsilon should decay to\n",
    "        epsilon_decay: decay rate for updating epsilon between episodes\n",
    "\n",
    "    Returns:\n",
    "        Q: the updated Q table\n",
    "    \"\"\"\n",
    "    eligibility_trace = np.zeros_like(Q)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_done = False\n",
    "\n",
    "        action = epsilon_greedy(Q, state, epsilon)\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon - epsilon_decay)\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            n_s, reward, episode_done, _ = env.step(action)\n",
    "            n_a = epsilon_greedy(Q, n_s, epsilon)\n",
    "\n",
    "            # Calculate TD error\n",
    "            td_error = reward + gamma * Q[n_s][n_a] - Q[state][action]\n",
    "\n",
    "            # Update eligibility trace\n",
    "            eligibility_trace *= lambtha * gamma\n",
    "            eligibility_trace[state][action] = 1.0\n",
    "\n",
    "            # Update Q values\n",
    "            Q += alpha * td_error * eligibility_trace\n",
    "\n",
    "            state = n_s\n",
    "            action = n_a\n",
    "\n",
    "            if episode_done:\n",
    "                break\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(len(Q[state]))\n",
    "    else:\n",
    "        return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5929 0.6669 0.5685 0.5609]\n",
      " [0.5401 0.5712 0.6599 0.5251]\n",
      " [0.6508 0.5524 0.528  0.5659]\n",
      " [0.4552 0.5299 0.4191 0.4677]\n",
      " [0.4823 0.5262 0.64   0.5597]\n",
      " [0.5016 0.6507 0.493  0.4886]\n",
      " [0.3519 0.6356 0.3198 0.5394]\n",
      " [0.6511 0.5191 0.3894 0.5458]\n",
      " [0.5743 0.7175 0.6175 0.6261]\n",
      " [0.5971 0.6942 0.6236 0.6137]\n",
      " [0.6725 0.4886 0.5165 0.5232]\n",
      " [0.3607 0.3682 0.4257 0.364 ]\n",
      " [0.5097 0.4253 0.6443 0.4497]\n",
      " [0.6604 0.465  0.4425 0.4943]\n",
      " [0.6643 0.2929 0.5099 0.2444]\n",
      " [0.2676 0.2762 0.6682 0.1751]\n",
      " [0.6628 0.7516 0.6277 0.6218]\n",
      " [0.7255 0.6231 0.5879 0.5897]\n",
      " [0.7109 0.5437 0.5272 0.57  ]\n",
      " [0.2828 0.1202 0.2961 0.1187]\n",
      " [0.4761 0.6666 0.3854 0.4799]\n",
      " [0.6227 0.4961 0.6864 0.3488]\n",
      " [0.6043 0.7512 0.5106 0.6096]\n",
      " [0.2377 0.7803 0.3768 0.2804]\n",
      " [0.689  0.8001 0.6844 0.626 ]\n",
      " [0.7097 0.77   0.7208 0.6442]\n",
      " [0.6708 0.8351 0.6862 0.6964]\n",
      " [0.6973 0.8575 0.6333 0.5406]\n",
      " [0.7386 0.8886 0.7274 0.6494]\n",
      " [0.8811 0.5813 0.8817 0.6925]\n",
      " [0.7839 0.5543 0.6725 0.674 ]\n",
      " [0.5296 0.7939 0.1599 0.4759]\n",
      " [0.6798 0.7307 0.7016 0.8166]\n",
      " [0.706  0.7104 0.8036 0.7268]\n",
      " [0.6099 0.8157 0.7126 0.6893]\n",
      " [0.8965 0.3676 0.4359 0.8919]\n",
      " [0.7732 0.7671 0.3471 0.9027]\n",
      " [0.7073 0.7866 0.5086 0.8491]\n",
      " [0.245  0.6359 0.417  0.8903]\n",
      " [0.9234 0.6178 0.4733 0.3545]\n",
      " [0.7254 0.7936 0.6682 0.7302]\n",
      " [0.9755 0.8558 0.0117 0.36  ]\n",
      " [0.73   0.1716 0.521  0.0543]\n",
      " [0.2    0.0185 0.7955 0.2239]\n",
      " [0.3454 0.8528 0.7044 0.0318]\n",
      " [0.1647 0.8072 0.6077 0.2973]\n",
      " [0.9342 0.614  0.5356 0.5899]\n",
      " [1.0688 0.3364 0.4662 0.3083]\n",
      " [0.24   0.4861 0.4782 0.5729]\n",
      " [0.2274 0.2544 0.058  0.4344]\n",
      " [0.3118 0.6348 0.3778 0.217 ]\n",
      " [0.0247 0.0672 0.6637 0.5049]\n",
      " [0.5366 0.8967 0.9903 0.2169]\n",
      " [0.6631 0.2633 0.0207 0.802 ]\n",
      " [0.32   0.3835 0.5883 0.831 ]\n",
      " [0.629  1.2917 0.2735 0.798 ]\n",
      " [0.2504 0.4866 0.5131 0.2996]\n",
      " [0.5368 0.5527 0.312  0.2869]\n",
      " [0.554  0.0741 0.2075 0.4247]\n",
      " [0.3742 0.4636 0.2776 0.5868]\n",
      " [0.8639 0.1175 0.5174 0.1321]\n",
      " [0.7169 0.3961 0.5654 0.1833]\n",
      " [0.1448 0.4881 0.3556 0.9404]\n",
      " [0.7653 0.7487 0.9037 0.0834]]\n"
     ]
    }
   ],
   "source": [
    "#2-main\n",
    "np.random.seed(0)\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "Q = np.random.uniform(size=(64, 4))\n",
    "np.set_printoptions(precision=4)\n",
    "print(sarsa_lambtha(env, Q, 0.9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
