{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0 Monte Carlo\n",
    "def run_episode(env, max_steps, policy):\n",
    "    \"\"\"\n",
    "    Runs an episode of the environment\n",
    "    Args:\n",
    "        env: the openAI environment instance\n",
    "        max_steps: maximum number of steps per episode\n",
    "\n",
    "    Return:\n",
    "        episode_results: np.ndarray of integers shape (state, reward)\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    episode_results = []\n",
    "\n",
    "    # Run each episode until we reach max_steps\n",
    "    for step in range(max_steps):\n",
    "        action = policy(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_results.append([state, reward])\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return np.array(episode_results, dtype=int)\n",
    "\n",
    "\n",
    "def monte_carlo(env, V, policy, episodes=5000, max_steps=100, alpha=0.1,\n",
    "                gamma=0.99):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: the openAI environment instance\n",
    "        V: np.ndarray shape (s,) containing the value estimate\n",
    "        policy: a function that takes in a state and returns the next action\n",
    "        episodes: total number of episodes to train over\n",
    "        max_steps: maximum number of steps per episode\n",
    "        alpha: learning rate\n",
    "        gamma: discount rate\n",
    "\n",
    "    Returns:\n",
    "        V: the updated value estimate\n",
    "    \"\"\"\n",
    "    # Loop through our episodes\n",
    "    for episode in range(episodes):\n",
    "        cumulative_reward = 0\n",
    "        episode_results = run_episode(env, max_steps, policy)\n",
    "        # Perform Monte Carlo Algorithm from finish to start\n",
    "        for time in reversed(range(0, len(episode_results))):\n",
    "            state, reward = episode_results[time]\n",
    "            cumulative_reward = gamma * cumulative_reward + reward\n",
    "            if state not in episode_results[:episode, 0]:\n",
    "                V[state] = V[state] + alpha * (cumulative_reward - V[state])\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.81    0.9     0.4783  0.4305  0.3874  0.4305  0.6561  0.9   ]\n",
      " [ 0.9     0.729   0.5905  0.4783  0.5905  0.2824  0.2824  0.3874]\n",
      " [ 1.      0.5314  0.729  -1.      1.      0.3874  0.2824  0.4305]\n",
      " [ 1.      0.5905  0.81    0.9     1.     -1.      0.3874  0.6561]\n",
      " [ 1.      0.6561  0.81   -1.      1.      1.      0.729   0.5314]\n",
      " [ 1.     -1.     -1.      1.      1.      1.     -1.      0.9   ]\n",
      " [ 1.     -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
      " [ 1.      1.      1.     -1.      1.      1.      1.      1.    ]]\n"
     ]
    }
   ],
   "source": [
    "# 0-main\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
    "np.set_printoptions(precision=4)\n",
    "env.seed(0)\n",
    "print(monte_carlo(env, V, policy).reshape((8, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - TD(λ)\n",
    "def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100,\n",
    "               alpha=0.1, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: the openAI environment instance\n",
    "        V: numpy.ndarray of shape (s,) containing the value estimate\n",
    "        policy: function that takes in state and returns  next action to take\n",
    "        lambtha: eligibility trace factor\n",
    "        episodes: total number of episodes to train over\n",
    "        max_steps: maximum number of steps per episode\n",
    "        alpha: learning rate\n",
    "        gamma: discount rate\n",
    "    Returns:\n",
    "        V: the updated value estimate\n",
    "    \"\"\"\n",
    "    eligibility_trace = np.zeros_like(V)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        env.seed(0)\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = policy(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # TD error\n",
    "            delta = reward + (gamma * V[next_state] - V[state])\n",
    "\n",
    "            # Update eligibility trace\n",
    "            eligibility_trace *= (gamma * lambtha)\n",
    "            eligibility_trace[state] += 1\n",
    "\n",
    "            # Update value estimate\n",
    "            V += delta * alpha * eligibility_trace\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done or step > max_steps:\n",
    "                break\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.8455 -0.8419 -0.8327 -0.7606 -0.7426 -0.7286 -0.5796 -0.7167]\n",
      " [-0.8805 -0.8791 -0.879  -0.82   -0.777  -0.736  -0.6973 -0.6678]\n",
      " [-0.8993 -0.9341 -0.9593 -1.     -0.8285 -0.8098 -0.8266 -0.8228]\n",
      " [-0.93   -0.9498 -0.953  -0.9714 -0.8712 -1.     -0.8732 -0.86  ]\n",
      " [-0.948  -0.9685 -0.9772 -1.     -0.6707 -0.7231 -0.9086 -0.8171]\n",
      " [-0.9195 -1.     -1.      0.2853 -0.7859 -0.6773 -1.     -0.4291]\n",
      " [-0.9397 -1.     -0.4064 -0.1074 -1.     -0.2633 -1.     -0.1985]\n",
      " [-0.8736 -0.9177 -0.7974 -1.      1.      0.354   0.9571  1.    ]]\n"
     ]
    }
   ],
   "source": [
    "# 1-main\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
    "np.set_printoptions(precision=4)\n",
    "print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 Sarsa(λ)\n",
    "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1,\n",
    "                  gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env: the openAI environment instance\n",
    "        Q: numpy.ndarray of shape (s,a) containing the Q table\n",
    "        lambtha: eligibility trace factor\n",
    "        episodes: total number of episodes to train over\n",
    "        max_steps: maximum number of steps per episode\n",
    "        alpha: learning rate\n",
    "        gamma: discount rate\n",
    "        epsilon: initial threshold for epsilon greedy\n",
    "        min_epsilon: minimum value that epsilon should decay to\n",
    "        epsilon_decay: decay rate for updating epsilon between episodes\n",
    "\n",
    "    Returns:\n",
    "        Q: the updated Q table\n",
    "    \"\"\"\n",
    "    eligibility_trace = np.zeros_like(Q)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_done = False\n",
    "\n",
    "        action = epsilon_greedy(Q, state, epsilon)\n",
    "\n",
    "        epsilon = max(min_epsilon, epsilon - epsilon_decay)\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            n_s, reward, episode_done, _ = env.step(action)\n",
    "            n_a = epsilon_greedy(Q, n_s, epsilon)\n",
    "\n",
    "            # Calculate TD error\n",
    "            td_error = reward + gamma * Q[n_s][n_a] - Q[state][action]\n",
    "\n",
    "            # Update eligibility trace\n",
    "            eligibility_trace *= lambtha * gamma\n",
    "            eligibility_trace[state][action] = 1.0\n",
    "\n",
    "            # Update Q values\n",
    "            Q += alpha * td_error * eligibility_trace\n",
    "\n",
    "            state = n_s\n",
    "            action = n_a\n",
    "\n",
    "            if episode_done:\n",
    "                break\n",
    "\n",
    "    return Q\n",
    "\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return np.random.choice(len(Q[state]))\n",
    "    else:\n",
    "        return np.argmax(Q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6262 0.6306 0.6409 0.6374]\n",
      " [0.6244 0.5979 0.6378 0.6168]\n",
      " [0.6409 0.6011 0.6005 0.598 ]\n",
      " [0.6266 0.6337 0.6826 0.6291]\n",
      " [0.6694 0.696  0.6841 0.6533]\n",
      " [0.6883 0.7174 0.683  0.7012]\n",
      " [0.566  0.7253 0.6443 0.6436]\n",
      " [0.7239 0.6588 0.5858 0.6408]\n",
      " [0.6305 0.6866 0.606  0.6411]\n",
      " [0.6003 0.6491 0.6356 0.6121]\n",
      " [0.6106 0.684  0.6148 0.6313]\n",
      " [0.4618 0.5625 0.5724 0.7058]\n",
      " [0.6766 0.662  0.7325 0.6508]\n",
      " [0.7068 0.7574 0.7088 0.6908]\n",
      " [0.7501 0.6216 0.7347 0.5917]\n",
      " [0.3948 0.4392 0.7269 0.452 ]\n",
      " [0.692  0.6922 0.6895 0.6698]\n",
      " [0.6548 0.6311 0.6104 0.6594]\n",
      " [0.6931 0.5376 0.5318 0.4434]\n",
      " [0.2828 0.1202 0.2961 0.1187]\n",
      " [0.5979 0.5127 0.779  0.5929]\n",
      " [0.7632 0.8006 0.7593 0.665 ]\n",
      " [0.7541 0.8298 0.7109 0.7564]\n",
      " [0.5809 0.8178 0.6943 0.68  ]\n",
      " [0.7085 0.7075 0.7249 0.6506]\n",
      " [0.6982 0.6414 0.6864 0.6936]\n",
      " [0.7303 0.6951 0.6783 0.6749]\n",
      " [0.6479 0.793  0.5564 0.6667]\n",
      " [0.7637 0.7129 0.7753 0.8138]\n",
      " [0.8811 0.5813 0.8817 0.6925]\n",
      " [0.8722 0.734  0.763  0.7765]\n",
      " [0.7623 0.8662 0.6942 0.6584]\n",
      " [0.7327 0.6947 0.7283 0.6838]\n",
      " [0.7279 0.7239 0.6964 0.6997]\n",
      " [0.709  0.7149 0.7434 0.7644]\n",
      " [0.8965 0.3676 0.4359 0.8919]\n",
      " [0.8311 0.7644 0.3685 0.756 ]\n",
      " [0.7373 0.8571 0.2753 0.7976]\n",
      " [0.6556 0.6991 0.5881 0.961 ]\n",
      " [0.9794 0.7032 0.6895 0.5993]\n",
      " [0.736  0.6968 0.7243 0.7772]\n",
      " [0.9755 0.8558 0.0117 0.36  ]\n",
      " [0.73   0.1716 0.521  0.0543]\n",
      " [0.2    0.0185 0.8411 0.2874]\n",
      " [0.4838 0.8812 0.7143 0.0982]\n",
      " [0.2079 0.8595 0.5767 0.3844]\n",
      " [0.9342 0.614  0.5356 0.5899]\n",
      " [1.0617 0.3119 0.5709 0.4318]\n",
      " [0.3295 0.4921 0.5447 0.4905]\n",
      " [0.2274 0.2544 0.058  0.4344]\n",
      " [0.3479 0.6443 0.3778 0.1796]\n",
      " [0.0748 0.0672 0.7941 0.4537]\n",
      " [0.5366 0.8967 0.9903 0.2169]\n",
      " [0.6853 0.2633 0.1017 0.7883]\n",
      " [0.32   0.3835 0.5883 0.831 ]\n",
      " [0.6346 1.2559 0.4304 0.798 ]\n",
      " [0.2243 0.4714 0.4738 0.2793]\n",
      " [0.5006 0.4975 0.2895 0.2663]\n",
      " [0.4969 0.1421 0.2759 0.4247]\n",
      " [0.3742 0.4636 0.2776 0.5868]\n",
      " [0.8639 0.1175 0.5174 0.1321]\n",
      " [0.7169 0.3961 0.5654 0.1833]\n",
      " [0.1448 0.4881 0.3556 0.9404]\n",
      " [0.7653 0.7487 0.9037 0.0834]]\n"
     ]
    }
   ],
   "source": [
    "#2-main\n",
    "np.random.seed(0)\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "Q = np.random.uniform(size=(64, 4))\n",
    "np.set_printoptions(precision=4)\n",
    "print(sarsa_lambtha(env, Q, 0.9))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
