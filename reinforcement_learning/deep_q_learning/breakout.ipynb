{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Read the README for instructions to create the conda environment and\n",
    "create a ipykernel to use that environment in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable TensorFlow Warnings(Because I don't like seeing them)\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import gym\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(env.action_space.n, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=2000, window_length=1)\n",
    "policy = EpsGreedyQPolicy(eps=0.1)\n",
    "dqn = DQNAgent(model=model, nb_actions=env.action_space.n, memory=memory, nb_steps_warmup=1000, target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "  531/10000: episode: 1, duration: 3.042s, episode steps: 531, steps per second: 175, episode reward: 4.000, mean reward: 0.008 [0.000, 1.000], mean action: 1.942 [0.000, 3.000], mean observation: 40.286 [0.000, 200.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "  967/10000: episode: 2, duration: 2.683s, episode steps: 436, steps per second: 163, episode reward: 2.000, mean reward: 0.005 [0.000, 1.000], mean action: 1.775 [0.000, 3.000], mean observation: 40.479 [0.000, 200.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      " 1263/10000: episode: 3, duration: 15.879s, episode steps: 296, steps per second: 19, episode reward: 2.000, mean reward: 0.007 [0.000, 1.000], mean action: 1.270 [0.000, 3.000], mean observation: 40.493 [0.000, 200.000], loss: 54893.251261, mean_absolute_error: 83.427633, mean_q: 13.880037\n",
      " 1552/10000: episode: 4, duration: 17.202s, episode steps: 289, steps per second: 17, episode reward: 2.000, mean reward: 0.007 [0.000, 1.000], mean action: 1.055 [0.000, 3.000], mean observation: 40.400 [0.000, 200.000], loss: 0.003326, mean_absolute_error: 0.020786, mean_q: 0.035557\n",
      " 1825/10000: episode: 5, duration: 16.249s, episode steps: 273, steps per second: 17, episode reward: 2.000, mean reward: 0.007 [0.000, 1.000], mean action: 1.033 [0.000, 3.000], mean observation: 40.560 [0.000, 200.000], loss: 0.003744, mean_absolute_error: 0.019085, mean_q: 0.035544\n",
      " 2091/10000: episode: 6, duration: 15.571s, episode steps: 266, steps per second: 17, episode reward: 2.000, mean reward: 0.008 [0.000, 1.000], mean action: 1.045 [0.000, 3.000], mean observation: 40.625 [0.000, 200.000], loss: 0.003646, mean_absolute_error: 0.017966, mean_q: 0.035611\n",
      " 2316/10000: episode: 7, duration: 14.804s, episode steps: 225, steps per second: 15, episode reward: 1.000, mean reward: 0.004 [0.000, 1.000], mean action: 1.071 [0.000, 3.000], mean observation: 40.518 [0.000, 200.000], loss: 0.004020, mean_absolute_error: 0.016483, mean_q: 0.035785\n",
      " 2608/10000: episode: 8, duration: 17.631s, episode steps: 292, steps per second: 17, episode reward: 2.000, mean reward: 0.007 [0.000, 1.000], mean action: 1.045 [0.000, 3.000], mean observation: 40.502 [0.000, 200.000], loss: 0.003458, mean_absolute_error: 0.014333, mean_q: 0.036066\n",
      " 2770/10000: episode: 9, duration: 9.418s, episode steps: 162, steps per second: 17, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.093 [0.000, 3.000], mean observation: 40.759 [0.000, 200.000], loss: 0.003391, mean_absolute_error: 0.012613, mean_q: 0.036340\n",
      " 3036/10000: episode: 10, duration: 16.193s, episode steps: 266, steps per second: 16, episode reward: 2.000, mean reward: 0.008 [0.000, 1.000], mean action: 1.068 [0.000, 3.000], mean observation: 40.461 [0.000, 200.000], loss: 0.003454, mean_absolute_error: 0.011089, mean_q: 0.036629\n",
      " 3282/10000: episode: 11, duration: 14.749s, episode steps: 246, steps per second: 17, episode reward: 2.000, mean reward: 0.008 [0.000, 1.000], mean action: 1.037 [0.000, 3.000], mean observation: 40.628 [0.000, 200.000], loss: 0.003326, mean_absolute_error: 0.010349, mean_q: 0.037284\n",
      " 3502/10000: episode: 12, duration: 12.682s, episode steps: 220, steps per second: 17, episode reward: 1.000, mean reward: 0.005 [0.000, 1.000], mean action: 1.059 [0.000, 3.000], mean observation: 40.639 [0.000, 200.000], loss: 0.003504, mean_absolute_error: 0.010431, mean_q: 0.037831\n",
      " 3700/10000: episode: 13, duration: 11.566s, episode steps: 198, steps per second: 17, episode reward: 1.000, mean reward: 0.005 [0.000, 1.000], mean action: 1.051 [0.000, 3.000], mean observation: 40.582 [0.000, 200.000], loss: 0.003379, mean_absolute_error: 0.010420, mean_q: 0.038522\n",
      " 3986/10000: episode: 14, duration: 16.822s, episode steps: 286, steps per second: 17, episode reward: 2.000, mean reward: 0.007 [0.000, 1.000], mean action: 1.073 [0.000, 3.000], mean observation: 40.509 [0.000, 200.000], loss: 0.002646, mean_absolute_error: 0.010127, mean_q: 0.039264\n",
      " 4148/10000: episode: 15, duration: 9.565s, episode steps: 162, steps per second: 17, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.068 [0.000, 3.000], mean observation: 40.754 [0.000, 200.000], loss: 0.002765, mean_absolute_error: 0.010214, mean_q: 0.039800\n",
      " 4310/10000: episode: 16, duration: 9.729s, episode steps: 162, steps per second: 17, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.037 [0.000, 3.000], mean observation: 40.766 [0.000, 200.000], loss: 0.002387, mean_absolute_error: 0.010116, mean_q: 0.040484\n",
      " 4474/10000: episode: 17, duration: 9.905s, episode steps: 164, steps per second: 17, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.067 [0.000, 3.000], mean observation: 40.748 [0.000, 200.000], loss: 0.002431, mean_absolute_error: 0.010234, mean_q: 0.040977\n",
      " 4700/10000: episode: 18, duration: 13.403s, episode steps: 226, steps per second: 17, episode reward: 1.000, mean reward: 0.004 [0.000, 1.000], mean action: 1.049 [0.000, 3.000], mean observation: 40.514 [0.000, 200.000], loss: 0.002127, mean_absolute_error: 0.010085, mean_q: 0.041768\n",
      " 4860/10000: episode: 19, duration: 9.733s, episode steps: 160, steps per second: 16, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.031 [0.000, 3.000], mean observation: 40.765 [0.000, 200.000], loss: 0.002896, mean_absolute_error: 0.010541, mean_q: 0.042673\n",
      " 5023/10000: episode: 20, duration: 10.285s, episode steps: 163, steps per second: 16, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.025 [0.000, 3.000], mean observation: 40.724 [0.000, 200.000], loss: 0.001980, mean_absolute_error: 0.010181, mean_q: 0.043449\n",
      " 5185/10000: episode: 21, duration: 12.333s, episode steps: 162, steps per second: 13, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.080 [0.000, 3.000], mean observation: 40.729 [0.000, 200.000], loss: 0.002182, mean_absolute_error: 0.010197, mean_q: 0.044095\n",
      " 5352/10000: episode: 22, duration: 10.742s, episode steps: 167, steps per second: 16, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.006 [0.000, 3.000], mean observation: 40.761 [0.000, 200.000], loss: 0.001492, mean_absolute_error: 0.010004, mean_q: 0.044511\n",
      " 5618/10000: episode: 23, duration: 15.234s, episode steps: 266, steps per second: 17, episode reward: 2.000, mean reward: 0.008 [0.000, 1.000], mean action: 1.056 [0.000, 3.000], mean observation: 40.540 [0.000, 200.000], loss: 0.000884, mean_absolute_error: 0.009671, mean_q: 0.044657\n",
      " 5780/10000: episode: 24, duration: 9.525s, episode steps: 162, steps per second: 17, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.037 [0.000, 3.000], mean observation: 40.765 [0.000, 200.000], loss: 0.001209, mean_absolute_error: 0.010068, mean_q: 0.045014\n",
      " 5975/10000: episode: 25, duration: 11.345s, episode steps: 195, steps per second: 17, episode reward: 1.000, mean reward: 0.005 [0.000, 1.000], mean action: 1.056 [0.000, 3.000], mean observation: 40.682 [0.000, 200.000], loss: 0.000932, mean_absolute_error: 0.010011, mean_q: 0.045525\n",
      " 6145/10000: episode: 26, duration: 12.266s, episode steps: 170, steps per second: 14, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.118 [0.000, 3.000], mean observation: 40.762 [0.000, 200.000], loss: 0.000697, mean_absolute_error: 0.010010, mean_q: 0.045680\n",
      " 6318/10000: episode: 27, duration: 12.427s, episode steps: 173, steps per second: 14, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.064 [0.000, 3.000], mean observation: 40.761 [0.000, 200.000], loss: 0.000864, mean_absolute_error: 0.010111, mean_q: 0.045765\n",
      " 6481/10000: episode: 28, duration: 9.806s, episode steps: 163, steps per second: 17, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.043 [0.000, 3.000], mean observation: 40.761 [0.000, 200.000], loss: 0.001106, mean_absolute_error: 0.010393, mean_q: 0.046378\n",
      " 6640/10000: episode: 29, duration: 9.937s, episode steps: 159, steps per second: 16, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.000 [0.000, 3.000], mean observation: 40.764 [0.000, 200.000], loss: 0.000446, mean_absolute_error: 0.010135, mean_q: 0.046506\n",
      " 6838/10000: episode: 30, duration: 12.117s, episode steps: 198, steps per second: 16, episode reward: 1.000, mean reward: 0.005 [0.000, 1.000], mean action: 1.045 [0.000, 3.000], mean observation: 40.647 [0.000, 200.000], loss: 0.000840, mean_absolute_error: 0.010467, mean_q: 0.046585\n",
      " 7190/10000: episode: 31, duration: 21.611s, episode steps: 352, steps per second: 16, episode reward: 4.000, mean reward: 0.011 [0.000, 1.000], mean action: 1.054 [0.000, 3.000], mean observation: 40.339 [0.000, 200.000], loss: 0.001911, mean_absolute_error: 0.011217, mean_q: 0.048362\n",
      " 7387/10000: episode: 32, duration: 14.597s, episode steps: 197, steps per second: 13, episode reward: 1.000, mean reward: 0.005 [0.000, 1.000], mean action: 1.091 [0.000, 3.000], mean observation: 40.586 [0.000, 200.000], loss: 0.001415, mean_absolute_error: 0.011334, mean_q: 0.050887\n",
      " 7587/10000: episode: 33, duration: 12.685s, episode steps: 200, steps per second: 16, episode reward: 1.000, mean reward: 0.005 [0.000, 1.000], mean action: 1.045 [0.000, 3.000], mean observation: 40.583 [0.000, 200.000], loss: 0.003406, mean_absolute_error: 0.012698, mean_q: 0.053052\n",
      " 7852/10000: episode: 34, duration: 16.302s, episode steps: 265, steps per second: 16, episode reward: 2.000, mean reward: 0.008 [0.000, 1.000], mean action: 1.023 [0.000, 3.000], mean observation: 40.563 [0.000, 200.000], loss: 0.002745, mean_absolute_error: 0.012634, mean_q: 0.056719\n",
      " 8017/10000: episode: 35, duration: 11.484s, episode steps: 165, steps per second: 14, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.042 [0.000, 3.000], mean observation: 40.745 [0.000, 200.000], loss: 0.002295, mean_absolute_error: 0.013069, mean_q: 0.060475\n",
      " 8182/10000: episode: 36, duration: 11.368s, episode steps: 165, steps per second: 15, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.073 [0.000, 3.000], mean observation: 40.763 [0.000, 200.000], loss: 0.002388, mean_absolute_error: 0.013257, mean_q: 0.062534\n",
      " 8346/10000: episode: 37, duration: 10.675s, episode steps: 164, steps per second: 15, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.024 [0.000, 3.000], mean observation: 40.766 [0.000, 200.000], loss: 0.002696, mean_absolute_error: 0.013643, mean_q: 0.065071\n",
      " 8514/10000: episode: 38, duration: 11.473s, episode steps: 168, steps per second: 15, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.024 [0.000, 3.000], mean observation: 40.764 [0.000, 200.000], loss: 0.002096, mean_absolute_error: 0.013802, mean_q: 0.066927\n",
      " 8717/10000: episode: 39, duration: 14.145s, episode steps: 203, steps per second: 14, episode reward: 1.000, mean reward: 0.005 [0.000, 1.000], mean action: 1.044 [0.000, 3.000], mean observation: 40.585 [0.000, 200.000], loss: 0.002530, mean_absolute_error: 0.014644, mean_q: 0.069164\n",
      " 8877/10000: episode: 40, duration: 11.270s, episode steps: 160, steps per second: 14, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.050 [0.000, 3.000], mean observation: 40.759 [0.000, 200.000], loss: 0.002102, mean_absolute_error: 0.015169, mean_q: 0.072201\n",
      " 9081/10000: episode: 41, duration: 13.216s, episode steps: 204, steps per second: 15, episode reward: 1.000, mean reward: 0.005 [0.000, 1.000], mean action: 1.152 [0.000, 3.000], mean observation: 40.518 [0.000, 200.000], loss: 0.002409, mean_absolute_error: 0.016280, mean_q: 0.074709\n",
      " 9251/10000: episode: 42, duration: 11.710s, episode steps: 170, steps per second: 15, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.047 [0.000, 3.000], mean observation: 40.766 [0.000, 200.000], loss: 0.002143, mean_absolute_error: 0.017005, mean_q: 0.076299\n",
      " 9513/10000: episode: 43, duration: 17.883s, episode steps: 262, steps per second: 15, episode reward: 2.000, mean reward: 0.008 [0.000, 1.000], mean action: 1.057 [0.000, 3.000], mean observation: 40.507 [0.000, 200.000], loss: 0.001473, mean_absolute_error: 0.018182, mean_q: 0.077670\n",
      " 9668/10000: episode: 44, duration: 10.193s, episode steps: 155, steps per second: 15, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.013 [0.000, 3.000], mean observation: 40.757 [0.000, 200.000], loss: 0.001977, mean_absolute_error: 0.019729, mean_q: 0.080026\n",
      " 9832/10000: episode: 45, duration: 10.534s, episode steps: 164, steps per second: 16, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.073 [0.000, 3.000], mean observation: 40.763 [0.000, 200.000], loss: 0.001210, mean_absolute_error: 0.020081, mean_q: 0.081743\n",
      " 9993/10000: episode: 46, duration: 9.722s, episode steps: 161, steps per second: 17, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.006 [0.000, 3.000], mean observation: 40.760 [0.000, 200.000], loss: 0.001203, mean_absolute_error: 0.020508, mean_q: 0.082194\n",
      "done, took 576.100 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9bc5c6eb38>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('policy.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 0.000, steps: 163\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9bfe25a470>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=1, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
