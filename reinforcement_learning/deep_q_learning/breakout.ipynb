{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Read the README for instructions to create the conda environment and\n",
    "create a ipykernel to use that environment in a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start off with all of our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/anaconda3/envs/deep_q/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Disable TensorFlow Warnings(Because I don't like seeing them)\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports for the whole notebook\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, GreedyQPolicy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this section, we're going to create both train and play environments and set some \"global\" (to this notebook) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 742738649]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATARI_ENV = 'Breakout-v0'\n",
    "train_env = gym.make(ATARI_ENV)\n",
    "np.random.seed(42)\n",
    "train_env.seed(42)\n",
    "\n",
    "play_env = gym.make(ATARI_ENV)\n",
    "np.random.seed(42)\n",
    "play_env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_actions = train_env.action_space.n\n",
    "nb_play_actions = play_env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've been playing with these values a lot during testing, so I wanted to\n",
    "# separate them for easy access\n",
    "nb_steps_fit = 750\n",
    "nb_steps_warmup = nb_steps_fit / 3\n",
    "update = nb_steps_warmup / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = Sequential()\n",
    "train_model.add(Flatten(input_shape=(3,) + train_env.observation_space.shape))\n",
    "train_model.add(Dense(64, activation='relu'))\n",
    "train_model.add(Dense(32, activation='relu'))\n",
    "train_model.add(Dense(16, activation='relu'))\n",
    "train_model.add(Dense(nb_train_actions, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000, window_length=3)\n",
    "policy = EpsGreedyQPolicy(eps=0.1)\n",
    "train_dqn = DQNAgent(model=train_model, nb_actions=nb_train_actions,\n",
    "                     memory=memory, nb_steps_warmup=nb_steps_warmup,\n",
    "                     target_model_update=update, policy=policy)\n",
    "train_dqn.compile(Adam(lr=1e-4), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 750 steps ...\n",
      " 444/750: episode: 1, duration: 57.234s, episode steps: 444, steps per second: 8, episode reward: 2.000, mean reward: 0.005 [0.000, 1.000], mean action: 0.385 [0.000, 3.000], mean observation: 40.593 [0.000, 200.000], loss: 3538.403526, mean_absolute_error: 82.915953, mean_q: 51.997073\n",
      " 713/750: episode: 2, duration: 73.242s, episode steps: 269, steps per second: 4, episode reward: 2.000, mean reward: 0.007 [0.000, 1.000], mean action: 0.922 [0.000, 3.000], mean observation: 40.459 [0.000, 200.000], loss: 51.082680, mean_absolute_error: 2.968939, mean_q: 4.790206\n",
      "done, took 141.053 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0faa60eda0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dqn.fit(train_env, nb_steps=nb_steps_fit, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dqn.save_weights('policy.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_model = Sequential()\n",
    "play_model.add(Flatten(input_shape=(3,) + play_env.observation_space.shape))\n",
    "play_model.add(Dense(64, activation='relu'))\n",
    "play_model.add(Dense(32, activation='relu'))\n",
    "play_model.add(Dense(16, activation='relu'))\n",
    "play_model.add(Dense(nb_play_actions, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000, window_length=3)\n",
    "policy = GreedyQPolicy()\n",
    "play_dqn = DQNAgent(model=play_model, nb_actions=nb_play_actions,\n",
    "                    memory=memory, nb_steps_warmup=nb_steps_warmup,\n",
    "                    target_model_update=update, policy=policy)\n",
    "play_dqn.compile(Adam(lr=1e-4), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_dqn.load_weights('./policy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 0.000, steps: 162\n",
      "Episode 2: reward: 0.000, steps: 165\n",
      "Episode 3: reward: 0.000, steps: 167\n",
      "Episode 4: reward: 0.000, steps: 161\n",
      "Episode 5: reward: 0.000, steps: 160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0f98363048>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_dqn.test(play_env, nb_episodes=5, visualize=True,\n",
    "              nb_max_episode_steps=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
