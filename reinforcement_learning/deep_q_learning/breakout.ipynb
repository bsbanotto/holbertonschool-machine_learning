{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Read the README for instructions to create the conda environment and\n",
    "create a ipykernel to use that environment in a Jupyter notebook.\n",
    "\n",
    "### DON'T RUN THE TRAINING LOCALLY. IT'LL TAKE TOO LONG\n",
    "[Here is a link to a colab notebook](https://colab.research.google.com/drive/1ov-ARfduhLPm-hUbw90GvUV28XLe6_w6?usp=sharing) if you want to train on one of their gpu's\n",
    "Then you can import the .h5 file and just play. The models are built the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start off with all of our imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/anaconda3/envs/deep_q/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Disable TensorFlow Warnings(Because I don't like seeing them)\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports for the whole notebook\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import EpsGreedyQPolicy, GreedyQPolicy, LinearAnnealedPolicy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, Reshape, MaxPooling2D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this section, we're going to create both train and play environments and set some \"global\" (to this notebook) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 742738649]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATARI_ENV = 'Breakout-v4'\n",
    "train_env = gym.make(ATARI_ENV)\n",
    "np.random.seed(42)\n",
    "train_env.seed(42)\n",
    "\n",
    "play_env = gym.make(ATARI_ENV)\n",
    "np.random.seed(42)\n",
    "play_env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_actions = train_env.action_space.n\n",
    "nb_play_actions = play_env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are basically all of the variables we can play with to try to get it\n",
    "# to learn better\n",
    "nb_steps_fit = 750\n",
    "nb_steps_warmup = nb_steps_fit / 3\n",
    "update = 0.1\n",
    "epsilon = 0.25\n",
    "learning_rate = 1e-3\n",
    "window_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_1 (Reshape)          (None, 210, 480, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 208, 478, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 104, 239, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 102, 237, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 51, 118, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 25, 59, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 94400)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               12083328  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 12,136,772\n",
      "Trainable params: 12,136,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_model = Sequential()\n",
    "'''\n",
    "There were some real shenanigans here. The train_dqn.fit was outputing\n",
    "a tensor of shape (1, 3, 210, 160, 3) and I had no idea what the 1 was\n",
    "The other shapes were:    3 = num images\n",
    "                          210 = image height\n",
    "                          160 = image width\n",
    "                          3 = color channels\n",
    "So, I attempted to stack by width to get it to a shape that the Conv2D layer\n",
    "would accept. Same thing for the play_model below.\n",
    "'''\n",
    "train_model.add(Reshape((210, 160 * window_size, 3), input_shape=(window_size, 210, 160, 3)))\n",
    "train_model.add(Conv2D(filters=(32), kernel_size=(3, 3), activation='relu'))\n",
    "train_model.add(MaxPooling2D((2, 2)))\n",
    "train_model.add(Conv2D(filters=(64), kernel_size=(3, 3), activation='relu'))\n",
    "train_model.add(MaxPooling2D((2, 2)))\n",
    "train_model.add(MaxPooling2D((2, 2)))\n",
    "# train_model.add(Conv2D(filters=(128), kernel_size=(3, 3), activation='relu'))\n",
    "train_model.add(Flatten())\n",
    "train_model.add(Dense(128, activation='relu'))\n",
    "train_model.add(Dense(256, activation='relu'))\n",
    "train_model.add(Dense(nb_train_actions, activation='linear'))\n",
    "\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=500, window_length=window_size)\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(eps=epsilon), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=100)\n",
    "train_dqn = DQNAgent(model=train_model, nb_actions=nb_train_actions,\n",
    "                     memory=memory, nb_steps_warmup=nb_steps_warmup,\n",
    "                     target_model_update=update, policy=policy)\n",
    "train_dqn.compile(Adam(lr=learning_rate), metrics=['mae'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 750 steps ...\n",
      " 354/750: episode: 1, duration: 268.033s, episode steps: 354, steps per second: 1, episode reward: 2.000, mean reward: 0.006 [0.000, 1.000], mean action: 1.771 [0.000, 3.000], mean observation: 40.430 [0.000, 200.000], loss: 3957.981125, mean_absolute_error: 61.421241, mean_q: -2.893220, mean_eps: 0.100000\n",
      " 667/750: episode: 2, duration: 786.773s, episode steps: 313, steps per second: 0, episode reward: 2.000, mean reward: 0.006 [0.000, 1.000], mean action: 1.380 [0.000, 3.000], mean observation: 40.527 [0.000, 200.000], loss: 1.242730, mean_absolute_error: 18.724601, mean_q: -23.159751, mean_eps: 0.100000\n",
      "done, took 1240.089 seconds\n"
     ]
    }
   ],
   "source": [
    "fit = train_dqn.fit(train_env, nb_steps=nb_steps_fit, visualize=False, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dqn.save_weights('policy.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_2 (Reshape)          (None, 210, 480, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 208, 478, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 104, 239, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 102, 237, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 51, 118, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 25, 59, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 94400)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               12083328  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 12,136,772\n",
      "Trainable params: 12,136,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "play_model = Sequential()\n",
    "\n",
    "play_model.add(Reshape((210, 160 * window_size, 3), input_shape=(window_size, 210, 160, 3)))\n",
    "play_model.add(Conv2D(filters=(32), kernel_size=(3, 3), activation='relu'))\n",
    "play_model.add(MaxPooling2D((2, 2)))\n",
    "play_model.add(Conv2D(filters=(64), kernel_size=(3, 3), activation='relu'))\n",
    "play_model.add(MaxPooling2D((2, 2)))\n",
    "play_model.add(MaxPooling2D((2, 2)))\n",
    "# play_model.add(Conv2D(filters=(128), kernel_size=(3, 3), activation='relu'))\n",
    "play_model.add(Flatten())\n",
    "play_model.add(Dense(128, activation='relu'))\n",
    "play_model.add(Dense(256, activation='relu'))\n",
    "play_model.add(Dense(nb_train_actions, activation='linear'))\n",
    "\n",
    "play_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000, window_length=window_size)\n",
    "policy = GreedyQPolicy()\n",
    "play_dqn = DQNAgent(model=play_model, nb_actions=nb_play_actions,\n",
    "                    memory=memory, nb_steps_warmup=nb_steps_warmup,\n",
    "                    target_model_update=update, policy=policy)\n",
    "play_dqn.compile(Adam(lr=learning_rate), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_dqn.load_weights('./policy.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 2.000, steps: 500\n",
      "Episode 2: reward: 0.000, steps: 500\n",
      "Episode 3: reward: 0.000, steps: 500\n",
      "Episode 4: reward: 0.000, steps: 500\n",
      "Episode 5: reward: 0.000, steps: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6f3c957a58>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_dqn.test(play_env, nb_episodes=5, visualize=True,\n",
    "              nb_max_episode_steps=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
