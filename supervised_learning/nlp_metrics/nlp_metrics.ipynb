{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0. Unigram BLEU score\n",
    "def uni_bleu(references, sentence):\n",
    "    \"\"\"\n",
    "    Calculates the unigram BLEU score for a sentence.\n",
    "\n",
    "    Args:\n",
    "        references: list of reference translations\n",
    "            - Each reference translataion is a list of the words in the\n",
    "              translation\n",
    "        sentence: list containing the model proposed sentence\n",
    "\n",
    "    Returns:\n",
    "        the unigram BLEU score\n",
    "\n",
    "    My Notes:\n",
    "        The final output is the brevity penalty multiplied by the number of\n",
    "        words from our machine translated sentence show up in our references\n",
    "        divided by the number of words in the machine translated sentence\n",
    "\n",
    "        Example:\n",
    "            machine_translation: \"there is a cat here\"\n",
    "            ref1: \"the cat is on the mat\"\n",
    "            ref2: \"there is a cat on the mat\"\n",
    "\n",
    "        brevity penalty if candidate is shorter than any reference, else 1:\n",
    "            e^(1-r/c)\n",
    "            r: length of reference sentence that is closest to length of\n",
    "                machine translated sentence\n",
    "            c: length of machine translated sentence\n",
    "\n",
    "        In our candidate: [there, is, a cat] are all in our references = 4\n",
    "        Candidate is 5 long\n",
    "\n",
    "        We finally end up with (4/5) * (e^(1-(6/5)))\n",
    "    \"\"\"\n",
    "    # Calculate the first number, P = m/w_t\n",
    "    # m = number of words from sentence in references\n",
    "    # w_t = number of words in translated sentence\n",
    "    w_t = len(sentence)\n",
    "    m = 0\n",
    "    corpus = []\n",
    "\n",
    "    for reference in references:\n",
    "        for word in sentence:\n",
    "            if word in reference and word not in corpus:\n",
    "                corpus.append(word)\n",
    "\n",
    "    m = len(corpus)\n",
    "    P = m / w_t\n",
    "\n",
    "    ref_len = min(len(reference) for reference in references)\n",
    "\n",
    "    BP = min(1, np.exp(1-(ref_len/w_t)))\n",
    "\n",
    "    return P * BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6549846024623855\n"
     ]
    }
   ],
   "source": [
    "# 0-main\n",
    "references = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]]\n",
    "sentence = [\"there\", \"is\", \"a\", \"cat\", \"here\"]\n",
    "\n",
    "print(uni_bleu(references, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1. N-gram BLEU score\n",
    "def ngram_bleu(references, sentence, n):\n",
    "    \"\"\"\n",
    "    Calculates the n-gram BLEU score for a sentence\n",
    "\n",
    "    Args:\n",
    "        references: list of reference translations\n",
    "            - Each reference translataion is a list of the words in the\n",
    "              translation\n",
    "        sentence: list containing the model proposed sentence\n",
    "        n: size of the n-gram to use for evaluation\n",
    "\n",
    "    Returns:\n",
    "        the unigram BLEU score\n",
    "\n",
    "    Notes:\n",
    "        Similar to unigram, except need to make a dictionaries of tuples to\n",
    "        find matches.\n",
    "    \"\"\"\n",
    "    # Calculate n-gram counts in the sentence (Create corpus of tuples)\n",
    "    corpus = {}\n",
    "    for i in range(len(sentence) - n + 1):\n",
    "        ngram = tuple(sentence[i:i + n])\n",
    "        corpus[ngram] = corpus.get(ngram, 0) + 1\n",
    "    print(\"Corpus:\", corpus)\n",
    "    w_t = len(corpus)\n",
    "\n",
    "    # Calculate maximum n-gram counts in the references\n",
    "    max_counts = {}\n",
    "    for reference in references:\n",
    "        ref_counts = {}\n",
    "        for i in range(len(reference) - n + 1):\n",
    "            ngram = tuple(reference[i:i + n])\n",
    "            ref_counts[ngram] = ref_counts.get(ngram, 0) + 1\n",
    "        for ngram, count in ref_counts.items():\n",
    "            max_counts[ngram] = max(max_counts.get(ngram, 0), count)\n",
    "    print(\"Max Counts:\", max_counts)\n",
    "    print(\"Length Max Counts:\", len(max_counts))\n",
    "\n",
    "    # Calculate clipped n-gram counts\n",
    "    clipped_counts = {}\n",
    "    for ngram, count in corpus.items():\n",
    "        clipped_counts[ngram] = min(count, max_counts.get(ngram, 0))\n",
    "    print(\"Clipped counts: \", clipped_counts)\n",
    "    print(\"Clipped Conts Length:\", len(clipped_counts))\n",
    "\n",
    "    m = sum(clipped_counts.values())\n",
    "\n",
    "    print(\"m:\", m)\n",
    "\n",
    "    P = m / w_t\n",
    "    print(\"P:\",P)\n",
    "\n",
    "    # Calculate brevity penalty\n",
    "    ref_len = min(len(reference) for reference in references)\n",
    "    c = len(sentence)\n",
    "    BP = min(1, np.exp(1 - ref_len / c))\n",
    "\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    library_n_gram = (sentence_bleu(references, sentence, weights=(0, 1, 0, 0)))\n",
    "    print(\"Library n-gram calculation: \", library_n_gram)\n",
    "\n",
    "    return P * BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: {('there', 'is'): 1, ('is', 'a'): 1, ('a', 'cat'): 1, ('cat', 'here'): 1}\n",
      "Max Counts: {('the', 'cat'): 1, ('cat', 'is'): 1, ('is', 'on'): 1, ('on', 'the'): 1, ('the', 'mat'): 1, ('there', 'is'): 1, ('is', 'a'): 1, ('a', 'cat'): 1, ('cat', 'on'): 1}\n",
      "Length Max Counts: 9\n",
      "Clipped counts:  {('there', 'is'): 1, ('is', 'a'): 1, ('a', 'cat'): 1, ('cat', 'here'): 0}\n",
      "Clipped Conts Length: 4\n",
      "m: 3\n",
      "P: 0.75\n",
      "Library n-gram calculation:  0.6140480648084865\n",
      "0.6140480648084865\n"
     ]
    }
   ],
   "source": [
    "# 1-main\n",
    "references = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]]\n",
    "sentence = [\"there\", \"is\", \"a\", \"cat\", \"here\"]\n",
    "\n",
    "print(ngram_bleu(references, sentence, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2. Cumulative N-gram BLEU score\n",
    "\"\"\"\n",
    "Calculate the cumulative n-gram BLEU score for a sentnece\n",
    "\"\"\"\n",
    "def cumulative_bleu(references, sentence, n):\n",
    "    \"\"\"\n",
    "    documentation\n",
    "    \"\"\"\n",
    "    n_gram_scores = []\n",
    "    for i in range(1, n + 1):\n",
    "        n_gram_scores.append(ngram_bleu(references, sentence, i))\n",
    "    return(gmean(n_gram_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: {('there',): 1, ('is',): 1, ('a',): 1, ('cat',): 1, ('here',): 1}\n",
      "Max Counts: {('the',): 2, ('cat',): 1, ('is',): 1, ('on',): 1, ('mat',): 1, ('there',): 1, ('a',): 1}\n",
      "Length Max Counts: 7\n",
      "Clipped counts:  {('there',): 1, ('is',): 1, ('a',): 1, ('cat',): 1, ('here',): 0}\n",
      "Clipped Conts Length: 5\n",
      "m: 4\n",
      "P: 0.8\n",
      "Library n-gram calculation:  0.6140480648084865\n",
      "Corpus: {('there', 'is'): 1, ('is', 'a'): 1, ('a', 'cat'): 1, ('cat', 'here'): 1}\n",
      "Max Counts: {('the', 'cat'): 1, ('cat', 'is'): 1, ('is', 'on'): 1, ('on', 'the'): 1, ('the', 'mat'): 1, ('there', 'is'): 1, ('is', 'a'): 1, ('a', 'cat'): 1, ('cat', 'on'): 1}\n",
      "Length Max Counts: 9\n",
      "Clipped counts:  {('there', 'is'): 1, ('is', 'a'): 1, ('a', 'cat'): 1, ('cat', 'here'): 0}\n",
      "Clipped Conts Length: 4\n",
      "m: 3\n",
      "P: 0.75\n",
      "Library n-gram calculation:  0.6140480648084865\n",
      "Corpus: {('there', 'is', 'a'): 1, ('is', 'a', 'cat'): 1, ('a', 'cat', 'here'): 1}\n",
      "Max Counts: {('the', 'cat', 'is'): 1, ('cat', 'is', 'on'): 1, ('is', 'on', 'the'): 1, ('on', 'the', 'mat'): 1, ('there', 'is', 'a'): 1, ('is', 'a', 'cat'): 1, ('a', 'cat', 'on'): 1, ('cat', 'on', 'the'): 1}\n",
      "Length Max Counts: 8\n",
      "Clipped counts:  {('there', 'is', 'a'): 1, ('is', 'a', 'cat'): 1, ('a', 'cat', 'here'): 0}\n",
      "Clipped Conts Length: 3\n",
      "m: 2\n",
      "P: 0.6666666666666666\n",
      "Library n-gram calculation:  0.6140480648084865\n",
      "Corpus: {('there', 'is', 'a', 'cat'): 1, ('is', 'a', 'cat', 'here'): 1}\n",
      "Max Counts: {('the', 'cat', 'is', 'on'): 1, ('cat', 'is', 'on', 'the'): 1, ('is', 'on', 'the', 'mat'): 1, ('there', 'is', 'a', 'cat'): 1, ('is', 'a', 'cat', 'on'): 1, ('a', 'cat', 'on', 'the'): 1, ('cat', 'on', 'the', 'mat'): 1}\n",
      "Length Max Counts: 7\n",
      "Clipped counts:  {('there', 'is', 'a', 'cat'): 1, ('is', 'a', 'cat', 'here'): 0}\n",
      "Clipped Conts Length: 2\n",
      "m: 1\n",
      "P: 0.5\n",
      "Library n-gram calculation:  0.6140480648084865\n",
      "0.5475182535069454\n"
     ]
    }
   ],
   "source": [
    "# 2-main\n",
    "references = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]]\n",
    "sentence = [\"there\", \"is\", \"a\", \"cat\", \"here\"]\n",
    "\n",
    "print(cumulative_bleu(references, sentence, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6549846024623855, 0.6140480648084865, 0.545820502051988, 0.40936537653899097]\n",
      "0.5475182535069454\n"
     ]
    }
   ],
   "source": [
    "# Testing out geometric mean\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "n_gram_1 = (sentence_bleu(references, sentence, weights=(1, 0, 0, 0)))\n",
    "n_gram_2 = (sentence_bleu(references, sentence, weights=(0, 1, 0, 0)))\n",
    "n_gram_3 = (sentence_bleu(references, sentence, weights=(0, 0, 1, 0)))\n",
    "n_gram_4 = (sentence_bleu(references, sentence, weights=(0, 0, 0, 1)))\n",
    "\n",
    "n_gram_scores = [n_gram_1, n_gram_2, n_gram_3, n_gram_4]\n",
    "print(n_gram_scores)\n",
    "\n",
    "import numpy as np\n",
    "def g_mean(xlist):\n",
    "    a = np.log(xlist)\n",
    "    return np.exp(a.mean())\n",
    "\n",
    "cumulative_ngram = g_mean(n_gram_scores)\n",
    "print (cumulative_ngram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
