{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from scipy.stats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0. Unigram BLEU score\n",
    "def uni_bleu(references, sentence):\n",
    "    \"\"\"\n",
    "    Calculates the unigram BLEU score for a sentence.\n",
    "\n",
    "    Args:\n",
    "        references: list of reference translations\n",
    "            - Each reference translataion is a list of the words in the\n",
    "              translation\n",
    "        sentence: list containing the model proposed sentence\n",
    "\n",
    "    Returns:\n",
    "        the unigram BLEU score\n",
    "\n",
    "    My Notes:\n",
    "        The final output is the brevity penalty multiplied by the number of\n",
    "        words from our machine translated sentence show up in our references\n",
    "        divided by the number of words in the machine translated sentence\n",
    "\n",
    "        Example:\n",
    "            machine_translation: \"there is a cat here\"\n",
    "            ref1: \"the cat is on the mat\"\n",
    "            ref2: \"there is a cat on the mat\"\n",
    "\n",
    "        brevity penalty if candidate is shorter than any reference, else 1:\n",
    "            e^(1-r/c)\n",
    "            r: length of reference sentence that is closest to length of\n",
    "                machine translated sentence\n",
    "            c: length of machine translated sentence\n",
    "\n",
    "        In our candidate: [there, is, a cat] are all in our references = 4\n",
    "        Candidate is 5 long\n",
    "\n",
    "        We finally end up with (4/5) * (e^(1-(6/5)))\n",
    "    \"\"\"\n",
    "    # Calculate the first number, P = m/w_t\n",
    "    # m = number of words from sentence in references\n",
    "    # w_t = number of words in translated sentence\n",
    "    w_t = len(sentence)\n",
    "    m = 0\n",
    "    corpus = []\n",
    "\n",
    "    for reference in references:\n",
    "        for word in sentence:\n",
    "            if word in reference and word not in corpus:\n",
    "                corpus.append(word)\n",
    "\n",
    "    m = len(corpus)\n",
    "    P = m / w_t\n",
    "\n",
    "    ref_len = min(len(reference) for reference in references)\n",
    "\n",
    "    BP = min(1, np.exp(1-(ref_len/w_t)))\n",
    "\n",
    "    return P * BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-main\n",
    "references = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]]\n",
    "sentence = [\"there\", \"is\", \"a\", \"cat\", \"here\"]\n",
    "\n",
    "print(uni_bleu(references, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1. N-gram BLEU score\n",
    "def ngram_bleu(references, sentence, n):\n",
    "    \"\"\"\n",
    "    Calculates the n-gram BLEU score for a sentence\n",
    "\n",
    "    Args:\n",
    "        references: list of reference translations\n",
    "            - Each reference translataion is a list of the words in the\n",
    "              translation\n",
    "        sentence: list containing the model proposed sentence\n",
    "        n: size of the n-gram to use for evaluation\n",
    "\n",
    "    Returns:\n",
    "        the unigram BLEU score\n",
    "\n",
    "    Notes:\n",
    "        Similar to unigram, except need to make a dictionaries of tuples to\n",
    "        find matches.\n",
    "    \"\"\"\n",
    "    # Calculate n-gram counts in the sentence (Create corpus of tuples)\n",
    "    corpus = {}\n",
    "    for i in range(len(sentence) - n + 1):\n",
    "        ngram = tuple(sentence[i:i + n])\n",
    "        corpus[ngram] = corpus.get(ngram, 0) + 1\n",
    "    # print(\"Corpus:\", corpus)\n",
    "    w_t = len(corpus)\n",
    "\n",
    "    # Calculate maximum n-gram counts in the references\n",
    "    max_counts = {}\n",
    "    for reference in references:\n",
    "        ref_counts = {}\n",
    "        for i in range(len(reference) - n + 1):\n",
    "            ngram = tuple(reference[i:i + n])\n",
    "            ref_counts[ngram] = ref_counts.get(ngram, 0) + 1\n",
    "        for ngram, count in ref_counts.items():\n",
    "            max_counts[ngram] = max(max_counts.get(ngram, 0), count)\n",
    "    # print(\"Max Counts:\", max_counts)\n",
    "    # print(\"Length Max Counts:\", len(max_counts))\n",
    "\n",
    "    # Calculate clipped n-gram counts\n",
    "    clipped_counts = {}\n",
    "    for ngram, count in corpus.items():\n",
    "        clipped_counts[ngram] = min(count, max_counts.get(ngram, 0))\n",
    "    # print(\"Clipped counts: \", clipped_counts)\n",
    "    # print(\"Clipped Conts Length:\", len(clipped_counts))\n",
    "\n",
    "    m = sum(clipped_counts.values())\n",
    "\n",
    "    # print(\"m:\", m)\n",
    "\n",
    "    P = m / w_t\n",
    "    # print(\"P:\",P)\n",
    "\n",
    "    # Calculate brevity penalty\n",
    "    ref_len = min(len(reference) for reference in references)\n",
    "    c = len(sentence)\n",
    "    BP = min(1, np.exp(1 - ref_len / c))\n",
    "\n",
    "    # from nltk.translate.bleu_score import sentence_bleu\n",
    "    # library_n_gram = (sentence_bleu(references, sentence, weights=(0, 1, 0, 0)))\n",
    "    # print(\"Library n-gram calculation: \", library_n_gram)\n",
    "\n",
    "    return P * BP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-main\n",
    "references = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]]\n",
    "sentence = [\"there\", \"is\", \"a\", \"cat\", \"here\"]\n",
    "\n",
    "print(ngram_bleu(references, sentence, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2. Cumulative N-gram BLEU score\n",
    "\"\"\"\n",
    "Calculate the cumulative n-gram BLEU score for a sentnece\n",
    "\"\"\"\n",
    "def cumulative_bleu(references, sentence, n):\n",
    "    \"\"\"\n",
    "    documentation\n",
    "    \"\"\"\n",
    "    n_gram_scores = []\n",
    "    for i in range(1, n + 1):\n",
    "        n_gram_scores.append(ngram_bleu(references, sentence, i))\n",
    "    return(gmean(n_gram_scores))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-main\n",
    "references = [[\"the\", \"cat\", \"is\", \"on\", \"the\", \"mat\"], [\"there\", \"is\", \"a\", \"cat\", \"on\", \"the\", \"mat\"]]\n",
    "sentence = [\"there\", \"is\", \"a\", \"cat\", \"here\"]\n",
    "\n",
    "print(cumulative_bleu(references, sentence, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 0-main - Perfect Precision and no brevity penalty\n",
    "references = [[\"hello\", \"there\"]]\n",
    "sentence = [\"hello\", \"there\"]\n",
    "\n",
    "print(np.round(cumulative_bleu(references, sentence, 2), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2 1-main Perfect precision but brevity penalty\n",
    "references = [[\"hello\", \"there\", \"my\", \"friend\"]]\n",
    "sentence = [\"hello\", \"there\"]\n",
    "\n",
    "print(np.round(cumulative_bleu(references, sentence, 2), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 2-main imperfect precision and previty penalty\n",
    "references = [[\"hello\", \"there\", \"my\", \"friend\"]]\n",
    "sentence = [\"hello\", \"there\", \"comrade\"]\n",
    "\n",
    "print(np.round(cumulative_bleu(references, sentence, 2), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 imperfect precision, no brevity penalty\n",
    "references = [[\"hello\", \"there\", \"my\" ,\"friend\"], [\"hello\", \"there\", \"buddy\", \"pal\"]]\n",
    "sentence = [\"hello\", \"there\", \"amigo\", \"friend\"]\n",
    "\n",
    "print(np.round(cumulative_bleu(references, sentence, 2), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out geometric mean\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "n_gram_1 = (sentence_bleu(references, sentence, weights=(1, 0, 0, 0)))\n",
    "n_gram_2 = (sentence_bleu(references, sentence, weights=(0, 1, 0, 0)))\n",
    "n_gram_3 = (sentence_bleu(references, sentence, weights=(0, 0, 1, 0)))\n",
    "n_gram_4 = (sentence_bleu(references, sentence, weights=(0, 0, 0, 1)))\n",
    "\n",
    "n_gram_scores = [n_gram_1, n_gram_2, n_gram_3, n_gram_4]\n",
    "print(n_gram_scores)\n",
    "\n",
    "import numpy as np\n",
    "def g_mean(xlist):\n",
    "    a = np.log(xlist)\n",
    "    return np.exp(a.mean())\n",
    "\n",
    "cumulative_ngram = g_mean(n_gram_scores)\n",
    "print (cumulative_ngram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
