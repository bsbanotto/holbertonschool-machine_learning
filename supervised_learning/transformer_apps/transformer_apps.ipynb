{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\t\"\"\"\n",
    "\tLoads and prepares a dataset for machine translation\n",
    "\t\"\"\"\n",
    "\tdef __init__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tClass constructor for Dataset class\n",
    "\t\t\"\"\"\n",
    "\t\tself.data_train = tfds.load('ted_hrlr_translate/pt_to_en',\n",
    "\t\t\t\t\t\t\t\t\tsplit='train',\n",
    "\t\t\t\t\t\t\t\t\tas_supervised=True)\n",
    "\t\tself.data_valid = tfds.load('ted_hrlr_translate/pt_to_en',\n",
    "\t\t\t\t\t\t\t\t\tsplit='validation',\n",
    "\t\t\t\t\t\t\t\t\tas_supervised=True)\n",
    "\t\tself.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(\n",
    "\t\t\tself.data_train\n",
    "\t\t\t)\n",
    "\n",
    "\tdef tokenize_dataset(self, data):\n",
    "\t\t\"\"\"\n",
    "\t\tCreates sub-word tokenizers for the dataset\n",
    "\t\tArgs:\n",
    "\t\t\tdata: tf.data.Dataset whose examples are formatted as a tuple\n",
    "\t\t\t\t\t(pt, en)\n",
    "\t\t\t\tpt: tf.Tensor containing the Portuguese sentence\n",
    "\t\t\t\ten: tf.Tensor containing the corresponding English sentence\n",
    "\t\tMaximum Vocab size should be set to 2**15\n",
    "\t\tReturns:\n",
    "\t\t\ttokenizer_pt: The Portuguese tokenizer\n",
    "\t\t\ttokenizer_en: The English tokenizer\n",
    "\t\t\"\"\"\n",
    "\t\ttoken_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "\t\t\t(pt.numpy() for pt, _ in data),\n",
    "\t\t\ttarget_vocab_size=2**15\n",
    "\t\t\t)\n",
    "\t\ttoken_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "\t\t\t(en.numpy() for _, en in data),\n",
    "\t\t\ttarget_vocab_size=2**15\n",
    "\t\t\t)\n",
    "\n",
    "\t\treturn token_pt, token_en\n",
    "\n",
    "\tdef encode(self, pt, en):\n",
    "\t\t\"\"\"\n",
    "\t\tEncodes a translation into tokens\n",
    "\t\tArgs:\n",
    "\t\t\tpt: the tf.Tensor containing the Portuguese sentence\n",
    "\t\t\ten: the tf.Tensore containing the corresponding English sentence\n",
    "\n",
    "\t\tThe tokenized sentences should contain the start end end tokens\n",
    "\t\tThe start token should be indexed as vocab_size\n",
    "\t\tThe end token should be indexed as vocab_size + 1\n",
    "\t\tReturns:\n",
    "\t\t\tpt_tokens: np.ndarray containing the Portuguese tokens\n",
    "\t\t\ten_tokens: np.ndarray containing the English tokens\n",
    "\t\t\"\"\"\n",
    "\t\tpt_start = [self.tokenizer_pt.vocab_size]\n",
    "\t\tpt_end = [self.tokenizer_pt.vocab_size + 1]\n",
    "\t\ten_start = [self.tokenizer_en.vocab_size]\n",
    "\t\ten_end = [self.tokenizer_en.vocab_size + 1]\n",
    "\n",
    "\t\tpt_tokens = pt_start + self.tokenizer_pt.encode(pt.numpy()) + pt_end\n",
    "\t\ten_tokens = en_start + self.tokenizer_en.encode(en.numpy()) + en_end\n",
    "\n",
    "\t\treturn pt_tokens, en_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "tinham comido peixe com batatas fritas ?\n",
      "did they eat fish and chips ?\n",
      "<class 'tensorflow_datasets.core.deprecated.text.subword_text_encoder.SubwordTextEncoder'>\n",
      "<class 'tensorflow_datasets.core.deprecated.text.subword_text_encoder.SubwordTextEncoder'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 15:19:29.449872: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Task 0 Main\n",
    "data = Dataset()\n",
    "for pt, en in data.data_train.take(1):\n",
    "    print(pt.numpy().decode('utf-8'))\n",
    "    print(en.numpy().decode('utf-8'))\n",
    "for pt, en in data.data_valid.take(1):\n",
    "    print(pt.numpy().decode('utf-8'))\n",
    "    print(en.numpy().decode('utf-8'))\n",
    "print(type(data.tokenizer_pt))\n",
    "print(type(data.tokenizer_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([30138, 6, 36, 17925, 13, 3, 3037, 1, 4880, 3, 387, 2832, 18, 18444, 1, 5, 8, 3, 16679, 19460, 739, 2, 30139], [28543, 4, 56, 15, 1266, 20397, 10721, 1, 15, 100, 125, 352, 3, 45, 3066, 6, 8004, 1, 88, 13, 14859, 2, 28544])\n",
      "([30138, 289, 15409, 2591, 19, 20318, 26024, 29997, 28, 30139], [28543, 93, 25, 907, 1366, 4, 5742, 33, 28544])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 15:21:50.957589: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# Task 1 Main\n",
    "data = Dataset()\n",
    "for pt, en in data.data_train.take(1):\n",
    "    print(data.encode(pt, en))\n",
    "for pt, en in data.data_valid.take(1):\n",
    "    print(data.encode(pt, en))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
