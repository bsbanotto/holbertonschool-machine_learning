{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 21:13:32.218354: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 21:13:33.350275: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-27 21:13:33.476466: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-27 21:13:33.476493: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-27 21:13:36.095608: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-27 21:13:36.095995: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-27 21:13:36.096014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bsbanotto/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 0. L2 Regularization Cost\n",
    "\"\"\"\n",
    "Function that calculates the cost of a neural network with L2 Regularization\n",
    "\"\"\"\n",
    "def l2_reg_cost(cost, lambtha, weights, L, m):\n",
    "    \"\"\"\n",
    "    cost: cost of the network without L2 regularization\n",
    "    lambtha: regularization parameter\n",
    "    weights: dictionary of the weights and biases of the neural network\n",
    "    L: number of layers in the neural network\n",
    "    m: number of data points used\n",
    "    Returns the cost of the network accounting for L2 regularization\n",
    "    \"\"\"\n",
    "    Frobenius_Norm = []\n",
    "    for key, value in weights.items():\n",
    "        if 'W' in key:\n",
    "            Frobenius_Norm.append(np.sum(value * value))\n",
    "    L2_norm = np.sum(Frobenius_Norm)\n",
    "    return cost + (lambtha / (2 * m)) * L2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41842822]\n",
      "[12.11229237]\n"
     ]
    }
   ],
   "source": [
    "# 0-main\n",
    "np.random.seed(0)\n",
    "\n",
    "weights = {}\n",
    "weights['W1'] = np.random.randn(256, 784)\n",
    "weights['W2'] = np.random.randn(128, 256)\n",
    "weights['W3'] = np.random.randn(10, 128)\n",
    "\n",
    "cost = np.abs(np.random.randn(1))\n",
    "\n",
    "print(cost)\n",
    "cost = l2_reg_cost(cost, 0.1, weights, 3, 1000)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1. Gradient Descent with L2 Regularization\n",
    "\"\"\"\n",
    "Function that updates the weights and biases of a neural network using\n",
    "gradient descent with L2 regularization\n",
    "\"\"\"\n",
    "def l2_reg_gradient_descent(Y, weights, cache, alpha, lambtha, L):\n",
    "    \"\"\"\n",
    "    Y: one-hot numpy.ndarray of shape (classes, m) that contains the correct\n",
    "    labels for the data\n",
    "        classes: number of classes\n",
    "        m: number of data points\n",
    "    weights: dictionary of the weights and biases of the neural network\n",
    "    cache: dictionary of the outputs of each layer of the neural network\n",
    "    alpha: learning rate\n",
    "    lambtha: L2 regularization parameter\n",
    "    L: number of layers of the network\n",
    "    The neural network uses tanh activations on each layer except the last\n",
    "    The last layer uses a softmax activation\n",
    "    The weights and biases of the network should be updated in place\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    for layer in range(L, 0, -1):\n",
    "        A_layer = cache[\"A{}\".format(layer)]\n",
    "        A_layer_prev = cache[\"A{}\".format(layer - 1)]\n",
    "\n",
    "        if layer == L:\n",
    "            dz_layer = (A_layer - Y)\n",
    "        else:\n",
    "            dz_layer = dA_layer_prev * (1 - np.square(A_layer))\n",
    "\n",
    "        W_layer = weights[\"W{}\".format(layer)]\n",
    "\n",
    "        dA_layer_prev = np.matmul(W_layer.T, dz_layer)\n",
    "\n",
    "        dW_layer = (np.matmul(dz_layer, A_layer_prev.T) / m) +\\\n",
    "        ((lambtha / m) * W_layer)\n",
    "        db_layer = np.sum(dz_layer, axis=1, keepdims=True) / m\n",
    "\n",
    "        weights[\"W{}\".format(layer)] -= alpha * dW_layer\n",
    "        weights[\"b{}\".format(layer)] -= alpha * db_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798 ...  0.52130375  0.61192719\n",
      "  -1.34149673]\n",
      " [ 0.47689837  0.14844958  0.52904524 ...  0.0960042  -0.0451133\n",
      "   0.07912172]\n",
      " [ 0.85053068 -0.83912419 -1.01177408 ... -0.07223876  0.31112445\n",
      "  -1.07836109]\n",
      " ...\n",
      " [-0.60467085  0.54751161 -1.23317415 ...  0.82895532  1.44161136\n",
      "   0.18972404]\n",
      " [-0.41044606  0.85719512  0.71789835 ... -0.73954771  0.5074628\n",
      "   1.23022874]\n",
      " [ 0.43129249  0.60767018 -0.07749988 ... -0.26611561  2.52287972\n",
      "   0.73131543]]\n",
      "[[ 1.76405199  0.40015713  0.97873779 ...  0.52130364  0.61192707\n",
      "  -1.34149646]\n",
      " [ 0.47689827  0.14844955  0.52904513 ...  0.09600419 -0.04511329\n",
      "   0.07912171]\n",
      " [ 0.85053051 -0.83912402 -1.01177388 ... -0.07223874  0.31112438\n",
      "  -1.07836088]\n",
      " ...\n",
      " [-0.60467073  0.5475115  -1.2331739  ...  0.82895516  1.44161107\n",
      "   0.189724  ]\n",
      " [-0.41044598  0.85719495  0.71789821 ... -0.73954756  0.5074627\n",
      "   1.2302285 ]\n",
      " [ 0.4312924   0.60767006 -0.07749987 ... -0.26611556  2.52287922\n",
      "   0.73131529]]\n"
     ]
    }
   ],
   "source": [
    "# 1-main\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((classes, m))\n",
    "    one_hot[Y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "lib= np.load('../data/MNIST.npz')\n",
    "X_train_3D = lib['X_train']\n",
    "Y_train = lib['Y_train']\n",
    "X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "weights = {}\n",
    "weights['W1'] = np.random.randn(256, 784)\n",
    "weights['b1'] = np.zeros((256, 1))\n",
    "weights['W2'] = np.random.randn(128, 256)\n",
    "weights['b2'] = np.zeros((128, 1))\n",
    "weights['W3'] = np.random.randn(10, 128)\n",
    "weights['b3'] = np.zeros((10, 1))\n",
    "\n",
    "cache = {}\n",
    "cache['A0'] = X_train\n",
    "cache['A1'] = np.tanh(np.matmul(weights['W1'], cache['A0']) + weights['b1'])\n",
    "cache['A2'] = np.tanh(np.matmul(weights['W2'], cache['A1']) + weights['b2'])\n",
    "Z3 = np.matmul(weights['W3'], cache['A2']) + weights['b3']\n",
    "cache['A3'] = np.exp(Z3) / np.sum(np.exp(Z3), axis=0)\n",
    "print(weights['W1'])\n",
    "l2_reg_gradient_descent(Y_train_oh, weights, cache, 0.1, 0.1, 3)\n",
    "print(weights['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2. L2 Regularization Cost\n",
    "\"\"\"\n",
    "Function that calculates the cost of a neural network with L2 regularization\n",
    "\"\"\"\n",
    "def l2_reg_cost(cost):\n",
    "    \"\"\"\n",
    "    cost: tensor containing the cost of the network without L2 reg\n",
    "    Return a tensor containing the cost of the network accounting for L2 reg\n",
    "    \"\"\"\n",
    "    return (cost + tf.losses.get_regularization_losses())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 21:13:49.665997: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-27 21:13:49.666806: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-27 21:13:49.667783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (BensLaptop): /proc/driver/nvidia/version does not exist\n",
      "2023-02-27 21:13:49.672903: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 21:13:49.709628: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56.608963  49.789898   5.9518623]\n"
     ]
    }
   ],
   "source": [
    "# 2-main\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    oh = np.zeros((classes, m))\n",
    "    oh[Y, np.arange(m)] = 1\n",
    "    return oh\n",
    "\n",
    "np.random.seed(4)\n",
    "m = np.random.randint(1000, 2000)\n",
    "c = 10\n",
    "lib= np.load('../data/MNIST.npz')\n",
    "\n",
    "X = lib['X_train'][:m].reshape((m, -1))\n",
    "Y = one_hot(lib['Y_train'][:m], c).T\n",
    "\n",
    "n0 = X.shape[1]\n",
    "n1, n2 = np.random.randint(10, 1000, 2)\n",
    "\n",
    "lam = np.random.uniform(0.01)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, n0))\n",
    "y = tf.placeholder(tf.float32, (None, c))\n",
    "\n",
    "a1 = tf.layers.Dense(n1, activation=tf.nn.tanh, kernel_initializer=tf.keras.initializers.VarianceScaling(mode=\"fan_avg\"), kernel_regularizer=tf.keras.regularizers.l2(lam))(x)\n",
    "a2 = tf.layers.Dense(n2, activation=tf.nn.sigmoid, kernel_initializer=tf.keras.initializers.VarianceScaling(mode=\"fan_avg\"), kernel_regularizer=tf.keras.regularizers.l2(lam))(a1)\n",
    "y_pred = tf.layers.Dense(c, activation=None, kernel_initializer=tf.keras.initializers.VarianceScaling(mode=\"fan_avg\"), kernel_regularizer=tf.keras.regularizers.l2(lam))(a2)\n",
    "\n",
    "cost = tf.losses.softmax_cross_entropy(y, y_pred)\n",
    "\n",
    "l2_cost = l2_reg_cost(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(l2_cost, feed_dict={x: X, y: Y}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3. Create a Layer with L2 Regularization\n",
    "\"\"\"\n",
    "Function that creates a tensorflow layer that includes L2 regularization\n",
    "\"\"\"\n",
    "def l2_reg_create_layer(prev, n, activation, lambtha):\n",
    "    \"\"\"\n",
    "    prev: tensor containing the output of the previous layer\n",
    "    n: number of nodes the new layer should contain\n",
    "    activation: activation function that should be used on the layer\n",
    "    lambtha: the L2 regularization parameter\n",
    "    Returns the output of the new layer\n",
    "    \"\"\"\n",
    "    l2_regular_layer = tf.layers.Dense(n,\n",
    "                                      activation=activation,\n",
    "                                      kernel_initializer=tf.keras.initializers.VarianceScaling(mode=\"fan_avg\"),\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l2(lambtha))\n",
    "    return l2_regular_layer(prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56.63227   49.813206   5.9751678 41.08217    2.4343293]\n"
     ]
    }
   ],
   "source": [
    "# 3-main\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((m, classes))\n",
    "    one_hot[np.arange(m), Y] = 1\n",
    "    return one_hot\n",
    "\n",
    "lib= np.load('../data/MNIST.npz')\n",
    "X_train_3D = lib['X_train']\n",
    "Y_train = lib['Y_train']\n",
    "X_train = X_train_3D.reshape((X_train_3D.shape[0], -1))\n",
    "Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "h1 = l2_reg_create_layer(x, 256, tf.nn.tanh, 0.1)\n",
    "y_pred = l2_reg_create_layer(x, 10, None, 0.)\n",
    "cost = tf.losses.softmax_cross_entropy(y, y_pred)\n",
    "l2_cost = l2_reg_cost(cost)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(l2_cost, feed_dict={x: X_train, y: Y_train_oh}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4. Forward Propagation with Dropout\n",
    "\"\"\"\n",
    "Function that conducts forward propagation using Dropout\n",
    "\"\"\"\n",
    "def dropout_forward_prop(X, weights, L, keep_prob):\n",
    "    \"\"\"\n",
    "    X: numpy.ndarray of shape (nx, m) containing the input data for the network\n",
    "        nx: number of input features\n",
    "        m: number of data points\n",
    "    weights: dictionary of the weights and biases of the neural network\n",
    "    L: number of layers in the network\n",
    "    keep_prob: the probability that a node will be kept\n",
    "    All layers except the last should use the tanh activation function\n",
    "    Last layer will use the softmax activation function\n",
    "    Returns a dictionary containing the outputs of each layer and the dropout\n",
    "        mask used on each layer\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    A = X\n",
    "    cache[\"A{}\".format(0)] = X\n",
    "    for layer in range(1, L + 1):\n",
    "        W = weights[\"W{}\".format(layer)]\n",
    "        b = weights[\"b{}\".format(layer)]\n",
    "        Z = np.matmul(W, A) + b\n",
    "        if layer == L:\n",
    "            A = np.exp(Z) / np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "            cache[\"A{}\".format(layer)] = A\n",
    "        else:\n",
    "            A = np.tanh(Z)\n",
    "            dropout_mask = np.random.binomial(n=1, p=keep_prob, size=A.shape)\n",
    "            A = A * dropout_mask / keep_prob\n",
    "            cache[\"A{}\".format(layer)] = A\n",
    "            cache[\"D{}\".format(layer)] = dropout_mask\n",
    "    return cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "A1 [[-1.24999999 -1.25       -1.24999945 ... -1.25       -1.25\n",
      "  -1.25      ]\n",
      " [ 1.25        1.24999777  1.25       ...  0.37738875  1.24999717\n",
      "  -1.24999889]\n",
      " [ 0.19383179 -0.80653094 -1.24950714 ...  1.24253535  1.08653948\n",
      "  -1.20190135]\n",
      " ...\n",
      " [-1.25       -1.25        0.         ... -0.         -1.25\n",
      "  -1.24999852]\n",
      " [-1.0858595  -1.25        0.         ...  1.24972487 -0.88878698\n",
      "  -1.24999933]\n",
      " [ 1.25        1.24999648  0.2057473  ...  0.          1.23194191\n",
      "  -1.24908257]]\n",
      "A2 [[-1.25        0.          1.24985922 ... -1.25        0.\n",
      "   1.24996854]\n",
      " [-0.         -0.         -0.         ... -1.24996232 -0.70684864\n",
      "   1.25      ]\n",
      " [-1.25        0.          0.18486152 ... -1.24999999 -1.25\n",
      "  -1.24999989]\n",
      " ...\n",
      " [ 1.2404131   1.25        1.25       ...  1.1670038   1.25\n",
      "  -0.        ]\n",
      " [ 1.25        1.25       -1.24998041 ...  1.2400913  -1.25\n",
      "   1.23620006]\n",
      " [ 0.93426582  1.25        1.25       ...  1.24999867 -1.25\n",
      "  -0.        ]]\n",
      "A3 [[9.13222086e-07 1.53352996e-09 4.02988574e-13 ... 2.93685964e-04\n",
      "  2.21615443e-11 7.95945899e-04]\n",
      " [4.10709405e-16 4.27810333e-11 7.38725096e-07 ... 2.05423847e-17\n",
      "  2.66482686e-09 1.74341031e-12]\n",
      " [9.82953561e-01 9.88655425e-01 9.73580864e-01 ... 1.14493065e-03\n",
      "  9.28074126e-10 1.92423905e-13]\n",
      " ...\n",
      " [3.03047424e-04 1.11981605e-02 4.72284535e-05 ... 1.25781567e-20\n",
      "  9.57462819e-01 3.33328605e-13]\n",
      " [3.20689297e-11 7.42324257e-08 5.62529910e-19 ... 2.05682936e-16\n",
      "  1.07622653e-12 1.41200115e-02]\n",
      " [5.06603174e-06 8.50852457e-11 5.51467429e-10 ... 9.98493133e-01\n",
      "  1.97896353e-14 2.38078250e-05]]\n",
      "D1 [[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 0 ... 0 1 1]\n",
      " [1 1 0 ... 1 1 1]\n",
      " [1 1 1 ... 0 1 1]]\n",
      "D2 [[1 0 1 ... 1 0 1]\n",
      " [0 0 0 ... 1 1 1]\n",
      " [1 0 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((classes, m))\n",
    "    one_hot[Y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "lib= np.load('../data/MNIST.npz')\n",
    "X_train_3D = lib['X_train']\n",
    "Y_train = lib['Y_train']\n",
    "X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "weights = {}\n",
    "weights['W1'] = np.random.randn(256, 784)\n",
    "weights['b1'] = np.zeros((256, 1))\n",
    "weights['W2'] = np.random.randn(128, 256)\n",
    "weights['b2'] = np.zeros((128, 1))\n",
    "weights['W3'] = np.random.randn(10, 128)\n",
    "weights['b3'] = np.zeros((10, 1))\n",
    "\n",
    "cache = dropout_forward_prop(X_train, weights, 3, 0.8)\n",
    "for k, v in sorted(cache.items()):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5. Gradient Descent with Dropout\n",
    "\"\"\"\n",
    "Function that updates the weights of a neural network with Dropout\n",
    "regularization using gradient descent\n",
    "\"\"\"\n",
    "def dropout_gradient_descent(Y, weights, cache, alpha, keep_prob, L):\n",
    "    \"\"\"\n",
    "    Y: one-hot numpy.ndarray of shape (classes, m) that contains data labels\n",
    "        classes: number of classes\n",
    "        m: number of data points\n",
    "    weights: dictionary of the weights and biases of the neural network\n",
    "    cache: dictionary of the outputs and dropout masks of each layer of the nn\n",
    "    alpha: learning rate\n",
    "    keep_prob: probability that a node will be kept\n",
    "    L: number of layers of the network\n",
    "    All layers except the last use the tanh activation function\n",
    "    The last layer uses the softmax activation function\n",
    "    The weights of the network should be updated in place\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    for layer in range(L, 0, -1):\n",
    "        A = cache[\"A{}\".format(layer)]\n",
    "        A_prev = cache[\"A{}\".format(layer - 1)]\n",
    "\n",
    "        if layer != L:\n",
    "            dZ = dA_prev * (1 - (A ** 2))\n",
    "            dZ = (dZ * cache[\"D{}\".format(layer)]) / keep_prob\n",
    "        else:\n",
    "            dZ = A - Y\n",
    "        W = weights[\"W{}\".format(layer)]\n",
    "        dW = np.matmul(dZ, A_prev.T) / m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "        dA_prev = np.matmul(W.T, dZ)\n",
    "\n",
    "        weights[\"W{}\".format(layer)] -= alpha * dW\n",
    "        weights[\"b{}\".format(layer)] -= alpha * db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.9282086  -0.71324613 -1.33191318 ... -2.14202626 -0.07737407\n",
      "   0.99832167]\n",
      " [-0.0237149  -0.18364778  0.08337452 ... -0.06093055 -0.03924408\n",
      "  -2.17625294]\n",
      " [-0.16181888  0.49237435 -0.47196279 ...  0.97504077  0.16272698\n",
      "   0.56159916]\n",
      " ...\n",
      " [ 0.39842474 -0.09870005  1.32173992 ... -0.33210834  0.66215988\n",
      "   0.87211421]\n",
      " [ 0.15767221  0.42236212  1.004765   ...  0.69883284  0.70857088\n",
      "  -0.44427252]\n",
      " [ 2.68588811 -0.60351958 -1.0759598  ... -1.2437044   0.69462324\n",
      "   1.00090403]]\n",
      "[[-1.92044686 -0.71894673 -1.32811693 ... -2.14071955 -0.07158198\n",
      "   0.98206832]\n",
      " [-0.03706116 -0.17088483  0.07798748 ... -0.07245569 -0.0491215\n",
      "  -2.16245276]\n",
      " [-0.17198668  0.49842244 -0.47369328 ...  0.96880194  0.15497217\n",
      "   0.5693131 ]\n",
      " ...\n",
      " [ 0.41997262 -0.11452751  1.32873227 ... -0.31312321  0.67162237\n",
      "   0.85928296]\n",
      " [ 0.13702353  0.44237056  1.00139188 ...  0.68128208  0.69020934\n",
      "  -0.43055442]\n",
      " [ 2.66514017 -0.59204122 -1.08943163 ... -1.26238074  0.69280683\n",
      "   1.02353101]]\n"
     ]
    }
   ],
   "source": [
    "# 5-main\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((classes, m))\n",
    "    one_hot[Y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "lib= np.load('../data/MNIST.npz')\n",
    "X_train_3D = lib['X_train']\n",
    "Y_train = lib['Y_train']\n",
    "X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "weights = {}\n",
    "weights['W1'] = np.random.randn(256, 784)\n",
    "weights['b1'] = np.zeros((256, 1))\n",
    "weights['W2'] = np.random.randn(128, 256)\n",
    "weights['b2'] = np.zeros((128, 1))\n",
    "weights['W3'] = np.random.randn(10, 128)\n",
    "weights['b3'] = np.zeros((10, 1))\n",
    "\n",
    "cache = dropout_forward_prop(X_train, weights, 3, 0.8)\n",
    "print(weights['W2'])\n",
    "dropout_gradient_descent(Y_train_oh, weights, cache, 0.1, 0.8, 3)\n",
    "print(weights['W2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6. Create a Layer with Dropout\n",
    "\"\"\"\n",
    "Function that creates a layer of a neural network using dropout\n",
    "\"\"\"\n",
    "def dropout_create_layer(prev, n, activation, keep_prob):\n",
    "    \"\"\"\n",
    "    prev: a tensor containing the output of the previous layer\n",
    "    n: number of nodes the new layer should contain\n",
    "    activation: the activation function that should be used on the layer\n",
    "    keep_prob: the probability that a node will be kept\n",
    "    Returns the output of the new layer\n",
    "    \"\"\"\n",
    "    init = tf.keras.initializers.VarianceScaling(mode=\"fan_avg\")\n",
    "    dropout_reg = tf.layers.Dropout(rate=keep_prob)\n",
    "    layer = tf.layers.dense(inputs=prev,\n",
    "                            units=n,\n",
    "                            activation=activation,\n",
    "                            kernel_initializer=init,\n",
    "                            kernel_regularizer=dropout_reg)\n",
    "    return layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.        -1.        -0.9997158 ...  1.        -1.        -1.       ]\n",
      " [ 1.        -1.         1.        ...  1.        -1.        -1.       ]\n",
      " [ 1.        -1.         1.        ...  1.        -1.        -1.       ]\n",
      " ...\n",
      " [ 1.        -1.         1.        ... -0.9998197 -1.        -1.       ]\n",
      " [ 1.        -1.        -1.        ... -1.        -1.         1.       ]\n",
      " [ 1.        -1.         1.        ...  1.        -1.        -1.       ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1870/1688624747.py:15: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  layer = tf.layers.dense(inputs=prev,\n"
     ]
    }
   ],
   "source": [
    "# 6-main\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "X = np.random.randint(0, 256, size=(10, 784))\n",
    "a = dropout_create_layer(x, 256, tf.nn.tanh, 0.8)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(a, feed_dict={x: X}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7. Early stopping\n",
    "\"\"\"\n",
    "Function that determines if gradient descent should stop early\n",
    "\"\"\"\n",
    "def early_stopping(cost, opt_cost, threshold, patience, count):\n",
    "    \"\"\"\n",
    "    cost: the current validation cost of the neural network\n",
    "    opt_cost: the lowest recorded validation cost of the network\n",
    "    threshold: the threshold used for early stopping\n",
    "    patience: the patience count used for early stopping\n",
    "    count: count of how long the threshold has not been met\n",
    "    Returns a boolean of whether the network should be stopped early\n",
    "        followed by the updated count\n",
    "    \"\"\"\n",
    "    cost_gap = opt_cost - cost\n",
    "\n",
    "    if cost_gap > threshold:\n",
    "        # Reset count because cost_gap is larger than threshold\n",
    "        count = 0\n",
    "    else:\n",
    "        # Add to count and wait patiently\n",
    "        count += 1\n",
    "\n",
    "    if count < patience:\n",
    "        # We don't want to stop because we haven't waited long enough\n",
    "        return(False, count)\n",
    "    else:\n",
    "        # We waited long enough and should stop early\n",
    "        return(True, count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, 0)\n",
      "(False, 3)\n",
      "(False, 9)\n",
      "(True, 15)\n"
     ]
    }
   ],
   "source": [
    "# 7-main\n",
    "print(early_stopping(1.0, 1.9, 0.5, 15, 5))\n",
    "print(early_stopping(1.1, 1.5, 0.5, 15, 2))\n",
    "print(early_stopping(1.0, 1.5, 0.5, 15, 8))\n",
    "print(early_stopping(1.0, 1.5, 0.5, 15, 14))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
