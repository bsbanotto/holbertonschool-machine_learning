{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow.compat.v1.keras as K\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Sequential\n",
    "\"\"\"\n",
    "Write a function that builds a neural network with the Keras library\n",
    "Use Sequential model since that's in the task name\n",
    "\"\"\"\n",
    "def build_model_0(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    nx: number of input features to the network\n",
    "    layers: list containing the number of nodes in each layer of the network\n",
    "    activations: list containing the activation functions for each layer\n",
    "    lambtha: L2 regularization parameter\n",
    "    keep_prob: probability that a node will be kept for dropout\n",
    "\n",
    "    Not allowed to use `Input` class\n",
    "    Returns the keras model\n",
    "    \"\"\"\n",
    "    model = K.Sequential()\n",
    "    # Create our input layer\n",
    "    model.add(K.layers.InputLayer(input_shape=(nx,)))\n",
    "    # Create all of our hidden layers\n",
    "    for layer in range(len(layers)):\n",
    "        model.add(K.layers.Dense(units=layers[layer],\n",
    "                                 activation=activations[layer],\n",
    "                                 kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                                 ))\n",
    "        # Handle dropout on hidden layers\n",
    "        if layer < len(layers) - 1:\n",
    "            model.add(K.layers.Dropout(1 - keep_prob))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-main\n",
    "network = build_model_0(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "network.summary()\n",
    "print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Sequential | Attempt 2\n",
    "\"\"\"\n",
    "Write a function that builds a neural network with the Keras library\n",
    "Use Sequential model since that's in the task name\n",
    "\"\"\"\n",
    "def build_model_1(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    nx: number of input features to the network\n",
    "    layers: list containing the number of nodes in each layer of the network\n",
    "    activations: list containing the activation functions for each layer\n",
    "    lambtha: L2 regularization parameter\n",
    "    keep_prob: probability that a node will be kept for dropout\n",
    "\n",
    "    Not allowed to use `Input` class\n",
    "    Returns the keras model\n",
    "    \"\"\"\n",
    "    model = K.Sequential()\n",
    "    # Create our input layer\n",
    "    model.add(K.layers.Dense(units=layers[0],\n",
    "                             activation = activations[0],\n",
    "                             kernel_regularizer=K.regularizers.l2(lambtha),\n",
    "                             input_shape=(nx,)))\n",
    "    # Create all of our hidden layers\n",
    "    for layer in range(1, len(layers)):\n",
    "        if layer <= len(layers) - 1:\n",
    "            model.add(K.layers.Dropout(1 - keep_prob))\n",
    "        model.add(K.layers.Dense(units=layers[layer],\n",
    "                                 activation=activations[layer],\n",
    "                                 kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                                 ))\n",
    "        # Handle dropout on hidden layers\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-main | Attempt 2\n",
    "network = build_model_1(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "network.summary()\n",
    "print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1. Input\n",
    "\"\"\"\n",
    "Write a function that builds a neural network with the Keras library\n",
    "\"\"\"\n",
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    nx: number of input features to the network\n",
    "    layers: list containing the number of nodes in each layer of the network\n",
    "    activations: list containing the activation functions for each layer\n",
    "    lambtha: L2 regularization parameter\n",
    "    keep_prob: probability that a node will be kept for dropout\n",
    "\n",
    "    Not allowed to use `Sequential` class\n",
    "    Returns the keras model\n",
    "    \"\"\"\n",
    "    inputs = K.Input(shape=(nx,))\n",
    "    x = K.layers.Dense(units=layers[0],\n",
    "                       activation=activations[0],\n",
    "                       kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                       )(inputs)\n",
    "    for layer in range(1, len(layers)):\n",
    "        dropout_layer = K.layers.Dropout(1 - keep_prob)(x)\n",
    "        x = K.layers.Dense(units=layers[layer],\n",
    "                           activation=activations[layer],\n",
    "                           kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                           )(dropout_layer)\n",
    "\n",
    "    model = K.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-main\n",
    "model = build_model(200, [10], ['tanh'], 0.01, 0.6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Optimize\n",
    "\"\"\"\n",
    "Write a function that sets up Adam optimization for a keras model with\n",
    "categorical crossentropy loss and accuracy metrics\n",
    "\"\"\"\n",
    "def optimize_model(network, alpha, beta1, beta2):\n",
    "    \"\"\"\n",
    "    network: the model to optimize\n",
    "    alpha: learning rate\n",
    "    beta1: the first Adam optimization parameter\n",
    "    beta2: second Adam optimization parameter\n",
    "    Returns none\n",
    "    \"\"\"\n",
    "\n",
    "    Adam_opt = K.optimizers.Adam(lr=alpha,\n",
    "                                 beta_1=beta1,\n",
    "                                 beta_2=beta2)\n",
    "    network.compile(optimizer=Adam_opt,\n",
    "                    metrics=['accuracy'],\n",
    "                    loss=\"categorical_crossentropy\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-main\n",
    "# model = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "# optimize_model(model, 0.01, 0.99, 0.9)\n",
    "# print(model.loss)\n",
    "# print(model.metrics)\n",
    "# opt = model.optimizer\n",
    "# print(opt.__class__)\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     print(sess.run((opt.lr, opt.beta_1, opt.beta_2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. One Hot\n",
    "\"\"\"\n",
    "Runction that converts a label vector into a one-hot matrix\n",
    "\"\"\"\n",
    "def one_hot(labels, classes=None):\n",
    "    \"\"\"\n",
    "    labels: labels for dat\n",
    "    \"\"\"\n",
    "    return K.utils.to_categorical(labels, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-main\n",
    "labels = np.load('../data/MNIST.npz')['Y_train'][:10]\n",
    "print(labels)\n",
    "print(one_hot(labels))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4. Train\n",
    "\"\"\"\n",
    "Write a function that trains a model using mini-batch gradient descent\n",
    "\"\"\"\n",
    "def train_model_4(network, data, labels, batch_size,\n",
    "                epochs, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    return network.fit(x=data,\n",
    "                       y=labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       shuffle=shuffle,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "train_model_4(network, X_train, Y_train_oh, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5. Validate\n",
    "\"\"\"\n",
    "Based on 4. Train, update the function def train_model to also analyze\n",
    "validation data\n",
    "\"\"\"\n",
    "def train_model_5(network, data, labels, batch_size, epochs,\n",
    "                  validation_data=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    validation_data: the data to validate the model with, if not `None`\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    return network.fit(x=data,\n",
    "                       y=labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       shuffle=shuffle,\n",
    "                       validation_data=validation_data\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "X_valid = datasets['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = datasets['Y_valid']\n",
    "Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "train_model_5(network, X_train, Y_train_oh, batch_size, epochs, validation_data=(X_valid, Y_valid_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6. Early Stopping\n",
    "\"\"\"\n",
    "Based on 5. Validate, update the function def train_model to also train the\n",
    "model using early stoping\n",
    "\"\"\"\n",
    "def train_model_6(network, data, labels, batch_size, epochs,\n",
    "                  validation_data=None, early_stopping=False, patience=0,\n",
    "                  verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    early_stopping: boolean that indicates whether or not to stop early\n",
    "        early stopping should only be performed if `validation_data` exists\n",
    "        early stopping should be based on validation loss\n",
    "    patience: patience used for early stopping\n",
    "    validation_data: the data to validate the model with, if not `None`\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    if validation_data is not None:\n",
    "        if early_stopping:\n",
    "            \"\"\"Create earlystop callback\"\"\"\n",
    "            earlystopping = K.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                      patience=patience)\n",
    "            return network.fit(x=data,\n",
    "                               y=labels,\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=epochs,\n",
    "                               verbose=verbose,\n",
    "                               shuffle=shuffle,\n",
    "                               validation_data=validation_data,\n",
    "                               callbacks=[earlystopping]\n",
    "                               )\n",
    "    else:\n",
    "        return network.fit(x=data,\n",
    "                           y=labels,\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=verbose,\n",
    "                           shuffle=shuffle,\n",
    "                           validation_data=validation_data,\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "X_valid = datasets['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = datasets['Y_valid']\n",
    "Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "train_model_6(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "            validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7. Learning Rate Decay\n",
    "\"\"\"\n",
    "Based on 6. Early Stopping, update the function def train_model to also train\n",
    "model with learning rate decay\n",
    "\"\"\"\n",
    "def train_model_7(network, data, labels, batch_size, epochs,\n",
    "                  validation_data=None, early_stopping=False, patience=0,\n",
    "                  learning_rate_decay=False, alpha=0.1, decay_rate=1,\n",
    "                  verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    learning_rate_decay: boolean that indicates whether or not learning rate\n",
    "    decay should be used\n",
    "        learning rate decay should only be performed if validation_data exists\n",
    "        the decay should be performed using inverse time decay\n",
    "        the learning rate should decay in a stepwise fashion after each epoch\n",
    "        each time the learning rate updates, Keras should print a message\n",
    "    early_stopping: boolean that indicates whether or not to stop early\n",
    "        early stopping should only be performed if `validation_data` exists\n",
    "        early stopping should be based on validation loss\n",
    "    patience: patience used for early stopping\n",
    "    validation_data: the data to validate the model with, if not `None`\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    callback_list = []\n",
    "\n",
    "    earlystopping = early_stopping\n",
    "    if earlystopping and validation_data is not None:\n",
    "        earlystopping = K.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                  patience=patience)\n",
    "        callback_list.append(earlystopping)\n",
    "\n",
    "    learningratedecay = learning_rate_decay\n",
    "    if learningratedecay and validation_data is not None:\n",
    "\n",
    "        def scheduler(epoch):\n",
    "            return (alpha / (1 + decay_rate * epoch))\n",
    "        learningratedecay = K.callbacks.LearningRateScheduler(scheduler,\n",
    "                                                              verbose=1)\n",
    "        callback_list.append(learningratedecay)\n",
    "\n",
    "    return network.fit(x=data,\n",
    "                        y=labels,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=verbose,\n",
    "                        shuffle=shuffle,\n",
    "                        validation_data=validation_data,\n",
    "                        callbacks=callback_list\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 7\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "# import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "lib = np.load('../data/MNIST.npz')\n",
    "X_train = lib['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = one_hot(lib['Y_train'], 10)\n",
    "X_valid = lib['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = one_hot(lib['Y_valid'], 10)\n",
    "model = build_model(784, [128, 64, 10], ['tanh', 'sigmoid', 'softmax'], 0.01, 0.6)\n",
    "optimize_model(model, 0.01, 0.99, 0.9)\n",
    "train_model_7(model, X_train, Y_train, 64, 5, validation_data=(X_valid, Y_valid), learning_rate_decay=True, alpha=0.01, decay_rate=2, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8. Save Only the Best\n",
    "\"\"\"\n",
    "Based on 7. Learning Rate Decay, update the function def train_model to also\n",
    "save the best iteration of the model\n",
    "\"\"\"\n",
    "def train_model_8(network, data, labels, batch_size, epochs,\n",
    "                  validation_data=None, early_stopping=False, patience=0,\n",
    "                  learning_rate_decay=False, alpha=0.1, decay_rate=1,\n",
    "                  save_best=False, filepath=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    save_best: boolean indicating whether to save the model after each epoch\n",
    "    if it is the best\n",
    "        A model is considered the best if its validation loss is the lowest\n",
    "        that the model has obtained\n",
    "    file_path: the file path to where the model should be saved\n",
    "    learning_rate_decay: boolean that indicates whether or not learning rate\n",
    "    decay should be used\n",
    "        learning rate decay should only be performed if validation_data exists\n",
    "        the decay should be performed using inverse time decay\n",
    "        the learning rate should decay in a stepwise fashion after each epoch\n",
    "        each time the learning rate updates, Keras should print a message\n",
    "    early_stopping: boolean that indicates whether or not to stop early\n",
    "        early stopping should only be performed if `validation_data` exists\n",
    "        early stopping should be based on validation loss\n",
    "    patience: patience used for early stopping\n",
    "    validation_data: the data to validate the model with, if not `None`\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    callback_list = []\n",
    "\n",
    "    earlystopping = early_stopping\n",
    "    if earlystopping and validation_data is not None:\n",
    "        earlystopping = K.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                  patience=patience)\n",
    "        callback_list.append(earlystopping)\n",
    "\n",
    "    learningratedecay = learning_rate_decay\n",
    "    if learningratedecay and validation_data is not None:\n",
    "\n",
    "        def scheduler(epoch):\n",
    "            return (alpha / (1 + decay_rate * epoch))\n",
    "        learningratedecay = K.callbacks.LearningRateScheduler(scheduler,\n",
    "                                                              verbose=1)\n",
    "        callback_list.append(learningratedecay)\n",
    "\n",
    "    savebest = save_best\n",
    "    if savebest:\n",
    "        savebest = K.callbacks.ModelCheckpoint(filepath, save_best_only=True)\n",
    "        callback_list.append(savebest)\n",
    "\n",
    "    return network.fit(x=data,\n",
    "                        y=labels,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=verbose,\n",
    "                        shuffle=shuffle,\n",
    "                        validation_data=validation_data,\n",
    "                        callbacks=callback_list\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "X_valid = datasets['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = datasets['Y_valid']\n",
    "Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "train_model_8(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "            validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "            patience=3, learning_rate_decay=True, alpha=alpha,\n",
    "            save_best=True, filepath='network1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9. Save and Load Model\n",
    "\"\"\"\n",
    "Write fnctions to save and load an entire model\n",
    "\"\"\"\n",
    "def save_model(network, filename):\n",
    "    \"\"\"\n",
    "    network: model to save\n",
    "    filename: path of the file that the model should be saved to\n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    network.save(filename)\n",
    "    return None\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    filename: path of the fale that the model should be loaded from\n",
    "    Returns the loaded model\n",
    "    \"\"\"\n",
    "    return K.models.load_model(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "# import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "X_valid = datasets['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = datasets['Y_valid']\n",
    "Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "network = load_model('network1.h5')\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "train_model_8(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "            validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "            patience=2, learning_rate_decay=True, alpha=0.001)\n",
    "save_model(network, 'network2.h5')\n",
    "network.summary()\n",
    "print(network.get_weights())\n",
    "del network\n",
    "\n",
    "network2 = load_model('network2.h5')\n",
    "network2.summary()\n",
    "print(network2.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 10. Save and Load Weights\n",
    "\"\"\"\n",
    "Write functions to save and load a models weights\n",
    "\"\"\"\n",
    "def save_weights(network, filename, save_format='h5'):\n",
    "    \"\"\"\n",
    "    network: model whose weights should be saved\n",
    "    filename: path where the weights should be saved to\n",
    "    save_format: format to which the weights should be saved\n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    network.save_weights(filepath=filename, save_format=save_format)\n",
    "def load_weights(network, filename):\n",
    "    \"\"\"\n",
    "    network: model whose weights should be loaded\n",
    "    filename: path to the file that the weights shoule be loaded from\n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    network.load_weights(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "network = load_model('network2.h5')\n",
    "save_weights(network, 'weights2.h5')\n",
    "del network\n",
    "\n",
    "network2 = load_model('network1.h5')\n",
    "print(network2.get_weights())\n",
    "load_weights(network2, 'weights2.h5')\n",
    "print(network2.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 11. Save and Load Configuration\n",
    "\"\"\"\n",
    "Write functions to save and load a models configuration in JSON format\n",
    "\"\"\"\n",
    "def save_config(network, filename):\n",
    "    \"\"\"\n",
    "    network: model whose configuration should be saved\n",
    "    filename: path of the file that the configuration should be saved to\n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    with open(filename, \"w\") as config_file:\n",
    "        config_file.write(network.to_json())\n",
    "\n",
    "def load_config(filename):\n",
    "    \"\"\"\n",
    "    filename: path of the file containing the model's configuration in JSON\n",
    "    Returns the loaded model\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\") as config_file:\n",
    "        network = config_file.read()\n",
    "    return K.models.model_from_json(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "network = load_model('network1.h5')\n",
    "save_config(network, 'config1.json')\n",
    "del network\n",
    "\n",
    "network2 = load_config('config1.json')\n",
    "network2.summary()\n",
    "print(network2.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 12. Test\n",
    "\"\"\"\n",
    "Write a function that tests a neural network\n",
    "\"\"\"\n",
    "def test_model(network, data, labels, verbose=True):\n",
    "    \"\"\"\n",
    "    network: network model to test\n",
    "    data: input data to test the model with\n",
    "    labels: correct one-hot labels of data\n",
    "    verbose: boolean that determines if output should be printed during\n",
    "    the testing process\n",
    "    Returns the loss and accuracy of the model with the testing data\n",
    "    respectively\n",
    "    \"\"\"\n",
    "    return network.evaluate(x=data, y=labels, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-main\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_test = datasets['X_test']\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "Y_test = datasets['Y_test']\n",
    "Y_test_oh = one_hot(Y_test)\n",
    "\n",
    "network = load_model('network2.h5')\n",
    "print(test_model(network, X_test, Y_test_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 13. Predict\n",
    "\"\"\"\n",
    "Write a function that makes a prediction using a neural network\n",
    "\"\"\"\n",
    "def predict(network, data, verbose=False):\n",
    "    \"\"\"\n",
    "    network: network model to make the prediction with\n",
    "    data: imput data to make the prediction with\n",
    "    verbose: boolean that determines if output should be printed during the\n",
    "    prediction process\n",
    "    Returns the prediction for the data\n",
    "    \"\"\"\n",
    "    return network.predict(x=data, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13-main\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_test = datasets['X_test']\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "Y_test = datasets['Y_test']\n",
    "\n",
    "network = load_model('network2.h5')\n",
    "Y_pred = predict(network, X_test)\n",
    "print(Y_pred)\n",
    "print(np.argmax(Y_pred, axis=1))\n",
    "print(Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
