{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow.compat.v1.keras as K\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Sequential\n",
    "\"\"\"\n",
    "Write a function that builds a neural network with the Keras library\n",
    "Use Sequential model since that's in the task name\n",
    "\"\"\"\n",
    "def build_model_0(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    nx: number of input features to the network\n",
    "    layers: list containing the number of nodes in each layer of the network\n",
    "    activations: list containing the activation functions for each layer\n",
    "    lambtha: L2 regularization parameter\n",
    "    keep_prob: probability that a node will be kept for dropout\n",
    "\n",
    "    Not allowed to use `Input` class\n",
    "    Returns the keras model\n",
    "    \"\"\"\n",
    "    model = K.Sequential()\n",
    "    # Create our input layer\n",
    "    model.add(K.layers.InputLayer(input_shape=(nx,)))\n",
    "    # Create all of our hidden layers\n",
    "    for layer in range(len(layers)):\n",
    "        model.add(K.layers.Dense(units=layers[layer],\n",
    "                                 activation=activations[layer],\n",
    "                                 kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                                 ))\n",
    "        # Handle dropout on hidden layers\n",
    "        if layer < len(layers) - 1:\n",
    "            model.add(K.layers.Dropout(1 - keep_prob))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_19 (Dense)            (None, 256)               200960    \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor 'dense_19/kernel/Regularizer/mul:0' shape=() dtype=float32>, <tf.Tensor 'dense_20/kernel/Regularizer/mul:0' shape=() dtype=float32>, <tf.Tensor 'dense_21/kernel/Regularizer/mul:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# 0-main\n",
    "network = build_model_0(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "network.summary()\n",
    "print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Sequential | Attempt 2\n",
    "\"\"\"\n",
    "Write a function that builds a neural network with the Keras library\n",
    "Use Sequential model since that's in the task name\n",
    "\"\"\"\n",
    "def build_model_1(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    nx: number of input features to the network\n",
    "    layers: list containing the number of nodes in each layer of the network\n",
    "    activations: list containing the activation functions for each layer\n",
    "    lambtha: L2 regularization parameter\n",
    "    keep_prob: probability that a node will be kept for dropout\n",
    "\n",
    "    Not allowed to use `Input` class\n",
    "    Returns the keras model\n",
    "    \"\"\"\n",
    "    model = K.Sequential()\n",
    "    # Create our input layer\n",
    "    model.add(K.layers.Dense(units=layers[0],\n",
    "                             activation = activations[0],\n",
    "                             kernel_regularizer=K.regularizers.l2(lambtha),\n",
    "                             input_shape=(nx,)))\n",
    "    # Create all of our hidden layers\n",
    "    for layer in range(1, len(layers)):\n",
    "        if layer <= len(layers) - 1:\n",
    "            model.add(K.layers.Dropout(1 - keep_prob))\n",
    "        model.add(K.layers.Dense(units=layers[layer],\n",
    "                                 activation=activations[layer],\n",
    "                                 kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                                 ))\n",
    "        # Handle dropout on hidden layers\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_22 (Dense)            (None, 256)               200960    \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor 'dense_22/kernel/Regularizer/mul:0' shape=() dtype=float32>, <tf.Tensor 'dense_23/kernel/Regularizer/mul:0' shape=() dtype=float32>, <tf.Tensor 'dense_24/kernel/Regularizer/mul:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# 0-main | Attempt 2\n",
    "network = build_model_1(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "network.summary()\n",
    "print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1. Input\n",
    "\"\"\"\n",
    "Write a function that builds a neural network with the Keras library\n",
    "\"\"\"\n",
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    nx: number of input features to the network\n",
    "    layers: list containing the number of nodes in each layer of the network\n",
    "    activations: list containing the activation functions for each layer\n",
    "    lambtha: L2 regularization parameter\n",
    "    keep_prob: probability that a node will be kept for dropout\n",
    "\n",
    "    Not allowed to use `Sequential` class\n",
    "    Returns the keras model\n",
    "    \"\"\"\n",
    "    inputs = K.Input(shape=(nx,))\n",
    "    x = K.layers.Dense(units=layers[0],\n",
    "                       activation=activations[0],\n",
    "                       kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                       )(inputs)\n",
    "    for layer in range(1, len(layers)):\n",
    "        dropout_layer = K.layers.Dropout(1 - keep_prob)(x)\n",
    "        x = K.layers.Dense(units=layers[layer],\n",
    "                           activation=activations[layer],\n",
    "                           kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                           )(dropout_layer)\n",
    "\n",
    "    model = K.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,010\n",
      "Trainable params: 2,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1-main\n",
    "model = build_model(200, [10], ['tanh'], 0.01, 0.6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Optimize\n",
    "\"\"\"\n",
    "Write a function that sets up Adam optimization for a keras model with\n",
    "categorical crossentropy loss and accuracy metrics\n",
    "\"\"\"\n",
    "def optimize_model(network, alpha, beta1, beta2):\n",
    "    \"\"\"\n",
    "    network: the model to optimize\n",
    "    alpha: learning rate\n",
    "    beta1: the first Adam optimization parameter\n",
    "    beta2: second Adam optimization parameter\n",
    "    Returns none\n",
    "    \"\"\"\n",
    "\n",
    "    Adam_opt = K.optimizers.Adam(lr=alpha,\n",
    "                                 beta_1=beta1,\n",
    "                                 beta_2=beta2)\n",
    "    network.compile(optimizer=Adam_opt,\n",
    "                    metrics=['accuracy'],\n",
    "                    loss=\"categorical_crossentropy\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-main\n",
    "# model = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "# optimize_model(model, 0.01, 0.99, 0.9)\n",
    "# print(model.loss)\n",
    "# print(model.metrics)\n",
    "# opt = model.optimizer\n",
    "# print(opt.__class__)\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     print(sess.run((opt.lr, opt.beta_1, opt.beta_2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. One Hot\n",
    "\"\"\"\n",
    "Runction that converts a label vector into a one-hot matrix\n",
    "\"\"\"\n",
    "def one_hot(labels, classes=None):\n",
    "    \"\"\"\n",
    "    labels: labels for dat\n",
    "    \"\"\"\n",
    "    return K.utils.to_categorical(labels, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 3-main\n",
    "labels = np.load('../data/MNIST.npz')['Y_train'][:10]\n",
    "print(labels)\n",
    "print(one_hot(labels))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4. Train\n",
    "\"\"\"\n",
    "Write a function that trains a model using mini-batch gradient descent\n",
    "\"\"\"\n",
    "def train_model_4(network, data, labels, batch_size,\n",
    "                epochs, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    return network.fit(x=data,\n",
    "                       y=labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       shuffle=shuffle,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "   64/50000 [..............................] - ETA: 1:07 - loss: 2.4673 - acc: 0.0625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 13:54:50.919426: W tensorflow/c/c_api.cc:291] Operation '{name:'training_6/Adam/dense_28/kernel/v/Assign' id:2556 op device:{requested: '', assigned: ''} def:{{{node training_6/Adam/dense_28/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_6/Adam/dense_28/kernel/v, training_6/Adam/dense_28/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 3s 64us/sample - loss: 0.3289 - acc: 0.9195\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 3s 60us/sample - loss: 0.1742 - acc: 0.9660\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 3s 63us/sample - loss: 0.1428 - acc: 0.9755\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 4s 72us/sample - loss: 0.1259 - acc: 0.9802\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 3s 64us/sample - loss: 0.1152 - acc: 0.9833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0ea8b8370>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "train_model_4(network, X_train, Y_train_oh, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5. Validate\n",
    "\"\"\"\n",
    "Based on 4. Train, update the function def train_model to also analyze\n",
    "validation data\n",
    "\"\"\"\n",
    "def train_model_5(network, data, labels, batch_size, epochs,\n",
    "                  validation_data=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    validation_data: the data to validate the model with, if not `None`\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    return network.fit(x=data,\n",
    "                       y=labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       shuffle=shuffle,\n",
    "                       validation_data=validation_data\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 13:55:08.188059: W tensorflow/c/c_api.cc:291] Operation '{name:'training_8/Adam/dense_31/kernel/m/Assign' id:3049 op device:{requested: '', assigned: ''} def:{{{node training_8/Adam/dense_31/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_8/Adam/dense_31/kernel/m, training_8/Adam/dense_31/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49728/50000 [============================>.] - ETA: 0s - loss: 0.3337 - acc: 0.9188"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/.local/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-03-06 13:55:11.599643: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_4/AddN_1' id:2853 op device:{requested: '', assigned: ''} def:{{{node loss_4/AddN_1}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_4/mul, loss_4/AddN)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 4s 72us/sample - loss: 0.3334 - acc: 0.9190 - val_loss: 0.1876 - val_acc: 0.9625\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 4s 72us/sample - loss: 0.1782 - acc: 0.9644 - val_loss: 0.1544 - val_acc: 0.9716\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 3s 61us/sample - loss: 0.1413 - acc: 0.9759 - val_loss: 0.1461 - val_acc: 0.9742\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 3s 61us/sample - loss: 0.1254 - acc: 0.9804 - val_loss: 0.1475 - val_acc: 0.9749\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 3s 63us/sample - loss: 0.1160 - acc: 0.9830 - val_loss: 0.1465 - val_acc: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0ea93b580>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "X_valid = datasets['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = datasets['Y_valid']\n",
    "Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "train_model_5(network, X_train, Y_train_oh, batch_size, epochs, validation_data=(X_valid, Y_valid_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6. Early Stopping\n",
    "\"\"\"\n",
    "Based on 5. Validate, update the function def train_model to also train the\n",
    "model using early stoping\n",
    "\"\"\"\n",
    "def train_model_6(network, data, labels, batch_size, epochs,\n",
    "                  validation_data=None, early_stopping=False, patience=0,\n",
    "                  verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    early_stopping: boolean that indicates whether or not to stop early\n",
    "        early stopping should only be performed if `validation_data` exists\n",
    "        early stopping should be based on validation loss\n",
    "    patience: patience used for early stopping\n",
    "    validation_data: the data to validate the model with, if not `None`\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    if validation_data is not None:\n",
    "        if early_stopping:\n",
    "            \"\"\"Create earlystop callback\"\"\"\n",
    "            earlystopping = K.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                      patience=patience)\n",
    "            return network.fit(x=data,\n",
    "                               y=labels,\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=epochs,\n",
    "                               verbose=verbose,\n",
    "                               shuffle=shuffle,\n",
    "                               validation_data=validation_data,\n",
    "                               callbacks=[earlystopping]\n",
    "                               )\n",
    "    else:\n",
    "        return network.fit(x=data,\n",
    "                           y=labels,\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=verbose,\n",
    "                           shuffle=shuffle,\n",
    "                           validation_data=validation_data,\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "   64/50000 [..............................] - ETA: 1:11 - loss: 2.4500 - acc: 0.1406"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 13:55:25.733795: W tensorflow/c/c_api.cc:291] Operation '{name:'training_10/Adam/learning_rate/Assign' id:3528 op device:{requested: '', assigned: ''} def:{{{node training_10/Adam/learning_rate/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_10/Adam/learning_rate, training_10/Adam/learning_rate/Initializer/initial_value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49344/50000 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.9202"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 13:55:29.449547: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_5/AddN_1' id:3363 op device:{requested: '', assigned: ''} def:{{{node loss_5/AddN_1}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_5/mul, loss_5/AddN)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 4s 79us/sample - loss: 0.3271 - acc: 0.9204 - val_loss: 0.1888 - val_acc: 0.9622\n",
      "Epoch 2/30\n",
      "50000/50000 [==============================] - 3s 64us/sample - loss: 0.1775 - acc: 0.9645 - val_loss: 0.1614 - val_acc: 0.9689\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 3s 63us/sample - loss: 0.1429 - acc: 0.9750 - val_loss: 0.1530 - val_acc: 0.9718\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 3s 65us/sample - loss: 0.1253 - acc: 0.9804 - val_loss: 0.1535 - val_acc: 0.9725\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 3s 65us/sample - loss: 0.1158 - acc: 0.9827 - val_loss: 0.1522 - val_acc: 0.9731\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 3s 62us/sample - loss: 0.1122 - acc: 0.9842 - val_loss: 0.1510 - val_acc: 0.9740\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 3s 63us/sample - loss: 0.1077 - acc: 0.9859 - val_loss: 0.1455 - val_acc: 0.9753\n",
      "Epoch 8/30\n",
      "50000/50000 [==============================] - 3s 62us/sample - loss: 0.1023 - acc: 0.9872 - val_loss: 0.1456 - val_acc: 0.9763\n",
      "Epoch 9/30\n",
      "50000/50000 [==============================] - 3s 63us/sample - loss: 0.0972 - acc: 0.9887 - val_loss: 0.1382 - val_acc: 0.9792\n",
      "Epoch 10/30\n",
      "50000/50000 [==============================] - 3s 62us/sample - loss: 0.0962 - acc: 0.9894 - val_loss: 0.1439 - val_acc: 0.9774\n",
      "Epoch 11/30\n",
      "50000/50000 [==============================] - 3s 64us/sample - loss: 0.0955 - acc: 0.9887 - val_loss: 0.1451 - val_acc: 0.9768\n",
      "Epoch 12/30\n",
      "50000/50000 [==============================] - 3s 66us/sample - loss: 0.0909 - acc: 0.9900 - val_loss: 0.1406 - val_acc: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0ea57c130>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "X_valid = datasets['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = datasets['Y_valid']\n",
    "Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "train_model_6(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "            validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7. Learning Rate Decay\n",
    "\"\"\"\n",
    "Based on 6. Early Stopping, update the function def train_model to also train\n",
    "model with learning rate decay\n",
    "\"\"\"\n",
    "def train_model_7(network, data, labels, batch_size, epochs,\n",
    "                  validation_data=None, early_stopping=False, patience=0,\n",
    "                  learning_rate_decay=False, alpha=0.1, decay_rate=1,\n",
    "                  verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    learning_rate_decay: boolean that indicates whether or not learning rate\n",
    "    decay should be used\n",
    "        learning rate decay should only be performed if validation_data exists\n",
    "        the decay should be performed using inverse time decay\n",
    "        the learning rate should decay in a stepwise fashion after each epoch\n",
    "        each time the learning rate updates, Keras should print a message\n",
    "    early_stopping: boolean that indicates whether or not to stop early\n",
    "        early stopping should only be performed if `validation_data` exists\n",
    "        early stopping should be based on validation loss\n",
    "    patience: patience used for early stopping\n",
    "    validation_data: the data to validate the model with, if not `None`\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    callback_list = []\n",
    "\n",
    "    earlystopping = early_stopping\n",
    "    if earlystopping and validation_data is not None:\n",
    "        earlystopping = K.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                  patience=patience)\n",
    "        callback_list.append(earlystopping)\n",
    "\n",
    "    learningratedecay = learning_rate_decay\n",
    "    if learningratedecay and validation_data is not None:\n",
    "\n",
    "        def scheduler(epoch):\n",
    "            return (alpha / (1 + decay_rate * epoch))\n",
    "        learningratedecay = K.callbacks.LearningRateScheduler(scheduler,\n",
    "                                                              verbose=1)\n",
    "        callback_list.append(learningratedecay)\n",
    "\n",
    "    return network.fit(x=data,\n",
    "                        y=labels,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=verbose,\n",
    "                        shuffle=shuffle,\n",
    "                        validation_data=validation_data,\n",
    "                        callbacks=callback_list\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.01.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 13:56:05.631122: W tensorflow/c/c_api.cc:291] Operation '{name:'training_12/Adam/dense_35/kernel/v/Assign' id:4079 op device:{requested: '', assigned: ''} def:{{{node training_12/Adam/dense_35/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_12/Adam/dense_35/kernel/v, training_12/Adam/dense_35/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-03-06 13:56:07.821622: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_6/AddN_1' id:3873 op device:{requested: '', assigned: ''} def:{{{node loss_6/AddN_1}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_6/mul, loss_6/AddN)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0033333333333333335.\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.002.\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0014285714285714286.\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0011111111111111111.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0ea46d310>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 7\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "# import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "lib = np.load('../data/MNIST.npz')\n",
    "X_train = lib['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = one_hot(lib['Y_train'], 10)\n",
    "X_valid = lib['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = one_hot(lib['Y_valid'], 10)\n",
    "model = build_model(784, [128, 64, 10], ['tanh', 'sigmoid', 'softmax'], 0.01, 0.6)\n",
    "optimize_model(model, 0.01, 0.99, 0.9)\n",
    "train_model_7(model, X_train, Y_train, 64, 5, validation_data=(X_valid, Y_valid), learning_rate_decay=True, alpha=0.01, decay_rate=2, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8. Save Only the Best\n",
    "\"\"\"\n",
    "Based on 7. Learning Rate Decay, update the function def train_model to also\n",
    "save the best iteration of the model\n",
    "\"\"\"\n",
    "def train_model_8(network, data, labels, batch_size, epochs,\n",
    "                  validation_data=None, early_stopping=False, patience=0,\n",
    "                  learning_rate_decay=False, alpha=0.1, decay_rate=1,\n",
    "                  save_best=False, filepath=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    save_best: boolean indicating whether to save the model after each epoch\n",
    "    if it is the best\n",
    "        A model is considered the best if its validation loss is the lowest\n",
    "        that the model has obtained\n",
    "    file_path: the file path to where the model should be saved\n",
    "    learning_rate_decay: boolean that indicates whether or not learning rate\n",
    "    decay should be used\n",
    "        learning rate decay should only be performed if validation_data exists\n",
    "        the decay should be performed using inverse time decay\n",
    "        the learning rate should decay in a stepwise fashion after each epoch\n",
    "        each time the learning rate updates, Keras should print a message\n",
    "    early_stopping: boolean that indicates whether or not to stop early\n",
    "        early stopping should only be performed if `validation_data` exists\n",
    "        early stopping should be based on validation loss\n",
    "    patience: patience used for early stopping\n",
    "    validation_data: the data to validate the model with, if not `None`\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    callback_list = []\n",
    "\n",
    "    earlystopping = early_stopping\n",
    "    if earlystopping and validation_data is not None:\n",
    "        earlystopping = K.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                  patience=patience)\n",
    "        callback_list.append(earlystopping)\n",
    "\n",
    "    learningratedecay = learning_rate_decay\n",
    "    if learningratedecay and validation_data is not None:\n",
    "\n",
    "        def scheduler(epoch):\n",
    "            return (alpha / (1 + decay_rate * epoch))\n",
    "        learningratedecay = K.callbacks.LearningRateScheduler(scheduler,\n",
    "                                                              verbose=1)\n",
    "        callback_list.append(learningratedecay)\n",
    "\n",
    "    savebest = save_best\n",
    "    if savebest:\n",
    "        savebest = K.callbacks.ModelCheckpoint(filepath, save_best_only=True)\n",
    "        callback_list.append(savebest)\n",
    "\n",
    "    return network.fit(x=data,\n",
    "                        y=labels,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=verbose,\n",
    "                        shuffle=shuffle,\n",
    "                        validation_data=validation_data,\n",
    "                        callbacks=callback_list\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 13:56:19.939197: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_40/kernel/Assign' id:4269 op device:{requested: '', assigned: ''} def:{{{node dense_40/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_40/kernel, dense_40/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/1000\n",
      "49984/50000 [============================>.] - ETA: 0s - loss: 0.3303 - acc: 0.9200"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 13:56:23.990552: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_7/AddN_1' id:4382 op device:{requested: '', assigned: ''} def:{{{node loss_7/AddN_1}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_7/mul, loss_7/AddN)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 5s 101us/sample - loss: 0.3303 - acc: 0.9200 - val_loss: 0.1893 - val_acc: 0.9626 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 2/1000\n",
      "50000/50000 [==============================] - 5s 93us/sample - loss: 0.1619 - acc: 0.9689 - val_loss: 0.1496 - val_acc: 0.9716 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0003333333333333333.\n",
      "Epoch 3/1000\n",
      "50000/50000 [==============================] - 4s 73us/sample - loss: 0.1270 - acc: 0.9794 - val_loss: 0.1366 - val_acc: 0.9756 - lr: 3.3333e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 4/1000\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.1101 - acc: 0.9840 - val_loss: 0.1307 - val_acc: 0.9765 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 5/1000\n",
      "50000/50000 [==============================] - 3s 65us/sample - loss: 0.0998 - acc: 0.9873 - val_loss: 0.1280 - val_acc: 0.9776 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.00016666666666666666.\n",
      "Epoch 6/1000\n",
      "50000/50000 [==============================] - 4s 86us/sample - loss: 0.0921 - acc: 0.9894 - val_loss: 0.1248 - val_acc: 0.9771 - lr: 1.6667e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.00014285714285714287.\n",
      "Epoch 7/1000\n",
      "50000/50000 [==============================] - 3s 69us/sample - loss: 0.0863 - acc: 0.9912 - val_loss: 0.1219 - val_acc: 0.9788 - lr: 1.4286e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.000125.\n",
      "Epoch 8/1000\n",
      "50000/50000 [==============================] - 4s 71us/sample - loss: 0.0825 - acc: 0.9924 - val_loss: 0.1208 - val_acc: 0.9785 - lr: 1.2500e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.00011111111111111112.\n",
      "Epoch 9/1000\n",
      "50000/50000 [==============================] - 4s 75us/sample - loss: 0.0789 - acc: 0.9933 - val_loss: 0.1176 - val_acc: 0.9798 - lr: 1.1111e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 10/1000\n",
      "50000/50000 [==============================] - 3s 65us/sample - loss: 0.0758 - acc: 0.9938 - val_loss: 0.1167 - val_acc: 0.9796 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 9.090909090909092e-05.\n",
      "Epoch 11/1000\n",
      "50000/50000 [==============================] - 3s 67us/sample - loss: 0.0737 - acc: 0.9942 - val_loss: 0.1156 - val_acc: 0.9801 - lr: 9.0909e-05\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 8.333333333333333e-05.\n",
      "Epoch 12/1000\n",
      "50000/50000 [==============================] - 4s 84us/sample - loss: 0.0709 - acc: 0.9953 - val_loss: 0.1149 - val_acc: 0.9803 - lr: 8.3333e-05\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 7.692307692307693e-05.\n",
      "Epoch 13/1000\n",
      "50000/50000 [==============================] - 3s 69us/sample - loss: 0.0690 - acc: 0.9957 - val_loss: 0.1142 - val_acc: 0.9809 - lr: 7.6923e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 7.142857142857143e-05.\n",
      "Epoch 14/1000\n",
      "50000/50000 [==============================] - 3s 66us/sample - loss: 0.0681 - acc: 0.9957 - val_loss: 0.1125 - val_acc: 0.9807 - lr: 7.1429e-05\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 6.666666666666667e-05.\n",
      "Epoch 15/1000\n",
      "50000/50000 [==============================] - 3s 67us/sample - loss: 0.0665 - acc: 0.9963 - val_loss: 0.1117 - val_acc: 0.9808 - lr: 6.6667e-05\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 6.25e-05.\n",
      "Epoch 16/1000\n",
      "50000/50000 [==============================] - 4s 75us/sample - loss: 0.0651 - acc: 0.9965 - val_loss: 0.1105 - val_acc: 0.9809 - lr: 6.2500e-05\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 5.882352941176471e-05.\n",
      "Epoch 17/1000\n",
      "50000/50000 [==============================] - 4s 80us/sample - loss: 0.0638 - acc: 0.9969 - val_loss: 0.1099 - val_acc: 0.9812 - lr: 5.8824e-05\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 5.555555555555556e-05.\n",
      "Epoch 18/1000\n",
      "50000/50000 [==============================] - 3s 67us/sample - loss: 0.0625 - acc: 0.9971 - val_loss: 0.1096 - val_acc: 0.9813 - lr: 5.5556e-05\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 5.2631578947368424e-05.\n",
      "Epoch 19/1000\n",
      "50000/50000 [==============================] - 4s 71us/sample - loss: 0.0616 - acc: 0.9971 - val_loss: 0.1094 - val_acc: 0.9810 - lr: 5.2632e-05\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 20/1000\n",
      "50000/50000 [==============================] - 3s 66us/sample - loss: 0.0606 - acc: 0.9973 - val_loss: 0.1100 - val_acc: 0.9808 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 4.761904761904762e-05.\n",
      "Epoch 21/1000\n",
      "50000/50000 [==============================] - 4s 72us/sample - loss: 0.0605 - acc: 0.9972 - val_loss: 0.1091 - val_acc: 0.9811 - lr: 4.7619e-05\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 4.545454545454546e-05.\n",
      "Epoch 22/1000\n",
      "50000/50000 [==============================] - 4s 75us/sample - loss: 0.0594 - acc: 0.9974 - val_loss: 0.1081 - val_acc: 0.9812 - lr: 4.5455e-05\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 4.347826086956522e-05.\n",
      "Epoch 23/1000\n",
      "50000/50000 [==============================] - 4s 72us/sample - loss: 0.0584 - acc: 0.9978 - val_loss: 0.1080 - val_acc: 0.9818 - lr: 4.3478e-05\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 4.1666666666666665e-05.\n",
      "Epoch 24/1000\n",
      "50000/50000 [==============================] - 4s 73us/sample - loss: 0.0579 - acc: 0.9977 - val_loss: 0.1072 - val_acc: 0.9823 - lr: 4.1667e-05\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 4e-05.\n",
      "Epoch 25/1000\n",
      "50000/50000 [==============================] - 4s 72us/sample - loss: 0.0573 - acc: 0.9978 - val_loss: 0.1070 - val_acc: 0.9815 - lr: 4.0000e-05\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 3.846153846153846e-05.\n",
      "Epoch 26/1000\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.0566 - acc: 0.9980 - val_loss: 0.1054 - val_acc: 0.9819 - lr: 3.8462e-05\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 3.7037037037037037e-05.\n",
      "Epoch 27/1000\n",
      "50000/50000 [==============================] - 4s 77us/sample - loss: 0.0563 - acc: 0.9981 - val_loss: 0.1054 - val_acc: 0.9818 - lr: 3.7037e-05\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 3.571428571428572e-05.\n",
      "Epoch 28/1000\n",
      "50000/50000 [==============================] - 4s 77us/sample - loss: 0.0556 - acc: 0.9981 - val_loss: 0.1057 - val_acc: 0.9818 - lr: 3.5714e-05\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 3.4482758620689657e-05.\n",
      "Epoch 29/1000\n",
      "50000/50000 [==============================] - 4s 71us/sample - loss: 0.0553 - acc: 0.9983 - val_loss: 0.1046 - val_acc: 0.9824 - lr: 3.4483e-05\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 3.3333333333333335e-05.\n",
      "Epoch 30/1000\n",
      "50000/50000 [==============================] - 3s 69us/sample - loss: 0.0546 - acc: 0.9984 - val_loss: 0.1046 - val_acc: 0.9821 - lr: 3.3333e-05\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 3.2258064516129034e-05.\n",
      "Epoch 31/1000\n",
      "50000/50000 [==============================] - 3s 65us/sample - loss: 0.0543 - acc: 0.9982 - val_loss: 0.1037 - val_acc: 0.9823 - lr: 3.2258e-05\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 3.125e-05.\n",
      "Epoch 32/1000\n",
      "50000/50000 [==============================] - 4s 71us/sample - loss: 0.0541 - acc: 0.9981 - val_loss: 0.1037 - val_acc: 0.9819 - lr: 3.1250e-05\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 3.0303030303030302e-05.\n",
      "Epoch 33/1000\n",
      "50000/50000 [==============================] - 4s 74us/sample - loss: 0.0535 - acc: 0.9983 - val_loss: 0.1042 - val_acc: 0.9818 - lr: 3.0303e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 2.9411764705882354e-05.\n",
      "Epoch 34/1000\n",
      "50000/50000 [==============================] - 3s 69us/sample - loss: 0.0533 - acc: 0.9981 - val_loss: 0.1026 - val_acc: 0.9823 - lr: 2.9412e-05\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 2.857142857142857e-05.\n",
      "Epoch 35/1000\n",
      "50000/50000 [==============================] - 3s 65us/sample - loss: 0.0531 - acc: 0.9982 - val_loss: 0.1027 - val_acc: 0.9827 - lr: 2.8571e-05\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 2.777777777777778e-05.\n",
      "Epoch 36/1000\n",
      "50000/50000 [==============================] - 3s 67us/sample - loss: 0.0523 - acc: 0.9985 - val_loss: 0.1024 - val_acc: 0.9826 - lr: 2.7778e-05\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 2.7027027027027027e-05.\n",
      "Epoch 37/1000\n",
      "50000/50000 [==============================] - 4s 71us/sample - loss: 0.0521 - acc: 0.9988 - val_loss: 0.1020 - val_acc: 0.9822 - lr: 2.7027e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 2.6315789473684212e-05.\n",
      "Epoch 38/1000\n",
      "50000/50000 [==============================] - 4s 71us/sample - loss: 0.0515 - acc: 0.9989 - val_loss: 0.1018 - val_acc: 0.9819 - lr: 2.6316e-05\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 2.5641025641025643e-05.\n",
      "Epoch 39/1000\n",
      "50000/50000 [==============================] - 4s 72us/sample - loss: 0.0517 - acc: 0.9986 - val_loss: 0.1013 - val_acc: 0.9823 - lr: 2.5641e-05\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 2.5e-05.\n",
      "Epoch 40/1000\n",
      "50000/50000 [==============================] - 3s 70us/sample - loss: 0.0512 - acc: 0.9985 - val_loss: 0.1019 - val_acc: 0.9822 - lr: 2.5000e-05\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 2.4390243902439026e-05.\n",
      "Epoch 41/1000\n",
      "50000/50000 [==============================] - 3s 70us/sample - loss: 0.0507 - acc: 0.9988 - val_loss: 0.1012 - val_acc: 0.9822 - lr: 2.4390e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 2.380952380952381e-05.\n",
      "Epoch 42/1000\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.0508 - acc: 0.9988 - val_loss: 0.1008 - val_acc: 0.9826 - lr: 2.3810e-05\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 2.3255813953488374e-05.\n",
      "Epoch 43/1000\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.0507 - acc: 0.9987 - val_loss: 0.1003 - val_acc: 0.9828 - lr: 2.3256e-05\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 2.272727272727273e-05.\n",
      "Epoch 44/1000\n",
      "50000/50000 [==============================] - 3s 64us/sample - loss: 0.0503 - acc: 0.9987 - val_loss: 0.1005 - val_acc: 0.9832 - lr: 2.2727e-05\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 2.2222222222222223e-05.\n",
      "Epoch 45/1000\n",
      "50000/50000 [==============================] - 3s 66us/sample - loss: 0.0499 - acc: 0.9989 - val_loss: 0.1003 - val_acc: 0.9829 - lr: 2.2222e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 2.173913043478261e-05.\n",
      "Epoch 46/1000\n",
      "50000/50000 [==============================] - 5s 92us/sample - loss: 0.0499 - acc: 0.9987 - val_loss: 0.1003 - val_acc: 0.9825 - lr: 2.1739e-05\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 2.1276595744680852e-05.\n",
      "Epoch 47/1000\n",
      "50000/50000 [==============================] - 3s 69us/sample - loss: 0.0496 - acc: 0.9988 - val_loss: 0.0997 - val_acc: 0.9830 - lr: 2.1277e-05\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 2.0833333333333333e-05.\n",
      "Epoch 48/1000\n",
      "50000/50000 [==============================] - 3s 70us/sample - loss: 0.0491 - acc: 0.9988 - val_loss: 0.0999 - val_acc: 0.9826 - lr: 2.0833e-05\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 2.0408163265306123e-05.\n",
      "Epoch 49/1000\n",
      "50000/50000 [==============================] - 3s 69us/sample - loss: 0.0492 - acc: 0.9988 - val_loss: 0.0998 - val_acc: 0.9830 - lr: 2.0408e-05\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 2e-05.\n",
      "Epoch 50/1000\n",
      "50000/50000 [==============================] - 3s 68us/sample - loss: 0.0490 - acc: 0.9988 - val_loss: 0.0996 - val_acc: 0.9825 - lr: 2.0000e-05\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 1.9607843137254903e-05.\n",
      "Epoch 51/1000\n",
      "50000/50000 [==============================] - 3s 69us/sample - loss: 0.0489 - acc: 0.9988 - val_loss: 0.0990 - val_acc: 0.9823 - lr: 1.9608e-05\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 1.923076923076923e-05.\n",
      "Epoch 52/1000\n",
      "50000/50000 [==============================] - 4s 71us/sample - loss: 0.0485 - acc: 0.9990 - val_loss: 0.0985 - val_acc: 0.9825 - lr: 1.9231e-05\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 1.8867924528301888e-05.\n",
      "Epoch 53/1000\n",
      "50000/50000 [==============================] - 4s 76us/sample - loss: 0.0483 - acc: 0.9991 - val_loss: 0.0987 - val_acc: 0.9827 - lr: 1.8868e-05\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 1.8518518518518518e-05.\n",
      "Epoch 54/1000\n",
      "50000/50000 [==============================] - 3s 69us/sample - loss: 0.0482 - acc: 0.9990 - val_loss: 0.0989 - val_acc: 0.9826 - lr: 1.8519e-05\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 1.8181818181818182e-05.\n",
      "Epoch 55/1000\n",
      "50000/50000 [==============================] - 4s 72us/sample - loss: 0.0480 - acc: 0.9990 - val_loss: 0.0987 - val_acc: 0.9828 - lr: 1.8182e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0ea34b970>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "X_valid = datasets['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = datasets['Y_valid']\n",
    "Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "train_model_8(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "            validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "            patience=3, learning_rate_decay=True, alpha=alpha,\n",
    "            save_best=True, filepath='network1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9. Save and Load Model\n",
    "\"\"\"\n",
    "Write fnctions to save and load an entire model\n",
    "\"\"\"\n",
    "def save_model(network, filename):\n",
    "    \"\"\"\n",
    "    network: model to save\n",
    "    filename: path of the file that the model should be saved to\n",
    "    Returns None\n",
    "    \"\"\"\n",
    "    network.save(filename)\n",
    "    return None\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\"\n",
    "    filename: path of the fale that the model should be loaded from\n",
    "    Returns the loaded model\n",
    "    \"\"\"\n",
    "    return K.models.load_model(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bsbanotto/.local/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py:93: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bsbanotto/.local/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py:93: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bsbanotto/.local/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py:93: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/bsbanotto/.local/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py:93: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "2023-03-06 14:17:57.676607: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_40_1/bias/Assign' id:4787 op device:{requested: '', assigned: ''} def:{{{node dense_40_1/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_40_1/bias, dense_40_1/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-03-06 14:17:58.127816: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_39_1/bias/m/Assign' id:4968 op device:{requested: '', assigned: ''} def:{{{node dense_39_1/bias/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_39_1/bias/m, dense_39_1/bias/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 14:17:58.669217: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_8/AddN_1' id:4920 op device:{requested: '', assigned: ''} def:{{{node loss_8/AddN_1}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_8/mul, loss_8/AddN)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/1000\n",
      "49600/50000 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9624"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/.local/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-03-06 14:18:06.068684: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_8/AddN_1' id:4920 op device:{requested: '', assigned: ''} def:{{{node loss_8/AddN_1}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_8/mul, loss_8/AddN)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.1803 - acc: 0.9624 - val_loss: 0.1609 - val_acc: 0.9710 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 2/1000\n",
      "50000/50000 [==============================] - 6s 110us/sample - loss: 0.1039 - acc: 0.9862 - val_loss: 0.1289 - val_acc: 0.9792 - lr: 5.0000e-04\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0003333333333333333.\n",
      "Epoch 3/1000\n",
      "50000/50000 [==============================] - 5s 110us/sample - loss: 0.0827 - acc: 0.9916 - val_loss: 0.1174 - val_acc: 0.9809 - lr: 3.3333e-04\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.00025.\n",
      "Epoch 4/1000\n",
      "50000/50000 [==============================] - 6s 111us/sample - loss: 0.0715 - acc: 0.9944 - val_loss: 0.1126 - val_acc: 0.9807 - lr: 2.5000e-04\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0002.\n",
      "Epoch 5/1000\n",
      "50000/50000 [==============================] - 6s 110us/sample - loss: 0.0648 - acc: 0.9960 - val_loss: 0.1156 - val_acc: 0.9807 - lr: 2.0000e-04\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.00016666666666666666.\n",
      "Epoch 6/1000\n",
      "50000/50000 [==============================] - 6s 114us/sample - loss: 0.0606 - acc: 0.9968 - val_loss: 0.1111 - val_acc: 0.9818 - lr: 1.6667e-04\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.00014285714285714287.\n",
      "Epoch 7/1000\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 0.0578 - acc: 0.9972 - val_loss: 0.1102 - val_acc: 0.9808 - lr: 1.4286e-04\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.000125.\n",
      "Epoch 8/1000\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 0.0546 - acc: 0.9978 - val_loss: 0.1058 - val_acc: 0.9824 - lr: 1.2500e-04\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.00011111111111111112.\n",
      "Epoch 9/1000\n",
      "50000/50000 [==============================] - 5s 104us/sample - loss: 0.0538 - acc: 0.9976 - val_loss: 0.1041 - val_acc: 0.9827 - lr: 1.1111e-04\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 10/1000\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 0.0511 - acc: 0.9983 - val_loss: 0.1013 - val_acc: 0.9816 - lr: 1.0000e-04\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 9.090909090909092e-05.\n",
      "Epoch 11/1000\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 0.0506 - acc: 0.9981 - val_loss: 0.1017 - val_acc: 0.9830 - lr: 9.0909e-05\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 8.333333333333333e-05.\n",
      "Epoch 12/1000\n",
      "50000/50000 [==============================] - 5s 108us/sample - loss: 0.0489 - acc: 0.9987 - val_loss: 0.0975 - val_acc: 0.9839 - lr: 8.3333e-05\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 7.692307692307693e-05.\n",
      "Epoch 13/1000\n",
      "50000/50000 [==============================] - 6s 110us/sample - loss: 0.0483 - acc: 0.9985 - val_loss: 0.0973 - val_acc: 0.9831 - lr: 7.6923e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 7.142857142857143e-05.\n",
      "Epoch 14/1000\n",
      "50000/50000 [==============================] - 8s 154us/sample - loss: 0.0477 - acc: 0.9984 - val_loss: 0.0970 - val_acc: 0.9829 - lr: 7.1429e-05\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 6.666666666666667e-05.\n",
      "Epoch 15/1000\n",
      "50000/50000 [==============================] - 6s 110us/sample - loss: 0.0463 - acc: 0.9987 - val_loss: 0.0963 - val_acc: 0.9837 - lr: 6.6667e-05\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 6.25e-05.\n",
      "Epoch 16/1000\n",
      "50000/50000 [==============================] - 6s 120us/sample - loss: 0.0459 - acc: 0.9987 - val_loss: 0.0964 - val_acc: 0.9825 - lr: 6.2500e-05\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 5.882352941176471e-05.\n",
      "Epoch 17/1000\n",
      "50000/50000 [==============================] - 5s 107us/sample - loss: 0.0452 - acc: 0.9989 - val_loss: 0.0948 - val_acc: 0.9829 - lr: 5.8824e-05\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 5.555555555555556e-05.\n",
      "Epoch 18/1000\n",
      "50000/50000 [==============================] - 5s 109us/sample - loss: 0.0446 - acc: 0.9989 - val_loss: 0.0936 - val_acc: 0.9835 - lr: 5.5556e-05\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 5.2631578947368424e-05.\n",
      "Epoch 19/1000\n",
      "50000/50000 [==============================] - 5s 109us/sample - loss: 0.0445 - acc: 0.9988 - val_loss: 0.0923 - val_acc: 0.9834 - lr: 5.2632e-05\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 5e-05.\n",
      "Epoch 20/1000\n",
      "50000/50000 [==============================] - 6s 115us/sample - loss: 0.0439 - acc: 0.9990 - val_loss: 0.0928 - val_acc: 0.9834 - lr: 5.0000e-05\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 4.761904761904762e-05.\n",
      "Epoch 21/1000\n",
      "50000/50000 [==============================] - 5s 104us/sample - loss: 0.0433 - acc: 0.9991 - val_loss: 0.0923 - val_acc: 0.9835 - lr: 4.7619e-05\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 4.545454545454546e-05.\n",
      "Epoch 22/1000\n",
      "50000/50000 [==============================] - 5s 108us/sample - loss: 0.0428 - acc: 0.9992 - val_loss: 0.0929 - val_acc: 0.9833 - lr: 4.5455e-05\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 4.347826086956522e-05.\n",
      "Epoch 23/1000\n",
      "50000/50000 [==============================] - 5s 109us/sample - loss: 0.0424 - acc: 0.9992 - val_loss: 0.0912 - val_acc: 0.9843 - lr: 4.3478e-05\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 4.1666666666666665e-05.\n",
      "Epoch 24/1000\n",
      "50000/50000 [==============================] - 6s 111us/sample - loss: 0.0420 - acc: 0.9993 - val_loss: 0.0915 - val_acc: 0.9827 - lr: 4.1667e-05\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 4e-05.\n",
      "Epoch 25/1000\n",
      "50000/50000 [==============================] - 5s 105us/sample - loss: 0.0416 - acc: 0.9994 - val_loss: 0.0925 - val_acc: 0.9831 - lr: 4.0000e-05\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 256)               200960    \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[array([[ 5.2083851e-32, -4.5834429e-32, -3.9512856e-32, ...,\n",
      "         4.0175705e-32, -4.8605633e-33, -5.0913599e-33],\n",
      "       [ 1.4321600e-32,  9.7771155e-34, -4.7958465e-32, ...,\n",
      "         5.3027156e-32,  4.7995517e-32, -9.9750540e-33],\n",
      "       [-6.9345938e-34, -3.1645671e-32, -1.4548976e-32, ...,\n",
      "         4.0143943e-32, -5.5053537e-32,  4.4981861e-32],\n",
      "       ...,\n",
      "       [-1.4397267e-32,  1.8458000e-32, -2.2599852e-32, ...,\n",
      "        -1.4517120e-32, -1.4639677e-32, -7.5152219e-34],\n",
      "       [-2.9018982e-32,  5.1277309e-32, -9.0226127e-33, ...,\n",
      "         3.4054151e-32, -1.4997749e-32, -1.3281702e-32],\n",
      "       [ 2.4085938e-32, -4.2268761e-32,  7.4096522e-33, ...,\n",
      "         7.8109249e-34, -5.2455142e-33, -2.0123228e-32]], dtype=float32), array([ 8.62530768e-02, -8.31839535e-03,  9.55670024e-04, -5.50825410e-02,\n",
      "        1.48219913e-02, -7.62033910e-02,  2.27520019e-02, -3.04525178e-02,\n",
      "        5.12484722e-02, -8.12617876e-03, -1.20907277e-01,  2.45105736e-02,\n",
      "       -2.01016851e-02, -1.81349833e-02,  2.08948227e-03,  1.49119765e-01,\n",
      "        5.64794987e-03,  6.32431172e-03, -1.75604485e-02,  1.01008732e-02,\n",
      "       -1.91236228e-01,  6.49904609e-02,  7.70019069e-02, -1.29839277e-03,\n",
      "       -2.63260156e-02, -2.13492718e-02, -3.48833241e-02, -1.03753589e-01,\n",
      "       -6.37769178e-02, -2.60874219e-02, -3.25557925e-02, -1.71913225e-02,\n",
      "        2.13271733e-02, -3.81590314e-02,  9.05831605e-02, -6.63282424e-02,\n",
      "       -7.71699250e-02,  3.98361981e-02,  2.01269500e-02, -1.96572524e-02,\n",
      "       -5.71079589e-02,  2.58442704e-02, -4.83821891e-02,  2.51058228e-02,\n",
      "        3.42626050e-02,  9.13265795e-02, -6.66640475e-02, -6.63007784e-04,\n",
      "        2.91177221e-02,  3.66814137e-02,  1.08364355e-02, -3.99697572e-02,\n",
      "       -7.87953958e-02, -6.76648552e-03,  2.72671208e-02,  4.27470244e-02,\n",
      "       -1.25209272e-01,  2.96991598e-02,  1.45779271e-02, -1.45073337e-02,\n",
      "       -1.00531019e-02, -1.50496215e-02, -5.20893075e-02,  1.45139601e-02,\n",
      "       -4.63270359e-02, -4.33303006e-02, -1.22246621e-02,  3.83626372e-02,\n",
      "       -1.63704418e-02,  1.50303487e-02, -1.32521344e-02, -3.49584073e-02,\n",
      "       -5.74010871e-02, -2.99698543e-02,  1.77332446e-01,  3.80478427e-02,\n",
      "       -6.11421876e-02, -9.05119628e-03,  7.18119964e-02, -3.85284331e-03,\n",
      "       -2.11240165e-02,  6.92689046e-03, -3.00221052e-02, -3.47050093e-02,\n",
      "       -2.83129402e-02,  8.28518346e-02, -4.15912904e-02, -6.24572411e-02,\n",
      "        1.30615700e-02, -1.83063932e-02, -1.45049933e-02, -2.82912608e-03,\n",
      "        8.28098655e-02,  7.61379674e-02, -2.75176987e-02, -1.09647345e-02,\n",
      "        3.60857733e-02,  7.68832956e-03, -1.09389620e-02,  4.48158272e-02,\n",
      "        7.93537796e-02, -3.27009298e-02, -8.11418227e-04,  2.30303444e-02,\n",
      "        1.18108932e-02, -2.88218614e-02, -2.23707650e-02, -8.25709775e-02,\n",
      "        8.06285664e-02, -3.97807406e-03, -4.86915596e-02, -2.60953493e-02,\n",
      "       -4.29698080e-02, -2.16690972e-02, -1.48874009e-02, -2.92073749e-02,\n",
      "        8.69347714e-04,  1.38913235e-03,  7.90536702e-02, -5.40850349e-02,\n",
      "       -1.25001252e-01,  4.42476943e-02, -4.43987641e-03, -2.98173027e-03,\n",
      "        3.82280052e-02,  4.02771868e-02, -3.18113336e-04,  1.61444470e-01,\n",
      "        9.43719223e-02, -1.11965006e-02,  9.04762279e-03, -2.67160535e-02,\n",
      "       -4.21489663e-02,  1.27805397e-01,  4.86915521e-02, -7.37055065e-03,\n",
      "        4.03744821e-03, -4.93572019e-02, -6.33331910e-02,  2.70556193e-02,\n",
      "        3.54343653e-02,  4.11559194e-02,  7.41911754e-02, -5.14119044e-02,\n",
      "        9.66426358e-03,  9.43559483e-02, -1.22941043e-02,  1.51820173e-02,\n",
      "       -6.95044082e-03, -4.58607078e-02, -3.11628953e-02, -4.42212038e-02,\n",
      "       -5.60717881e-02, -3.65791507e-02, -5.77625483e-02,  1.91770196e-02,\n",
      "        1.29145402e-02, -1.71936806e-02, -1.46811865e-02,  7.79169500e-02,\n",
      "       -6.27206266e-02, -9.42822720e-04,  4.82690381e-03, -2.72236560e-02,\n",
      "       -3.03747933e-02,  5.97342514e-02, -2.45041922e-02,  9.41376481e-03,\n",
      "       -1.04874629e-03, -3.92374769e-03,  1.82885993e-02,  3.11453789e-02,\n",
      "        1.70295145e-02, -1.19111978e-01, -7.92977680e-03,  1.27610296e-01,\n",
      "       -4.21622721e-03,  7.58036822e-02,  2.56725904e-02, -7.80415237e-02,\n",
      "        8.05815831e-02, -2.68203835e-03, -1.32181318e-02,  7.33008087e-02,\n",
      "        2.60888990e-02,  1.12051116e-02,  7.20857605e-02, -2.30429377e-02,\n",
      "       -1.48450928e-02,  3.78622748e-02,  2.42237076e-02,  1.06386043e-01,\n",
      "        3.75866331e-02, -1.36964826e-03, -5.71181811e-02,  1.01029120e-01,\n",
      "        1.31087899e-01, -2.02247817e-02, -5.32845296e-02, -3.98663953e-02,\n",
      "       -2.71745119e-02,  5.11851907e-02, -1.02164835e-01,  1.89173874e-02,\n",
      "       -5.07655069e-02,  2.16731578e-02, -3.40492912e-02, -5.65010309e-02,\n",
      "        2.34909821e-02, -3.16549502e-02,  8.12474936e-02,  2.42230226e-03,\n",
      "        2.46908534e-02, -1.44703444e-02, -2.07489735e-04,  6.53195083e-02,\n",
      "       -9.07793865e-02,  7.14688301e-02, -1.02920588e-02,  8.48497525e-02,\n",
      "        9.78412926e-02,  1.13144834e-02,  6.52122870e-02,  4.13638726e-02,\n",
      "        7.66576529e-02,  6.47638589e-02,  3.93904783e-02,  8.38349238e-02,\n",
      "        8.78987610e-02,  9.45498887e-03,  8.33759755e-02, -5.24552613e-02,\n",
      "        1.05010226e-01, -2.77033579e-02,  2.87880991e-02,  7.46304393e-02,\n",
      "       -3.17467339e-02, -1.29237786e-04, -4.04264405e-03,  6.71582893e-02,\n",
      "       -2.44222600e-02, -2.57928185e-02, -1.64353102e-02, -6.57766536e-02,\n",
      "       -6.39817864e-02, -5.51737733e-02, -1.54455984e-02,  7.74971861e-03,\n",
      "        1.45444041e-02,  1.28622660e-02,  4.78897040e-04,  1.80753469e-02,\n",
      "        2.89234500e-02,  1.93243306e-02,  7.64483213e-02, -6.92613330e-03],\n",
      "      dtype=float32), array([[-0.00266744, -0.09004945,  0.01464715, ...,  0.14781721,\n",
      "         0.02733858,  0.01829443],\n",
      "       [-0.02835939,  0.02214683,  0.0368585 , ..., -0.02087937,\n",
      "         0.02029936,  0.04890806],\n",
      "       [-0.00414461,  0.01770197, -0.00619651, ..., -0.00271617,\n",
      "         0.0082811 , -0.03085013],\n",
      "       ...,\n",
      "       [ 0.00726598,  0.04482333,  0.01845717, ...,  0.00236286,\n",
      "         0.02697176,  0.00428759],\n",
      "       [ 0.02178492,  0.0295711 ,  0.04929947, ...,  0.03696263,\n",
      "         0.09166723, -0.048558  ],\n",
      "       [ 0.00819972, -0.0154288 ,  0.03018758, ...,  0.02169614,\n",
      "         0.01562465, -0.04590276]], dtype=float32), array([ 0.07264675,  0.04903809,  0.16087277, -0.10254315,  0.07277024,\n",
      "        0.16412444, -0.02235746,  0.10166115,  0.04622962, -0.0933399 ,\n",
      "        0.08227658,  0.04317146,  0.10356514,  0.10547188,  0.08069966,\n",
      "       -0.04808303,  0.0652068 ,  0.11041095, -0.01500788, -0.00523831,\n",
      "        0.0443038 ,  0.0453033 ,  0.00842937,  0.19162086,  0.11224119,\n",
      "        0.03443902,  0.01801828,  0.12292988,  0.04164241, -0.02811614,\n",
      "        0.09400473,  0.17562567,  0.09555557,  0.03573526, -0.031639  ,\n",
      "       -0.07522953,  0.04926643,  0.16299783,  0.07664816,  0.10373268,\n",
      "        0.14404935,  0.1648833 ,  0.1557875 , -0.05724157,  0.1855904 ,\n",
      "        0.03748217,  0.02886731,  0.00213537,  0.03987583,  0.04128076,\n",
      "        0.01724737,  0.08293738,  0.10889384,  0.05335944, -0.05340833,\n",
      "        0.01339202,  0.14890134,  0.04745034,  0.10467256,  0.01753677,\n",
      "        0.15704499,  0.0035849 ,  0.02557885,  0.13374053,  0.00506186,\n",
      "        0.02802952, -0.09341158,  0.1826377 ,  0.03298888, -0.00348746,\n",
      "        0.21357524,  0.10130274,  0.02191225,  0.08352593,  0.10909419,\n",
      "       -0.02637353,  0.11952768, -0.02574825, -0.04376065, -0.04981356,\n",
      "       -0.04348337,  0.04427174, -0.00394766,  0.155714  ,  0.08064886,\n",
      "        0.06579563,  0.02364776,  0.19568306, -0.05032034, -0.04746008,\n",
      "        0.11791408,  0.03880764,  0.1704596 ,  0.11074578,  0.13949342,\n",
      "       -0.04846672,  0.16671047,  0.10974282,  0.17808619,  0.06456121,\n",
      "        0.06393234,  0.03002561,  0.03674229,  0.10376556, -0.016941  ,\n",
      "        0.00671812,  0.11469529, -0.03713677,  0.024461  , -0.03640553,\n",
      "        0.15906113, -0.02156405, -0.06735326,  0.02984953, -0.03916499,\n",
      "        0.15529099, -0.01428678, -0.01030556,  0.01993123, -0.02007611,\n",
      "       -0.04969285,  0.01790646,  0.04171621, -0.00386607,  0.01508977,\n",
      "       -0.05256676,  0.15755112, -0.05543449,  0.11691533,  0.00142599,\n",
      "       -0.02405608,  0.07745108,  0.14428975,  0.09153403,  0.07343979,\n",
      "       -0.01518059,  0.04699907,  0.00644902,  0.06481237,  0.18725611,\n",
      "        0.207525  ,  0.02619202,  0.08887378, -0.01384263, -0.0353312 ,\n",
      "       -0.01652866, -0.01812604,  0.00924148,  0.07274479,  0.03045964,\n",
      "       -0.02381273, -0.00766203,  0.01367849,  0.0418172 , -0.0736603 ,\n",
      "       -0.01010098, -0.0213807 , -0.05578637, -0.0356971 , -0.02629323,\n",
      "        0.07461506, -0.04873221,  0.07901479,  0.12540342,  0.10754339,\n",
      "       -0.01970891, -0.02656631, -0.06158991,  0.00250764,  0.15978561,\n",
      "        0.02401769,  0.17058991,  0.03088844,  0.18616016,  0.14886646,\n",
      "       -0.0528625 ,  0.08864275, -0.02183018,  0.12053749,  0.07615625,\n",
      "        0.04520301,  0.16583015, -0.04008656,  0.0662927 ,  0.12109338,\n",
      "        0.07008906,  0.04866848, -0.00148936,  0.06776264, -0.02986807,\n",
      "        0.11609469,  0.14371084,  0.18758363,  0.00165424,  0.04880097,\n",
      "       -0.04248513,  0.18740849,  0.13196515, -0.05634061,  0.05119942,\n",
      "        0.16040184,  0.1147477 ,  0.0229021 ,  0.15131383,  0.0792253 ,\n",
      "        0.02898989,  0.18839878,  0.04468602,  0.13572815,  0.15332605,\n",
      "        0.04167363,  0.07921954,  0.04286644,  0.1410995 ,  0.20030586,\n",
      "        0.04642846,  0.08394491,  0.11155701,  0.0112706 , -0.03292399,\n",
      "       -0.01732697,  0.12153708,  0.02699341,  0.06461786, -0.03665378,\n",
      "        0.02493052, -0.01642258,  0.01770419,  0.1576934 ,  0.11541805,\n",
      "        0.16592266, -0.03457259,  0.03942877, -0.01085995, -0.01325674,\n",
      "       -0.01306757,  0.14542991,  0.12022938,  0.06175241, -0.02701773,\n",
      "        0.00927297,  0.06626541, -0.07586993, -0.0019239 ,  0.01868704,\n",
      "       -0.06579214,  0.12939663,  0.00636831,  0.05030293,  0.13026069,\n",
      "        0.06550696,  0.0080289 ,  0.14337999,  0.0626049 ,  0.16154398,\n",
      "        0.13352606], dtype=float32), array([[ 0.13457452, -0.10225517, -0.08390855, ..., -0.3277547 ,\n",
      "         0.01231808,  0.22550167],\n",
      "       [-0.34066924,  0.07575487,  0.07395692, ..., -0.21789728,\n",
      "        -0.04580585, -0.27350613],\n",
      "       [-0.13944684, -0.12020504, -0.29441336, ..., -0.2051268 ,\n",
      "         0.12302856,  0.219606  ],\n",
      "       ...,\n",
      "       [ 0.00864716, -0.08597519,  0.059101  , ...,  0.15450995,\n",
      "        -0.29397705,  0.18611974],\n",
      "       [ 0.01737148,  0.05736921, -0.2575534 , ..., -0.1646101 ,\n",
      "         0.27157483,  0.19324727],\n",
      "       [ 0.09870632, -0.3492899 ,  0.08037058, ...,  0.05875591,\n",
      "         0.05333301,  0.07638631]], dtype=float32), array([-0.02435027, -0.07759133, -0.02565664, -0.04794778, -0.00926919,\n",
      "       -0.02088713, -0.03641646, -0.08688238,  0.20124163,  0.03214953],\n",
      "      dtype=float32)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 14:20:20.196038: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_38_2/bias/Assign' id:5284 op device:{requested: '', assigned: ''} def:{{{node dense_38_2/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_38_2/bias, dense_38_2/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-03-06 14:20:21.356445: W tensorflow/c/c_api.cc:291] Operation '{name:'count_9/Assign' id:5399 op device:{requested: '', assigned: ''} def:{{{node count_9/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_9, count_9/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_13 (InputLayer)       [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 256)               200960    \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[array([[ 5.2083851e-32, -4.5834429e-32, -3.9512856e-32, ...,\n",
      "         4.0175705e-32, -4.8605633e-33, -5.0913599e-33],\n",
      "       [ 1.4321600e-32,  9.7771155e-34, -4.7958465e-32, ...,\n",
      "         5.3027156e-32,  4.7995517e-32, -9.9750540e-33],\n",
      "       [-6.9345938e-34, -3.1645671e-32, -1.4548976e-32, ...,\n",
      "         4.0143943e-32, -5.5053537e-32,  4.4981861e-32],\n",
      "       ...,\n",
      "       [-1.4397267e-32,  1.8458000e-32, -2.2599852e-32, ...,\n",
      "        -1.4517120e-32, -1.4639677e-32, -7.5152219e-34],\n",
      "       [-2.9018982e-32,  5.1277309e-32, -9.0226127e-33, ...,\n",
      "         3.4054151e-32, -1.4997749e-32, -1.3281702e-32],\n",
      "       [ 2.4085938e-32, -4.2268761e-32,  7.4096522e-33, ...,\n",
      "         7.8109249e-34, -5.2455142e-33, -2.0123228e-32]], dtype=float32), array([ 8.62530768e-02, -8.31839535e-03,  9.55670024e-04, -5.50825410e-02,\n",
      "        1.48219913e-02, -7.62033910e-02,  2.27520019e-02, -3.04525178e-02,\n",
      "        5.12484722e-02, -8.12617876e-03, -1.20907277e-01,  2.45105736e-02,\n",
      "       -2.01016851e-02, -1.81349833e-02,  2.08948227e-03,  1.49119765e-01,\n",
      "        5.64794987e-03,  6.32431172e-03, -1.75604485e-02,  1.01008732e-02,\n",
      "       -1.91236228e-01,  6.49904609e-02,  7.70019069e-02, -1.29839277e-03,\n",
      "       -2.63260156e-02, -2.13492718e-02, -3.48833241e-02, -1.03753589e-01,\n",
      "       -6.37769178e-02, -2.60874219e-02, -3.25557925e-02, -1.71913225e-02,\n",
      "        2.13271733e-02, -3.81590314e-02,  9.05831605e-02, -6.63282424e-02,\n",
      "       -7.71699250e-02,  3.98361981e-02,  2.01269500e-02, -1.96572524e-02,\n",
      "       -5.71079589e-02,  2.58442704e-02, -4.83821891e-02,  2.51058228e-02,\n",
      "        3.42626050e-02,  9.13265795e-02, -6.66640475e-02, -6.63007784e-04,\n",
      "        2.91177221e-02,  3.66814137e-02,  1.08364355e-02, -3.99697572e-02,\n",
      "       -7.87953958e-02, -6.76648552e-03,  2.72671208e-02,  4.27470244e-02,\n",
      "       -1.25209272e-01,  2.96991598e-02,  1.45779271e-02, -1.45073337e-02,\n",
      "       -1.00531019e-02, -1.50496215e-02, -5.20893075e-02,  1.45139601e-02,\n",
      "       -4.63270359e-02, -4.33303006e-02, -1.22246621e-02,  3.83626372e-02,\n",
      "       -1.63704418e-02,  1.50303487e-02, -1.32521344e-02, -3.49584073e-02,\n",
      "       -5.74010871e-02, -2.99698543e-02,  1.77332446e-01,  3.80478427e-02,\n",
      "       -6.11421876e-02, -9.05119628e-03,  7.18119964e-02, -3.85284331e-03,\n",
      "       -2.11240165e-02,  6.92689046e-03, -3.00221052e-02, -3.47050093e-02,\n",
      "       -2.83129402e-02,  8.28518346e-02, -4.15912904e-02, -6.24572411e-02,\n",
      "        1.30615700e-02, -1.83063932e-02, -1.45049933e-02, -2.82912608e-03,\n",
      "        8.28098655e-02,  7.61379674e-02, -2.75176987e-02, -1.09647345e-02,\n",
      "        3.60857733e-02,  7.68832956e-03, -1.09389620e-02,  4.48158272e-02,\n",
      "        7.93537796e-02, -3.27009298e-02, -8.11418227e-04,  2.30303444e-02,\n",
      "        1.18108932e-02, -2.88218614e-02, -2.23707650e-02, -8.25709775e-02,\n",
      "        8.06285664e-02, -3.97807406e-03, -4.86915596e-02, -2.60953493e-02,\n",
      "       -4.29698080e-02, -2.16690972e-02, -1.48874009e-02, -2.92073749e-02,\n",
      "        8.69347714e-04,  1.38913235e-03,  7.90536702e-02, -5.40850349e-02,\n",
      "       -1.25001252e-01,  4.42476943e-02, -4.43987641e-03, -2.98173027e-03,\n",
      "        3.82280052e-02,  4.02771868e-02, -3.18113336e-04,  1.61444470e-01,\n",
      "        9.43719223e-02, -1.11965006e-02,  9.04762279e-03, -2.67160535e-02,\n",
      "       -4.21489663e-02,  1.27805397e-01,  4.86915521e-02, -7.37055065e-03,\n",
      "        4.03744821e-03, -4.93572019e-02, -6.33331910e-02,  2.70556193e-02,\n",
      "        3.54343653e-02,  4.11559194e-02,  7.41911754e-02, -5.14119044e-02,\n",
      "        9.66426358e-03,  9.43559483e-02, -1.22941043e-02,  1.51820173e-02,\n",
      "       -6.95044082e-03, -4.58607078e-02, -3.11628953e-02, -4.42212038e-02,\n",
      "       -5.60717881e-02, -3.65791507e-02, -5.77625483e-02,  1.91770196e-02,\n",
      "        1.29145402e-02, -1.71936806e-02, -1.46811865e-02,  7.79169500e-02,\n",
      "       -6.27206266e-02, -9.42822720e-04,  4.82690381e-03, -2.72236560e-02,\n",
      "       -3.03747933e-02,  5.97342514e-02, -2.45041922e-02,  9.41376481e-03,\n",
      "       -1.04874629e-03, -3.92374769e-03,  1.82885993e-02,  3.11453789e-02,\n",
      "        1.70295145e-02, -1.19111978e-01, -7.92977680e-03,  1.27610296e-01,\n",
      "       -4.21622721e-03,  7.58036822e-02,  2.56725904e-02, -7.80415237e-02,\n",
      "        8.05815831e-02, -2.68203835e-03, -1.32181318e-02,  7.33008087e-02,\n",
      "        2.60888990e-02,  1.12051116e-02,  7.20857605e-02, -2.30429377e-02,\n",
      "       -1.48450928e-02,  3.78622748e-02,  2.42237076e-02,  1.06386043e-01,\n",
      "        3.75866331e-02, -1.36964826e-03, -5.71181811e-02,  1.01029120e-01,\n",
      "        1.31087899e-01, -2.02247817e-02, -5.32845296e-02, -3.98663953e-02,\n",
      "       -2.71745119e-02,  5.11851907e-02, -1.02164835e-01,  1.89173874e-02,\n",
      "       -5.07655069e-02,  2.16731578e-02, -3.40492912e-02, -5.65010309e-02,\n",
      "        2.34909821e-02, -3.16549502e-02,  8.12474936e-02,  2.42230226e-03,\n",
      "        2.46908534e-02, -1.44703444e-02, -2.07489735e-04,  6.53195083e-02,\n",
      "       -9.07793865e-02,  7.14688301e-02, -1.02920588e-02,  8.48497525e-02,\n",
      "        9.78412926e-02,  1.13144834e-02,  6.52122870e-02,  4.13638726e-02,\n",
      "        7.66576529e-02,  6.47638589e-02,  3.93904783e-02,  8.38349238e-02,\n",
      "        8.78987610e-02,  9.45498887e-03,  8.33759755e-02, -5.24552613e-02,\n",
      "        1.05010226e-01, -2.77033579e-02,  2.87880991e-02,  7.46304393e-02,\n",
      "       -3.17467339e-02, -1.29237786e-04, -4.04264405e-03,  6.71582893e-02,\n",
      "       -2.44222600e-02, -2.57928185e-02, -1.64353102e-02, -6.57766536e-02,\n",
      "       -6.39817864e-02, -5.51737733e-02, -1.54455984e-02,  7.74971861e-03,\n",
      "        1.45444041e-02,  1.28622660e-02,  4.78897040e-04,  1.80753469e-02,\n",
      "        2.89234500e-02,  1.93243306e-02,  7.64483213e-02, -6.92613330e-03],\n",
      "      dtype=float32), array([[-0.00266744, -0.09004945,  0.01464715, ...,  0.14781721,\n",
      "         0.02733858,  0.01829443],\n",
      "       [-0.02835939,  0.02214683,  0.0368585 , ..., -0.02087937,\n",
      "         0.02029936,  0.04890806],\n",
      "       [-0.00414461,  0.01770197, -0.00619651, ..., -0.00271617,\n",
      "         0.0082811 , -0.03085013],\n",
      "       ...,\n",
      "       [ 0.00726598,  0.04482333,  0.01845717, ...,  0.00236286,\n",
      "         0.02697176,  0.00428759],\n",
      "       [ 0.02178492,  0.0295711 ,  0.04929947, ...,  0.03696263,\n",
      "         0.09166723, -0.048558  ],\n",
      "       [ 0.00819972, -0.0154288 ,  0.03018758, ...,  0.02169614,\n",
      "         0.01562465, -0.04590276]], dtype=float32), array([ 0.07264675,  0.04903809,  0.16087277, -0.10254315,  0.07277024,\n",
      "        0.16412444, -0.02235746,  0.10166115,  0.04622962, -0.0933399 ,\n",
      "        0.08227658,  0.04317146,  0.10356514,  0.10547188,  0.08069966,\n",
      "       -0.04808303,  0.0652068 ,  0.11041095, -0.01500788, -0.00523831,\n",
      "        0.0443038 ,  0.0453033 ,  0.00842937,  0.19162086,  0.11224119,\n",
      "        0.03443902,  0.01801828,  0.12292988,  0.04164241, -0.02811614,\n",
      "        0.09400473,  0.17562567,  0.09555557,  0.03573526, -0.031639  ,\n",
      "       -0.07522953,  0.04926643,  0.16299783,  0.07664816,  0.10373268,\n",
      "        0.14404935,  0.1648833 ,  0.1557875 , -0.05724157,  0.1855904 ,\n",
      "        0.03748217,  0.02886731,  0.00213537,  0.03987583,  0.04128076,\n",
      "        0.01724737,  0.08293738,  0.10889384,  0.05335944, -0.05340833,\n",
      "        0.01339202,  0.14890134,  0.04745034,  0.10467256,  0.01753677,\n",
      "        0.15704499,  0.0035849 ,  0.02557885,  0.13374053,  0.00506186,\n",
      "        0.02802952, -0.09341158,  0.1826377 ,  0.03298888, -0.00348746,\n",
      "        0.21357524,  0.10130274,  0.02191225,  0.08352593,  0.10909419,\n",
      "       -0.02637353,  0.11952768, -0.02574825, -0.04376065, -0.04981356,\n",
      "       -0.04348337,  0.04427174, -0.00394766,  0.155714  ,  0.08064886,\n",
      "        0.06579563,  0.02364776,  0.19568306, -0.05032034, -0.04746008,\n",
      "        0.11791408,  0.03880764,  0.1704596 ,  0.11074578,  0.13949342,\n",
      "       -0.04846672,  0.16671047,  0.10974282,  0.17808619,  0.06456121,\n",
      "        0.06393234,  0.03002561,  0.03674229,  0.10376556, -0.016941  ,\n",
      "        0.00671812,  0.11469529, -0.03713677,  0.024461  , -0.03640553,\n",
      "        0.15906113, -0.02156405, -0.06735326,  0.02984953, -0.03916499,\n",
      "        0.15529099, -0.01428678, -0.01030556,  0.01993123, -0.02007611,\n",
      "       -0.04969285,  0.01790646,  0.04171621, -0.00386607,  0.01508977,\n",
      "       -0.05256676,  0.15755112, -0.05543449,  0.11691533,  0.00142599,\n",
      "       -0.02405608,  0.07745108,  0.14428975,  0.09153403,  0.07343979,\n",
      "       -0.01518059,  0.04699907,  0.00644902,  0.06481237,  0.18725611,\n",
      "        0.207525  ,  0.02619202,  0.08887378, -0.01384263, -0.0353312 ,\n",
      "       -0.01652866, -0.01812604,  0.00924148,  0.07274479,  0.03045964,\n",
      "       -0.02381273, -0.00766203,  0.01367849,  0.0418172 , -0.0736603 ,\n",
      "       -0.01010098, -0.0213807 , -0.05578637, -0.0356971 , -0.02629323,\n",
      "        0.07461506, -0.04873221,  0.07901479,  0.12540342,  0.10754339,\n",
      "       -0.01970891, -0.02656631, -0.06158991,  0.00250764,  0.15978561,\n",
      "        0.02401769,  0.17058991,  0.03088844,  0.18616016,  0.14886646,\n",
      "       -0.0528625 ,  0.08864275, -0.02183018,  0.12053749,  0.07615625,\n",
      "        0.04520301,  0.16583015, -0.04008656,  0.0662927 ,  0.12109338,\n",
      "        0.07008906,  0.04866848, -0.00148936,  0.06776264, -0.02986807,\n",
      "        0.11609469,  0.14371084,  0.18758363,  0.00165424,  0.04880097,\n",
      "       -0.04248513,  0.18740849,  0.13196515, -0.05634061,  0.05119942,\n",
      "        0.16040184,  0.1147477 ,  0.0229021 ,  0.15131383,  0.0792253 ,\n",
      "        0.02898989,  0.18839878,  0.04468602,  0.13572815,  0.15332605,\n",
      "        0.04167363,  0.07921954,  0.04286644,  0.1410995 ,  0.20030586,\n",
      "        0.04642846,  0.08394491,  0.11155701,  0.0112706 , -0.03292399,\n",
      "       -0.01732697,  0.12153708,  0.02699341,  0.06461786, -0.03665378,\n",
      "        0.02493052, -0.01642258,  0.01770419,  0.1576934 ,  0.11541805,\n",
      "        0.16592266, -0.03457259,  0.03942877, -0.01085995, -0.01325674,\n",
      "       -0.01306757,  0.14542991,  0.12022938,  0.06175241, -0.02701773,\n",
      "        0.00927297,  0.06626541, -0.07586993, -0.0019239 ,  0.01868704,\n",
      "       -0.06579214,  0.12939663,  0.00636831,  0.05030293,  0.13026069,\n",
      "        0.06550696,  0.0080289 ,  0.14337999,  0.0626049 ,  0.16154398,\n",
      "        0.13352606], dtype=float32), array([[ 0.13457452, -0.10225517, -0.08390855, ..., -0.3277547 ,\n",
      "         0.01231808,  0.22550167],\n",
      "       [-0.34066924,  0.07575487,  0.07395692, ..., -0.21789728,\n",
      "        -0.04580585, -0.27350613],\n",
      "       [-0.13944684, -0.12020504, -0.29441336, ..., -0.2051268 ,\n",
      "         0.12302856,  0.219606  ],\n",
      "       ...,\n",
      "       [ 0.00864716, -0.08597519,  0.059101  , ...,  0.15450995,\n",
      "        -0.29397705,  0.18611974],\n",
      "       [ 0.01737148,  0.05736921, -0.2575534 , ..., -0.1646101 ,\n",
      "         0.27157483,  0.19324727],\n",
      "       [ 0.09870632, -0.3492899 ,  0.08037058, ...,  0.05875591,\n",
      "         0.05333301,  0.07638631]], dtype=float32), array([-0.02435027, -0.07759133, -0.02565664, -0.04794778, -0.00926919,\n",
      "       -0.02088713, -0.03641646, -0.08688238,  0.20124163,  0.03214953],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# 9-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "# import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "X_valid = datasets['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = datasets['Y_valid']\n",
    "Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "network = load_model('network1.h5')\n",
    "batch_size = 32\n",
    "epochs = 1000\n",
    "train_model_8(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "            validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "            patience=2, learning_rate_decay=True, alpha=0.001)\n",
    "save_model(network, 'network2.h5')\n",
    "network.summary()\n",
    "print(network.get_weights())\n",
    "del network\n",
    "\n",
    "network2 = load_model('network2.h5')\n",
    "network2.summary()\n",
    "print(network2.get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
