{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow.compat.v1.keras as K\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Sequential\n",
    "\"\"\"\n",
    "Write a function that builds a neural network with the Keras library\n",
    "Use Sequential model since that's in the task name\n",
    "\"\"\"\n",
    "def build_model_0(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    nx: number of input features to the network\n",
    "    layers: list containing the number of nodes in each layer of the network\n",
    "    activations: list containing the activation functions for each layer\n",
    "    lambtha: L2 regularization parameter\n",
    "    keep_prob: probability that a node will be kept for dropout\n",
    "\n",
    "    Not allowed to use `Input` class\n",
    "    Returns the keras model\n",
    "    \"\"\"\n",
    "    model = K.Sequential()\n",
    "    # Create our input layer\n",
    "    model.add(K.layers.InputLayer(input_shape=(nx,)))\n",
    "    # Create all of our hidden layers\n",
    "    for layer in range(len(layers)):\n",
    "        model.add(K.layers.Dense(units=layers[layer],\n",
    "                                 activation=activations[layer],\n",
    "                                 kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                                 ))\n",
    "        # Handle dropout on hidden layers\n",
    "        if layer < len(layers) - 1:\n",
    "            model.add(K.layers.Dropout(1 - keep_prob))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               200960    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor 'dense/kernel/Regularizer/mul:0' shape=() dtype=float32>, <tf.Tensor 'dense_1/kernel/Regularizer/mul:0' shape=() dtype=float32>, <tf.Tensor 'dense_2/kernel/Regularizer/mul:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# 0-main\n",
    "network = build_model_0(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "network.summary()\n",
    "print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Sequential | Attempt 2\n",
    "\"\"\"\n",
    "Write a function that builds a neural network with the Keras library\n",
    "Use Sequential model since that's in the task name\n",
    "\"\"\"\n",
    "def build_model_1(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    nx: number of input features to the network\n",
    "    layers: list containing the number of nodes in each layer of the network\n",
    "    activations: list containing the activation functions for each layer\n",
    "    lambtha: L2 regularization parameter\n",
    "    keep_prob: probability that a node will be kept for dropout\n",
    "\n",
    "    Not allowed to use `Input` class\n",
    "    Returns the keras model\n",
    "    \"\"\"\n",
    "    model = K.Sequential()\n",
    "    # Create our input layer\n",
    "    model.add(K.layers.Dense(units=layers[0],\n",
    "                             activation = activations[0],\n",
    "                             kernel_regularizer=K.regularizers.l2(lambtha),\n",
    "                             input_shape=(nx,)))\n",
    "    # Create all of our hidden layers\n",
    "    for layer in range(1, len(layers)):\n",
    "        if layer <= len(layers) - 1:\n",
    "            model.add(K.layers.Dropout(1 - keep_prob))\n",
    "        model.add(K.layers.Dense(units=layers[layer],\n",
    "                                 activation=activations[layer],\n",
    "                                 kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                                 ))\n",
    "        # Handle dropout on hidden layers\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 256)               200960    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor 'dense_3/kernel/Regularizer/mul:0' shape=() dtype=float32>, <tf.Tensor 'dense_4/kernel/Regularizer/mul:0' shape=() dtype=float32>, <tf.Tensor 'dense_5/kernel/Regularizer/mul:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# 0-main | Attempt 2\n",
    "network = build_model_1(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "network.summary()\n",
    "print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1. Input\n",
    "\"\"\"\n",
    "Write a function that builds a neural network with the Keras library\n",
    "\"\"\"\n",
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\"\n",
    "    nx: number of input features to the network\n",
    "    layers: list containing the number of nodes in each layer of the network\n",
    "    activations: list containing the activation functions for each layer\n",
    "    lambtha: L2 regularization parameter\n",
    "    keep_prob: probability that a node will be kept for dropout\n",
    "\n",
    "    Not allowed to use `Sequential` class\n",
    "    Returns the keras model\n",
    "    \"\"\"\n",
    "    inputs = K.Input(shape=(nx,))\n",
    "    x = K.layers.Dense(units=layers[0],\n",
    "                       activation=activations[0],\n",
    "                       kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                       )(inputs)\n",
    "    for layer in range(1, len(layers)):\n",
    "        dropout_layer = K.layers.Dropout(1 - keep_prob)(x)\n",
    "        x = K.layers.Dense(units=layers[layer],\n",
    "                           activation=activations[layer],\n",
    "                           kernel_regularizer=K.regularizers.l2(lambtha)\n",
    "                           )(dropout_layer)\n",
    "\n",
    "    model = K.Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 10)                2010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,010\n",
      "Trainable params: 2,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1-main\n",
    "model = build_model(200, [10], ['tanh'], 0.01, 0.6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Optimize\n",
    "\"\"\"\n",
    "Write a function that sets up Adam optimization for a keras model with\n",
    "categorical crossentropy loss and accuracy metrics\n",
    "\"\"\"\n",
    "def optimize_model(network, alpha, beta1, beta2):\n",
    "    \"\"\"\n",
    "    network: the model to optimize\n",
    "    alpha: learning rate\n",
    "    beta1: the first Adam optimization parameter\n",
    "    beta2: second Adam optimization parameter\n",
    "    Returns none\n",
    "    \"\"\"\n",
    "\n",
    "    Adam_opt = K.optimizers.Adam(lr=alpha,\n",
    "                                 beta_1=beta1,\n",
    "                                 beta_2=beta2)\n",
    "    network.compile(optimizer=Adam_opt,\n",
    "                    metrics=['accuracy'],\n",
    "                    loss=\"categorical_crossentropy\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-main\n",
    "# model = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "# optimize_model(model, 0.01, 0.99, 0.9)\n",
    "# print(model.loss)\n",
    "# print(model.metrics)\n",
    "# opt = model.optimizer\n",
    "# print(opt.__class__)\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     print(sess.run((opt.lr, opt.beta_1, opt.beta_2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. One Hot\n",
    "\"\"\"\n",
    "Runction that converts a label vector into a one-hot matrix\n",
    "\"\"\"\n",
    "def one_hot(labels, classes=None):\n",
    "    \"\"\"\n",
    "    labels: labels for dat\n",
    "    \"\"\"\n",
    "    return K.utils.to_categorical(labels, classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 3-main\n",
    "labels = np.load('../data/MNIST.npz')['Y_train'][:10]\n",
    "print(labels)\n",
    "print(one_hot(labels))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4. Train\n",
    "\"\"\"\n",
    "Write a function that trains a model using mini-batch gradient descent\n",
    "\"\"\"\n",
    "def train_model_4(network, data, labels, batch_size,\n",
    "                epochs, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    return network.fit(x=data,\n",
    "                       y=labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       shuffle=shuffle,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 12:08:14.038270: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-06 12:08:14.044167: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-06 12:08:14.045395: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-06 12:08:14.046232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (BensLaptop): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_2722/3525871577.py:16: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 12:08:15.211806: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-03-06 12:08:15.271639: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/dense_8/kernel/v/Assign' id:659 op device:{requested: '', assigned: ''} def:{{{node training/Adam/dense_8/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/dense_8/kernel/v, training/Adam/dense_8/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 3s 59us/sample - loss: 0.3311 - acc: 0.9192\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 3s 54us/sample - loss: 0.1777 - acc: 0.9647\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 3s 59us/sample - loss: 0.1444 - acc: 0.9745\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 3s 51us/sample - loss: 0.1272 - acc: 0.9798\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 3s 52us/sample - loss: 0.1162 - acc: 0.9832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd98c0bcd60>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "train_model_4(network, X_train, Y_train_oh, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5. Validate\n",
    "\"\"\"\n",
    "Based on 4. Train, update the function def train_model to also analyze\n",
    "validation data\n",
    "\"\"\"\n",
    "def train_model_5(network, data, labels, batch_size, epochs,\n",
    "                  validation_data=None, verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    validation_data: the data to validate the model with, if not `None`\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    return network.fit(x=data,\n",
    "                       y=labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       shuffle=shuffle,\n",
    "                       validation_data=validation_data\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      " 1088/50000 [..............................] - ETA: 5s - loss: 1.5856 - acc: 0.6158 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 12:08:29.923891: W tensorflow/c/c_api.cc:291] Operation '{name:'training_2/Adam/dense_10/bias/v/Assign' id:1175 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/dense_10/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/dense_10/bias/v, training_2/Adam/dense_10/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49088/50000 [============================>.] - ETA: 0s - loss: 0.3291 - acc: 0.9196"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/.local/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-03-06 12:08:33.508842: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_1/AddN_1' id:962 op device:{requested: '', assigned: ''} def:{{{node loss_1/AddN_1}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul, loss_1/AddN)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 4s 76us/sample - loss: 0.3278 - acc: 0.9200 - val_loss: 0.1940 - val_acc: 0.9590\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 3s 54us/sample - loss: 0.1742 - acc: 0.9650 - val_loss: 0.1695 - val_acc: 0.9667\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 3s 54us/sample - loss: 0.1420 - acc: 0.9751 - val_loss: 0.1723 - val_acc: 0.9649\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 3s 58us/sample - loss: 0.1242 - acc: 0.9808 - val_loss: 0.1488 - val_acc: 0.9732\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 3s 59us/sample - loss: 0.1155 - acc: 0.9835 - val_loss: 0.1507 - val_acc: 0.9732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda2145a820>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "X_valid = datasets['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = datasets['Y_valid']\n",
    "Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "train_model_5(network, X_train, Y_train_oh, batch_size, epochs, validation_data=(X_valid, Y_valid_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6. Early Stopping\n",
    "\"\"\"\n",
    "Based on 5. Validate, update the function def train_model to also train the\n",
    "model using early stoping\n",
    "\"\"\"\n",
    "def train_model_6(network, data, labels, batch_size, epochs,\n",
    "                  validation_data=None, early_stopping=False, patience=0,\n",
    "                  verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    early_stopping: boolean that indicates whether or not to stop early\n",
    "        early stopping should only be performed if `validation_data` exists\n",
    "        early stopping should be based on validation loss\n",
    "    patience: patience used for early stopping\n",
    "    validation_data: the data to validate the model with, if not `None`\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    if validation_data is not None:\n",
    "        if early_stopping:\n",
    "            \"\"\"Create earlystop callback\"\"\"\n",
    "            earlystopping = K.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                      patience=patience)\n",
    "            return network.fit(x=data,\n",
    "                               y=labels,\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=epochs,\n",
    "                               verbose=verbose,\n",
    "                               shuffle=shuffle,\n",
    "                               validation_data=validation_data,\n",
    "                               callbacks=[earlystopping]\n",
    "                               )\n",
    "    else:\n",
    "        return network.fit(x=data,\n",
    "                           y=labels,\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=verbose,\n",
    "                           shuffle=shuffle,\n",
    "                           validation_data=validation_data,\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "   64/50000 [..............................] - ETA: 1:20 - loss: 2.4827 - acc: 0.0781"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-06 12:28:49.845081: W tensorflow/c/c_api.cc:291] Operation '{name:'training_2/Adam/dense_7/kernel/v/Assign' id:1032 op device:{requested: '', assigned: ''} def:{{{node training_2/Adam/dense_7/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_2/Adam/dense_7/kernel/v, training_2/Adam/dense_7/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 3s 61us/sample - loss: 0.3291 - acc: 0.9197 - val_loss: 0.1783 - val_acc: 0.9647\n",
      "Epoch 2/30\n",
      "   64/50000 [..............................] - ETA: 3s - loss: 0.1437 - acc: 0.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/.local/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-03-06 12:28:52.741186: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_1/AddN_1' id:812 op device:{requested: '', assigned: ''} def:{{{node loss_1/AddN_1}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul, loss_1/AddN)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 3s 51us/sample - loss: 0.1749 - acc: 0.9658 - val_loss: 0.1539 - val_acc: 0.9726\n",
      "Epoch 3/30\n",
      "50000/50000 [==============================] - 3s 51us/sample - loss: 0.1402 - acc: 0.9765 - val_loss: 0.1506 - val_acc: 0.9738\n",
      "Epoch 4/30\n",
      "50000/50000 [==============================] - 3s 53us/sample - loss: 0.1248 - acc: 0.9810 - val_loss: 0.1434 - val_acc: 0.9759\n",
      "Epoch 5/30\n",
      "50000/50000 [==============================] - 3s 54us/sample - loss: 0.1137 - acc: 0.9843 - val_loss: 0.1520 - val_acc: 0.9755\n",
      "Epoch 6/30\n",
      "50000/50000 [==============================] - 3s 53us/sample - loss: 0.1103 - acc: 0.9849 - val_loss: 0.1546 - val_acc: 0.9750\n",
      "Epoch 7/30\n",
      "50000/50000 [==============================] - 3s 53us/sample - loss: 0.1043 - acc: 0.9872 - val_loss: 0.1479 - val_acc: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbe81291b80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "X_valid = datasets['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = datasets['Y_valid']\n",
    "Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "train_model_6(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "            validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7. Learning Rate Decay\n",
    "\"\"\"\n",
    "Based on 6. Early Stopping, update the function def train_model to also train\n",
    "model with learning rate decay\n",
    "\"\"\"\n",
    "def train_model_7(network, data, labels, batch_size, epochs,\n",
    "                  validation_data=None, early_stopping=False, patience=0,\n",
    "                  learning_rate_decay=False, alpha=0.1, decay_rate=1,\n",
    "                  verbose=True, shuffle=False):\n",
    "    \"\"\"\n",
    "    learning_rate_decay: boolean that indicates whether or not learning rate\n",
    "    decay should be used\n",
    "        learning rate decay should only be performed if validation_data exists\n",
    "        the decay should be performed using inverse time decay\n",
    "        the learning rate should decay in a stepwise fashion after each epoch\n",
    "        each time the learning rate updates, Keras should print a message\n",
    "    early_stopping: boolean that indicates whether or not to stop early\n",
    "        early stopping should only be performed if `validation_data` exists\n",
    "        early stopping should be based on validation loss\n",
    "    patience: patience used for early stopping\n",
    "    validation_data: the data to validate the model with, if not `None`\n",
    "    network: model to train\n",
    "    data: numpy.ndarray of shape (m, nx) containing the input data\n",
    "        m: number of data points\n",
    "        nx: number of features\n",
    "    labels: one-hot numpy.ndarray of shape (m, classes) with the data labels\n",
    "        m: number of data points\n",
    "        classes: number of classes\n",
    "    batch_size: size of the batch used for mini-batch gradient descent\n",
    "    epochs: number of passes through data for mini-batch gradient descent\n",
    "    verbose: boolean that determines if the output should be printed during\n",
    "        training\n",
    "    shuffle: boolean that determines whether to shuffle the batches every\n",
    "        epoch. Normally, it is a good idea to shuffle, but for reproducibility\n",
    "        we have chosen to set the default to False\n",
    "    Returns: the History object generated after training the model\n",
    "    \"\"\"\n",
    "    if validation_data is not None:\n",
    "        if early_stopping:\n",
    "            \"\"\"Create earlystop callback\"\"\"\n",
    "            print(\"Creating early stopping callback\")\n",
    "            earlystopping = K.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
    "                                                      patience=patience)\n",
    "        if learning_rate_decay:\n",
    "            \"\"\"\n",
    "            Create learning rate decay callback\n",
    "            \"\"\"\n",
    "            print(\"Creating learning rate decay callback\")\n",
    "\n",
    "            def scheduler(epoch):\n",
    "                \"\"\"\n",
    "                This function changes the learning rate in a stepwise manner\n",
    "                using inverse time decay\n",
    "                \"\"\"\n",
    "                return alpha / (1 + decay_rate * (epoch))\n",
    "            learningratedecay = K.callbacks.LearningRateScheduler(scheduler,\n",
    "                                                                  verbose)\n",
    "        if early_stopping and learning_rate_decay:\n",
    "            print(\"early stopping and learning rate decay\")\n",
    "            return network.fit(x=data,\n",
    "                               y=labels,\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=epochs,\n",
    "                               verbose=verbose,\n",
    "                               shuffle=shuffle,\n",
    "                               validation_data=validation_data,\n",
    "                               callbacks=[earlystopping, learningratedecay]\n",
    "                               )\n",
    "        if early_stopping and not learning_rate_decay:\n",
    "            print(\"early stopping and not learning rate decay\")\n",
    "            return network.fit(x=data,\n",
    "                               y=labels,\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=epochs,\n",
    "                               verbose=verbose,\n",
    "                               shuffle=shuffle,\n",
    "                               validation_data=validation_data,\n",
    "                               callbacks=[earlystopping]\n",
    "                               )\n",
    "        if learning_rate_decay and not early_stopping:\n",
    "            print(\"learning rate decay and not early stopping\")\n",
    "            return network.fit(x=data,\n",
    "                               y=labels,\n",
    "                               batch_size=batch_size,\n",
    "                               epochs=epochs,\n",
    "                               verbose=verbose,\n",
    "                               shuffle=shuffle,\n",
    "                               validation_data=validation_data,\n",
    "                               callbacks=[learningratedecay]\n",
    "                               )\n",
    "    else:\n",
    "        print(\"no validation data\")\n",
    "        return network.fit(x=data,\n",
    "                           y=labels,\n",
    "                           batch_size=batch_size,\n",
    "                           epochs=epochs,\n",
    "                           verbose=verbose,\n",
    "                           shuffle=shuffle,\n",
    "                           validation_data=validation_data,\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating learning rate decay callback\n",
      "learning rate decay and not early stopping\n",
      "Train on 50000 samples, validate on 10000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/.local/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2023-03-06 12:57:35.025873: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_27/bias/Assign' id:4210 op device:{requested: '', assigned: ''} def:{{{node dense_27/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_27/bias, dense_27/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.001.\n",
      "Epoch 1/1000\n",
      "49344/50000 [============================>.] - ETA: 0s - loss: 0.3272 - acc: 0.9207"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsbanotto/.local/lib/python3.8/site-packages/keras/engine/training_v1.py:2333: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n",
      "2023-03-06 12:57:39.361787: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_8/AddN_1' id:4392 op device:{requested: '', assigned: ''} def:{{{node loss_8/AddN_1}} = AddN[N=2, T=DT_FLOAT, _has_manual_control_dependencies=true](loss_8/mul, loss_8/AddN)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 4s 89us/sample - loss: 0.3265 - acc: 0.9209 - val_loss: 0.1909 - val_acc: 0.9614 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0005.\n",
      "Epoch 2/1000\n",
      "34880/50000 [===================>..........] - ETA: 1s - loss: 0.1628 - acc: 0.9683"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m\n\u001b[1;32m     36\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m---> 37\u001b[0m train_model_7(network, X_train, Y_train_oh, batch_size, epochs,\n\u001b[1;32m     38\u001b[0m             validation_data\u001b[39m=\u001b[39;49m(X_valid, Y_valid_oh), early_stopping\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     39\u001b[0m             patience\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, learning_rate_decay\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, alpha\u001b[39m=\u001b[39;49malpha)\n",
      "Cell \u001b[0;32mIn[20], line 82\u001b[0m, in \u001b[0;36mtrain_model_7\u001b[0;34m(network, data, labels, batch_size, epochs, validation_data, early_stopping, patience, learning_rate_decay, alpha, decay_rate, verbose, shuffle)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[39mif\u001b[39;00m learning_rate_decay \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m early_stopping:\n\u001b[1;32m     81\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlearning rate decay and not early stopping\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 82\u001b[0m         \u001b[39mreturn\u001b[39;00m network\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m     83\u001b[0m                            y\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m     84\u001b[0m                            batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     85\u001b[0m                            epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     86\u001b[0m                            verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m     87\u001b[0m                            shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m     88\u001b[0m                            validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m     89\u001b[0m                            callbacks\u001b[39m=\u001b[39;49m[learningratedecay]\n\u001b[1;32m     90\u001b[0m                            )\n\u001b[1;32m     91\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mno validation data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training_v1.py:854\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    853\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 854\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    855\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    856\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    857\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    858\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    859\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    860\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    861\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    862\u001b[0m     validation_split\u001b[39m=\u001b[39;49mvalidation_split,\n\u001b[1;32m    863\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m    864\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    865\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m    866\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    867\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    868\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    869\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    870\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    871\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m    872\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    873\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m    874\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:734\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    729\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`validation_steps` should not be specified if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`validation_data` is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m         )\n\u001b[1;32m    732\u001b[0m     val_x, val_y, val_sample_weights \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m \u001b[39mreturn\u001b[39;00m fit_loop(\n\u001b[1;32m    735\u001b[0m     model,\n\u001b[1;32m    736\u001b[0m     inputs\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    737\u001b[0m     targets\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    738\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weights,\n\u001b[1;32m    739\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    740\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    741\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    742\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    743\u001b[0m     val_inputs\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m    744\u001b[0m     val_targets\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m    745\u001b[0m     val_sample_weights\u001b[39m=\u001b[39;49mval_sample_weights,\n\u001b[1;32m    746\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    747\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    748\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    749\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    750\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    751\u001b[0m     steps_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps_per_epoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    752\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:421\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m callbacks\u001b[39m.\u001b[39m_call_batch_hook(\n\u001b[1;32m    417\u001b[0m     mode, \u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m, batch_index, batch_logs\n\u001b[1;32m    418\u001b[0m )\n\u001b[1;32m    420\u001b[0m \u001b[39m# Get outputs.\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m batch_outs \u001b[39m=\u001b[39m f(ins_batch)\n\u001b[1;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_outs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    423\u001b[0m     batch_outs \u001b[39m=\u001b[39m [batch_outs]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/backend.py:4581\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4571\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callable_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4573\u001b[0m     \u001b[39mor\u001b[39;00m feed_arrays \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed_arrays\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4577\u001b[0m     \u001b[39mor\u001b[39;00m session \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session\n\u001b[1;32m   4578\u001b[0m ):\n\u001b[1;32m   4579\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[0;32m-> 4581\u001b[0m fetched \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callable_fn(\u001b[39m*\u001b[39;49marray_vals, run_metadata\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_metadata)\n\u001b[1;32m   4582\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches) :])\n\u001b[1;32m   4583\u001b[0m output_structure \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[1;32m   4584\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_structure,\n\u001b[1;32m   4585\u001b[0m     fetched[: \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)],\n\u001b[1;32m   4586\u001b[0m     expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   4587\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/client/session.py:1481\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1480\u001b[0m   run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1481\u001b[0m   ret \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39;49mTF_SessionRunCallable(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49m_session,\n\u001b[1;32m   1482\u001b[0m                                          \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, args,\n\u001b[1;32m   1483\u001b[0m                                          run_metadata_ptr)\n\u001b[1;32m   1484\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m   1485\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 7-main\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "# import numpy as np\n",
    "np.random.seed(SEED)\n",
    "# import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "# import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "datasets = np.load('../data/MNIST.npz')\n",
    "X_train = datasets['X_train']\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "Y_train = datasets['Y_train']\n",
    "Y_train_oh = one_hot(Y_train)\n",
    "X_valid = datasets['X_valid']\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "Y_valid = datasets['Y_valid']\n",
    "Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "lambtha = 0.0001\n",
    "keep_prob = 0.95\n",
    "network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "alpha = 0.001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "optimize_model(network, alpha, beta1, beta2)\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "train_model_7(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "            validation_data=(X_valid, Y_valid_oh), early_stopping=False,\n",
    "            patience=3, learning_rate_decay=True, alpha=alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8. Save Only the Best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
