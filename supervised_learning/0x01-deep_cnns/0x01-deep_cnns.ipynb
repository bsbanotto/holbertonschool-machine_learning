{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.disable_v2_behavior()\n",
    "import tensorflow.compat.v1.keras as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1. Inception Block\n",
    "\"\"\"\n",
    "Write a function that builds an inception block as described in Going Deeper\n",
    "with Convolutions (2014)\n",
    "\"\"\"\n",
    "def inception_block(A_prev, filters):\n",
    "    \"\"\"\n",
    "    A_prev: output from the previous layer\n",
    "    filters: tuple containing F1, F3R, F3, F5R, F5, FPP\n",
    "        F1: number of filters in the 1x1 convolution\n",
    "        F3R: number of filters in the 1x1 convolution before the 3x3\n",
    "        convolution\n",
    "        F3: number of filters in the 3x3 convolution\n",
    "        F5R: number of filters in the 1x1 convolution before the 5x5\n",
    "        convolution\n",
    "        F5: number of filters in the 5x5 convolution\n",
    "        FPP: number of filters in the 1x1 convolution after the max pooling\n",
    "    All convolutions inside the inception block should use a ReLU activation\n",
    "    Returns: the concatenated output of the inception block\n",
    "    \"\"\"\n",
    "    F1 = filters[0]\n",
    "    F3R = filters[1]\n",
    "    F3 = filters[2]\n",
    "    F5R = filters[3]\n",
    "    F5 = filters[4]\n",
    "    FPP = filters[5]\n",
    "\n",
    "    conv_1x1 = K.layers.Conv2D(filters=F1,\n",
    "                               kernel_size=(1, 1),\n",
    "                               padding='same',\n",
    "                               activation='relu'\n",
    "                               )(A_prev)\n",
    "\n",
    "    conv_1x1_3x3 = K.layers.Conv2D(filters=F3R,\n",
    "                                   kernel_size=(1, 1),\n",
    "                                   padding='same',\n",
    "                                   activation='relu'\n",
    "                                   )(A_prev)\n",
    "\n",
    "    conv_3x3 = K.layers.Conv2D(filters=F3,\n",
    "                               kernel_size=(3, 3),\n",
    "                               padding='same',\n",
    "                               activation='relu'\n",
    "                               )(conv_1x1_3x3)\n",
    "\n",
    "    conv_1x1_5x5 = K.layers.Conv2D(filters=F5R,\n",
    "                                   kernel_size=(1, 1),\n",
    "                                   padding='same',\n",
    "                                   activation='relu'\n",
    "                                   )(A_prev)\n",
    "\n",
    "    conv_5x5 = K.layers.Conv2D(filters=F5,\n",
    "                               kernel_size=(5, 5),\n",
    "                               padding='same',\n",
    "                               activation='relu'\n",
    "                               )(conv_1x1_5x5)\n",
    "\n",
    "    max_pool_3x3 = K.layers.MaxPooling2D(pool_size=(3, 3),\n",
    "                                         strides=1,\n",
    "                                         padding='same'\n",
    "                                         )(A_prev)\n",
    "\n",
    "    conv_1x1_pooled = K.layers.Conv2D(filters=FPP,\n",
    "                                      kernel_size=(1, 1),\n",
    "                                      padding='same',\n",
    "                                      activation='relu'\n",
    "                                      )(max_pool_3x3)\n",
    "\n",
    "    output = K.layers.Concatenate()([\n",
    "        conv_1x1, conv_3x3, conv_5x5, conv_1x1_pooled\n",
    "    ])\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-main\n",
    "X = K.Input(shape=(224, 224, 3))\n",
    "Y = inception_block(X, [64, 96, 128, 16, 32, 32])\n",
    "model = K.models.Model(inputs=X, outputs=Y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1. Inception Network\n",
    "\"\"\"\n",
    "Write a function that builds the inception network as described in\n",
    "    `Going Deeper with Convolutions (2014)`\n",
    "\"\"\"\n",
    "def inception_network():\n",
    "    \"\"\"\n",
    "    Assume input data will have shape (224, 224, 3)\n",
    "    All convolutions inside and outside the inception block should use a ReLU\n",
    "    activation\n",
    "    Returns the Keras model\n",
    "    \"\"\"\n",
    "    inputs = K.Input(shape=(224, 224, 3))\n",
    "\n",
    "    conv_7x7_2s = K.layers.Conv2D(filters=64,\n",
    "                                 kernel_size=(7, 7),\n",
    "                                 strides=2,\n",
    "                                 padding='same',\n",
    "                                 activation='relu'\n",
    "                                 )(inputs)\n",
    "\n",
    "    MaxPool3x3_2s = K.layers.MaxPooling2D(pool_size=(3, 3),\n",
    "                                          strides=2,\n",
    "                                          padding='same'\n",
    "                                          )(conv_7x7_2s)\n",
    "\n",
    "    conv_1x1_1v = K.layers.Conv2D(filters=64,\n",
    "                                  kernel_size=(1, 1),\n",
    "                                  padding='valid',\n",
    "                                  activation='relu'\n",
    "                                  )(MaxPool3x3_2s)\n",
    "\n",
    "    conv_3x3_1s = K.layers.Conv2D(filters=192,\n",
    "                                  kernel_size=(3, 3),\n",
    "                                  strides=1,\n",
    "                                  padding='same',\n",
    "                                  activation='relu'\n",
    "                                  )(conv_1x1_1v)\n",
    "\n",
    "    MaxPool3x3_2s_1 = K.layers.MaxPooling2D(pool_size=(3, 3),\n",
    "                                            strides=2,\n",
    "                                            padding='same'\n",
    "                                            )(conv_3x3_1s)\n",
    "\n",
    "    inception_layer_0 = inception_block(MaxPool3x3_2s_1,\n",
    "                                        [64, 96, 128, 16, 32, 32])\n",
    "\n",
    "    inception_layer_1 = inception_block(inception_layer_0,\n",
    "                                        [128, 128, 192, 32, 96, 64])\n",
    "\n",
    "    MaxPool3x3_2s_2 = K.layers.MaxPooling2D(pool_size=(3, 3),\n",
    "                                            strides=2,\n",
    "                                            padding='same'\n",
    "                                            )(inception_layer_1)\n",
    "\n",
    "    inception_layer_2 = inception_block(MaxPool3x3_2s_2,\n",
    "                                        [192, 96, 208, 16, 48, 64])\n",
    "\n",
    "    inception_layer_3 = inception_block(inception_layer_2,\n",
    "                                        [160, 112, 224, 24, 64, 64])\n",
    "\n",
    "    inception_layer_4 = inception_block(inception_layer_3,\n",
    "                                        [128, 128, 256, 24, 64, 64])\n",
    "\n",
    "    inception_layer_5 = inception_block(inception_layer_4,\n",
    "                                        [112, 144, 288, 32, 64, 64])\n",
    "\n",
    "    inception_layer_6 = inception_block(inception_layer_5,\n",
    "                                        [256, 160, 320, 32, 128, 128])\n",
    "\n",
    "    MaxPool3x3_2s_3 = K.layers.MaxPooling2D(pool_size=(3, 3),\n",
    "                                            strides=2,\n",
    "                                            padding='same'\n",
    "                                            )(inception_layer_6)\n",
    "\n",
    "    inception_layer_7 = inception_block(MaxPool3x3_2s_3,\n",
    "                                        [256, 160, 320, 32, 128, 128])\n",
    "\n",
    "    inception_layer_8 = inception_block(inception_layer_7,\n",
    "                                        [384, 192, 384, 48, 128, 128])\n",
    "\n",
    "    AvgPool7x7_1v = K.layers.AveragePooling2D(pool_size=(7, 7),\n",
    "                                              strides=1,\n",
    "                                              padding='valid'\n",
    "                                              )(inception_layer_8)\n",
    "\n",
    "    dropout = K.layers.Dropout(.4)(AvgPool7x7_1v)\n",
    "\n",
    "    softmax = K.layers.Dense(units=1000,\n",
    "                             activation='softmax'\n",
    "                             )(dropout)\n",
    "\n",
    "    return K.Model(inputs=inputs, outputs=softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-main\n",
    "model = inception_network()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2. Identity Block\n",
    "\"\"\"\n",
    "Write a function that builds an identity block as described in\n",
    "    `Deep Residual Learning for Image Recognition(2015)`\n",
    "\"\"\"\n",
    "def identity_block(A_prev, filters):\n",
    "    \"\"\"\n",
    "    A_prev: output from the previous layer\n",
    "    filters: tuple or list containing F11, F3, F12 respectively\n",
    "        F11: number of filters in the first 1x1 convolution\n",
    "        F3: number of filters in the 3x3 convolution\n",
    "        F12: numnber of filters in the second 1x1 convolution\n",
    "    All convolutions inside the block should be followed by batch normalization\n",
    "        along the channels axis and a rectified linear activation (ReLU)\n",
    "    All weights should use he normal initialization\n",
    "    Returns the activated output of the identity block\n",
    "    \"\"\"\n",
    "    F11 = filters[0]\n",
    "    F3 = filters[1]\n",
    "    F12 = filters[2]\n",
    "\n",
    "    init = K.initializers.he_normal()\n",
    "\n",
    "    conv1x1_0 = K.layers.Conv2D(filters=F11,\n",
    "                                kernel_size=(1, 1),\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                kernel_initializer=init\n",
    "                                )(A_prev)\n",
    "\n",
    "    batch_norm_0 = K.layers.BatchNormalization(axis=3)(conv1x1_0)\n",
    "\n",
    "    relu_0 = K.layers.ReLU()(batch_norm_0)\n",
    "\n",
    "    conv3x3_1 = K.layers.Conv2D(filters=F3,\n",
    "                                kernel_size=(3, 3),\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                kernel_initializer=init)(relu_0)\n",
    "\n",
    "    batch_norm_1 = K.layers.BatchNormalization(axis=3)(conv3x3_1)\n",
    "\n",
    "    relu_1 = K.layers.ReLU()(batch_norm_1)\n",
    "\n",
    "    conv1x1_2 = K.layers.Conv2D(filters=F12,\n",
    "                                kernel_size=(1, 1),\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                kernel_initializer=init)(relu_1)\n",
    "\n",
    "    batch_norm_2 = K.layers.BatchNormalization(axis=3)(conv1x1_2)\n",
    "\n",
    "    add_layer = K.layers.Add()([batch_norm_2, A_prev])\n",
    "\n",
    "    relu_output = K.layers.ReLU()(add_layer)\n",
    "\n",
    "    return relu_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-main\n",
    "X = K.Input(shape=(224, 224, 256))\n",
    "Y = identity_block(X, [64, 64, 256])\n",
    "model = K.models.Model(inputs=X, outputs=Y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3. Projection Block\n",
    "\"\"\"\n",
    "Write a function that builds an projection block as described in\n",
    "    `Deep Residual Learning for Image Recognition(2015)`\n",
    "\"\"\"\n",
    "def projection_block(A_prev, filters, s=2):\n",
    "    \"\"\"\n",
    "    A_prev: output from the previous layer\n",
    "    filters: tuple or list containing F11, F3, F12 respectively\n",
    "        F11: number of filters in the first 1x1 convolution\n",
    "        F3: number of filters in the 3x3 convolution\n",
    "        F12: numnber of filters in the second 1x1 convolution\n",
    "    s: stride for the first convolution in both the main path and the shortcut\n",
    "        connection\n",
    "    All convolutions inside the block should be followed by batch normalization\n",
    "        along the channels axis and a rectified linear activation (ReLU)\n",
    "    All weights should use he normal initialization\n",
    "    Returns the activated output of the identity block\n",
    "    \"\"\"\n",
    "    F11 = filters[0]\n",
    "    F3 = filters[1]\n",
    "    F12 = filters[2]\n",
    "\n",
    "    init = K.initializers.he_normal()\n",
    "\n",
    "    conv1x1_0 = K.layers.Conv2D(filters=F11,\n",
    "                                kernel_size=(1, 1),\n",
    "                                strides=s,\n",
    "                                padding='same',\n",
    "                                kernel_initializer=init\n",
    "                                )(A_prev)\n",
    "\n",
    "    batch_norm_0 = K.layers.BatchNormalization(axis=3)(conv1x1_0)\n",
    "\n",
    "    relu_0 = K.layers.ReLU()(batch_norm_0)\n",
    "\n",
    "    conv3x3_1 = K.layers.Conv2D(filters=F3,\n",
    "                                kernel_size=(3, 3),\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                kernel_initializer=init)(relu_0)\n",
    "\n",
    "    batch_norm_1 = K.layers.BatchNormalization(axis=3)(conv3x3_1)\n",
    "\n",
    "    relu_1 = K.layers.ReLU()(batch_norm_1)\n",
    "\n",
    "    conv1x1_2 = K.layers.Conv2D(filters=F12,\n",
    "                                kernel_size=(1, 1),\n",
    "                                strides=1,\n",
    "                                padding='same',\n",
    "                                kernel_initializer=init)(relu_1)\n",
    "\n",
    "    batch_norm_2 = K.layers.BatchNormalization(axis=3)(conv1x1_2)\n",
    "\n",
    "    conv1x1_shortcut = K.layers.Conv2D(filters=F12,\n",
    "                                       kernel_size=(1, 1),\n",
    "                                       strides=s,\n",
    "                                       padding='same',\n",
    "                                       kernel_initializer=init)(A_prev)\n",
    "\n",
    "    batch_norm_shortcut = K.layers.BatchNormalization(axis=3)(conv1x1_shortcut)\n",
    "\n",
    "    add_layer = K.layers.Add()([batch_norm_2, batch_norm_shortcut])\n",
    "\n",
    "    relu_output = K.layers.ReLU()(add_layer)\n",
    "\n",
    "    return relu_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-main\n",
    "X = K.Input(shape=(224, 224, 3))\n",
    "Y = projection_block(X, [64, 64, 256])\n",
    "model = K.models.Model(inputs=X, outputs=Y)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4. ResNet-50\n",
    "\"\"\"\n",
    "Write a function that builds the ResNet-50 architecture as described in\n",
    "    `Deep Residual Learning for Image Recognition (2015)`\n",
    "\"\"\"\n",
    "\n",
    "def resnet50():\n",
    "    \"\"\"\n",
    "    Assume input data will have shape (224, 224, 3)\n",
    "    All convolutions inside and outside the blocks should be followed by batch\n",
    "    normalization along the channels axis and a ReLU activation\n",
    "    All weights should use he normal initialization\n",
    "    Returns the keras model\n",
    "    \"\"\"\n",
    "    inputs = K.Input(shape=(224, 224, 3))\n",
    "    init = K.initializers.he_normal()\n",
    "\n",
    "    conv1 = K.layers.Conv2D(filters=64,\n",
    "                            kernel_size=(7, 7),\n",
    "                            strides=2,\n",
    "                            padding='same',\n",
    "                            kernel_initializer=init\n",
    "                            )(inputs)\n",
    "\n",
    "    batch_1 = K.layers.BatchNormalization(axis=3)(conv1)\n",
    "\n",
    "    ReLU_1 = K.layers.ReLU()(batch_1)\n",
    "\n",
    "    MaxPool1 = K.layers.MaxPooling2D(pool_size=(3, 3),\n",
    "                                     strides=2,\n",
    "                                     padding='same'\n",
    "                                     )(ReLU_1)\n",
    "\n",
    "    conv2_a = projection_block(MaxPool1, [64, 64, 256], s=1)\n",
    "    conv2_b = identity_block(conv2_a, [64, 64, 256])\n",
    "    conv2_c = identity_block(conv2_b, [64, 64, 256])\n",
    "\n",
    "    conv3_a = projection_block(conv2_c, [128, 128, 512])\n",
    "    conv3_b = identity_block(conv3_a, [128, 128, 512])\n",
    "    conv3_c = identity_block(conv3_b, [128, 128, 512])\n",
    "    conv3_d = identity_block(conv3_c, [128, 128, 512])\n",
    "\n",
    "    conv4_a = projection_block(conv3_d, [256, 256, 1024])\n",
    "    conv4_b = identity_block(conv4_a, [256, 256, 1024])\n",
    "    conv4_c = identity_block(conv4_b, [256, 256, 1024])\n",
    "    conv4_d = identity_block(conv4_c, [256, 256, 1024])\n",
    "    conv4_e = identity_block(conv4_d, [256, 256, 1024])\n",
    "    conv4_f = identity_block(conv4_e, [256, 256, 1024])\n",
    "\n",
    "    conv5_a = projection_block(conv4_f, [512, 512, 2048])\n",
    "    conv5_b = identity_block(conv5_a, [512, 512, 2048])\n",
    "    conv5_c = identity_block(conv5_b, [512, 512, 2048])\n",
    "\n",
    "    AvgPool = K.layers.AveragePooling2D(pool_size=(7, 7),\n",
    "                                        strides=1,\n",
    "                                        padding='valid'\n",
    "                                        )(conv5_c)\n",
    "\n",
    "    softmax = K.layers.Dense(units=1000,\n",
    "                             activation='softmax'\n",
    "                             )(AvgPool)\n",
    "\n",
    "    return K.Model(inputs=inputs, outputs=softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-main\n",
    "model = resnet50()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5. Dense Block\n",
    "\"\"\"\n",
    "Write a function that builds a densse block as described in\n",
    "    `Densely Connectec Convolutional Networks`\n",
    "\"\"\"\n",
    "def dense_block(X, nb_filters, growth_rate, layers):\n",
    "    \"\"\"\n",
    "    X: output from the previous layer\n",
    "    nb_filters: integer representing the number of filters in X\n",
    "    growth_rate: growth rate for the dense block\n",
    "    layers: number of layers in the dense block\n",
    "    Use the bottleneck layers used for DenseNet-B\n",
    "    All weights should use the he normal initialization\n",
    "    Convolutions should be preceded by Batch Normalization & ReLU activation\n",
    "    Returns the concatenated output of each layer within the Dense Block and\n",
    "        the number of filters within the concatenated outputs, respectively\n",
    "    \"\"\"\n",
    "    init = K.initializers.he_normal()\n",
    "\n",
    "    concatenation = X\n",
    "    for layer in range(layers):\n",
    "        batch_norm_0 = K.layers.BatchNormalization(axis=3)(concatenation)\n",
    "        ReLU_0 = K.layers.ReLU()(batch_norm_0)\n",
    "        conv_0 = K.layers.Conv2D(filters=(4 * growth_rate),\n",
    "                                 kernel_size=(1, 1),\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=init\n",
    "                                 )(ReLU_0)\n",
    "        batch_norm_1 = K.layers.BatchNormalization(axis=3)(conv_0)\n",
    "        ReLU_1 = K.layers.ReLU()(batch_norm_1)\n",
    "        conv_1 = K.layers.Conv2D(filters=growth_rate,\n",
    "                                 kernel_size=(3, 3),\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=init\n",
    "                                 )(ReLU_1)\n",
    "        concatenation = K.layers.Concatenate()([concatenation, conv_1])\n",
    "        nb_filters = nb_filters + growth_rate\n",
    "\n",
    "    return (concatenation, nb_filters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-main\n",
    "X = K.Input(shape=(56, 56, 64))\n",
    "Y, nb_filters = dense_block(X, 64, 32, 6)\n",
    "model = K.models.Model(inputs=X, outputs=Y)\n",
    "model.summary()\n",
    "print(nb_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6. Transition Layer\n",
    "\"\"\"\n",
    "Write a function that builds a transition layer as described in \n",
    "    `Densely Connected Convolutional Networks`\n",
    "\"\"\"\n",
    "def transition_layer(X, nb_filters, compression):\n",
    "    \"\"\"\n",
    "    X: the output from the previous layer\n",
    "    nb_filters: integer representing the number of filters in X\n",
    "    compression: compression factor for the transition layer\n",
    "    Code should implement compression as used in DenseNet-C\n",
    "    All weights should use he normal initialization\n",
    "    All convolutions should be preceded by Batch Normalization and a ReLU\n",
    "    Returns the output of the transition layer an the number of filters within\n",
    "        the output, respectively\n",
    "    \"\"\"\n",
    "    init = K.initializers.he_normal()\n",
    "    filters = (int)(nb_filters * compression)\n",
    "\n",
    "    batch_norm = K.layers.BatchNormalization(axis=3)(X)\n",
    "    ReLU = K.layers.ReLU()(batch_norm)\n",
    "    conv = K.layers.Conv2D(filters=filters,\n",
    "                           kernel_size=(1, 1),\n",
    "                           padding='same',\n",
    "                           kernel_initializer=init\n",
    "                           )(ReLU)\n",
    "    AvgPool = K.layers.AveragePooling2D(pool_size=(2, 2),\n",
    "                                        strides=2,\n",
    "                                        padding='same'\n",
    "                                        )(conv)\n",
    "\n",
    "    return (AvgPool, filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6-main\n",
    "X = K.Input(shape=(56, 56, 256))\n",
    "Y, nb_filters = transition_layer(X, 256, 0.5)\n",
    "model = K.models.Model(inputs=X, outputs=Y)\n",
    "model.summary()\n",
    "print(nb_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7. DenseNet-121\n",
    "\"\"\"\n",
    "Write a function that builds the DenseNet-121 architecture as described in\n",
    "    `Densely Connected Convolutional Networks`\n",
    "\"\"\"\n",
    "def densenet121(growth_rate=32, compression=1.0):\n",
    "    \"\"\"\n",
    "    growth_rate: growth rate\n",
    "    compression: compression factor\n",
    "    Assume all input data will have the shape (224, 224, 3)\n",
    "    All convolutions should be preceded by BN-ReLU\n",
    "    All weights should use the he normal initialization\n",
    "    Returns the keras model\n",
    "    \"\"\"\n",
    "    inputs = K.Input(shape=(224, 224, 3))\n",
    "    init = K.initializers.he_normal()\n",
    "\n",
    "    batch_norm_0 = K.layers.BatchNormalization(axis=3)(inputs)\n",
    "    ReLU_0 = K.layers.Activation('relu')(batch_norm_0)\n",
    "    conv_0 = K.layers.Conv2D(filters=64,\n",
    "                             kernel_size=(7, 7),\n",
    "                             strides=2,\n",
    "                             padding='same',\n",
    "                             kernel_initializer=init\n",
    "                             )(ReLU_0)\n",
    "    MaxPooling = K.layers.MaxPooling2D(pool_size=(3, 3),\n",
    "                                       strides=2,\n",
    "                                       padding='same'\n",
    "                                       )(conv_0)\n",
    "\n",
    "    dense_block_0, nb_filters = dense_block(X=MaxPooling,\n",
    "                                            nb_filters=64,\n",
    "                                            growth_rate=growth_rate,\n",
    "                                            layers=6)\n",
    "    transition_block_0, nb_filters = transition_layer(X=dense_block_0,\n",
    "                                                      nb_filters=nb_filters,\n",
    "                                                      compression=compression)\n",
    "    dense_block_1, nb_filters = dense_block(X=transition_block_0,\n",
    "                                            nb_filters=nb_filters,\n",
    "                                            growth_rate=growth_rate,\n",
    "                                            layers=12)\n",
    "    transition_block_1, nb_filters = transition_layer(X=dense_block_1,\n",
    "                                                      nb_filters=nb_filters,\n",
    "                                                      compression=compression)\n",
    "    dense_block_2, nb_filters = dense_block(X=transition_block_1,\n",
    "                                            nb_filters=nb_filters,\n",
    "                                            growth_rate=growth_rate,\n",
    "                                            layers=24)\n",
    "    transition_block_2, nb_filters = transition_layer(X=dense_block_2,\n",
    "                                                      nb_filters=nb_filters,\n",
    "                                                      compression=compression)\n",
    "    dense_block_3, nb_filters = dense_block(X=transition_block_2,\n",
    "                                            nb_filters=nb_filters,\n",
    "                                            growth_rate=growth_rate,\n",
    "                                            layers=16)\n",
    "\n",
    "    global_average = K.layers.AveragePooling2D(pool_size=(7, 7),\n",
    "                                               strides=1,\n",
    "                                               )(dense_block_3)\n",
    "\n",
    "    softmax = K.layers.Dense(units=1000,\n",
    "                             activation='softmax'\n",
    "                             )(global_average)\n",
    "\n",
    "    return K.Model(inputs=inputs, outputs=softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-main\n",
    "model = densenet121(32, 0.5)\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
